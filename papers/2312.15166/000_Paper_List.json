{
    "2312.15166": {
        "paper_id": "2312.15166",
        "abs_url": "https://arxiv.org/abs/2312.15166",
        "pdf_url": "https://arxiv.org/pdf/2312.15166.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2312.15166_SOLAR_107B_Scaling_Large_Language_Models_with_Simple_yet_Effective_Depth_Up-Scaling.pdf",
        "title": "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Dahyun Kim",
            "Chanjun Park",
            "Sanghoon Kim",
            "Wonsung Lee",
            "Wonho Song",
            "Yunsu Kim",
            "Hyeonwoo Kim",
            "Yungi Kim",
            "Hyeonju Lee",
            "Jihoo Kim",
            "Changbae Ahn",
            "Seonghoon Yang",
            "Sukyung Lee",
            "Hyunbyung Park",
            "Gyoungjin Gim",
            "Mikyoung Cha",
            "Hwalsuk Lee",
            "Sunghun Kim"
        ],
        "abstract": "We introduce depth up-scaling (DUS), a novel technique to up-scale base LLMs efficiently and effectively in a simple manner. In contrast to mixture-of-experts (MoE), DUS does not require complex changes to train and inference. Using DUS, we build SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. Comparative evaluations show that SOLAR 10.7B outperforms existing open-source pretrained LLMs, such as Llama 2 and Mistral 7B. We additionally present SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, surpassing Mixtral-8x7B. SOLAR 10.7B is publicly available under the Apache 2.0 license, promoting broad access and application in the LLM field.",
        "comments": "12 pages",
        "official_code_urls": [],
        "pwc_page_url": "",
        "bibtex": "@misc{kim2023solar,\n      title={SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling}, \n      author={Dahyun Kim and Chanjun Park and Sanghoon Kim and Wonsung Lee and Wonho Song and Yunsu Kim and Hyeonwoo Kim and Yungi Kim and Hyeonju Lee and Jihoo Kim and Changbae Ahn and Seonghoon Yang and Sukyung Lee and Hyunbyung Park and Gyoungjin Gim and Mikyoung Cha and Hwalsuk Lee and Sunghun Kim},\n      year={2023},\n      eprint={2312.15166},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}