# SOLAR 10.7B: Simple yet Effective Depth Up-Scaling로 Large Language Models Scaling

 김다현({}^{*}\), 박찬준({}^{*\dagger}\), 김상훈({}^{*\dagger}\), 이원성({}^{*\dagger}\), 송원호

김윤수

안창배

차미경, 이활숙, 김성훈.

Equal Contribution \({}^{\dagger}\) Corresponding Author

###### Abstract

본 논문에서는 깊이 업스케일링(Depth Up-scaling, DUS) 기법을 도입하여 LMM을 간단하고 효율적으로 확장한다. 혼합 전문가(MoE)와 달리 DUS는 훈련 및 추론을 위해 복잡한 변경을 요구하지 않는다. DUS를 이용하여 107억 개의 파라미터를 갖는 대용량 언어 모델(LLM)인 SOLAR 10.7B를 구축하여 다양한 자연어 처리(NLP) 작업에서 우수한 성능을 보인다. 비교 평가 결과 SOLAR 10.7B는 Llama 2 및 Mistral 7B와 같은 기존 오픈 소스 사전 훈련 LLM보다 성능이 우수한 것으로 나타났다. 또한 미스트랄-8x7B를 능가하는 명령어 추종 기능을 위해 미세 조정된 변형 SOLAR 10.7B-강사를 제시한다. SOLAR 10.7B는 LLM 필드 1에서 광범위한 액세스 및 애플리케이션을 촉진하는 아파치 2.0 라이선스에 따라 공개적으로 이용 가능하다.

각주 1: [https://huggingface.co/upstage/](https://huggingface.co/upstage/) SOLAR-10.7B-v1.0

각주 2: [https://github.com/google-learning/](https://github.com/google-learning/)

## 1 Introduction

자연어 처리(NLP) 분야는 대규모 언어 모델(LLM)의 도입으로 상당히 변형되었으며, 이는 인간 언어와의 우리의 이해와 상호 작용을 향상시켰다(Zhang et al., 2023). 이러한 발전은 유익하지만, 성능 스케일링 법칙(Kaplan et al., 2020; Hernandez et al., 2021; Anil et al., 2023)으로 인해 더 큰 모델을 훈련해야 하는 필요성이 증가하는 것과 같은 과제를 가져온다. 상기 내용을 효율적으로 다루기 위해, 전문가 혼합(MoE)과 같은 언어 모델을 스케일링하는 최근의 작업들(Shazeer et al., 2017; Komatsuzaki et al., 2022)이 제안되었다. 이러한 접근법들은 LLM들을 효율적이고 효과적으로 스케일업할 수 있지만, 종종 훈련 및 추론 프레임워크(Gale 등, 2023)에 대한 자명하지 않은 변경들을 요구하며, 이는 광범위한 적용성을 방해한다. 사용 용이성에 대한 _단순성_ 또한 유지하면서 LLM을 효과적이고 효율적으로 스케일업하는 것은 중요한 문제이다(Alberts et al., 2023; Fraiwan and Khasawneh, 2023; Sallam et al., 2023; Bahrini et al., 2023).

본 연구에서는 LLM을 확장하기 위한 효과적이고 효율적인 방법인 깊이 확장(depth up-scaling, DUS)을 도입하면서도 적용하기가 매우 용이하다. DUS를 사용하여, 다양한 벤치마크에서 Llama 2(Touvron et al., 2023) 및 Mistral 7B(Jiang et al., 2023)와 같은 기존 모델보다 우수한 107억 파라미터를 갖는 LLM인 SOLAR 10.7B를 출시한다. 제안된 DUS 방법은 HuggingFace(Wolf et al., 2019)와 같은 사용하기 쉬운 LLM 프레임워크와 즉시 호환되며 훈련 또는 추론 프레임워크에 추가 변경이 필요하다. 또한 DUS는 모든 변압기 아키텍처와 호환되어 새로운 게이트웨이를 열어 간단한 방식으로 LLM을 효과적이고 효율적으로 확장할 수 있습니다.

또한 복잡한 지침을 엄격하게 준수해야 하는 작업을 위해 미세 조정된 변형 SOLAR 10.7B-강사를 개발했습니다. 다양한 평가 메트릭에 걸쳐 미스트랄-8x7B 모델을 훨씬 능가하여 벤치마크 성능 측면에서 훨씬 더 큰 모델의 성능을 능가하는 고급 숙련도를 입증합니다.

Apache 2.0 라이선스에 따라 SOLAR 10.7B를 출시함으로써 NLP의 협업과 혁신을 촉진하는 것을 목표로 한다. 이 오픈 소스 접근법은 전 세계적으로 연구자와 개발자가 이러한 모델을 더 광범위하게 액세스하고 적용할 수 있도록 한다.

## 2 SOLAR 10.7B 아키텍처 세부 정보

성능 대 크기 트레이드 오프가 더 나은 LLM을 개발하려면 일반적으로 사용되는 7B 크기의 LLM이 파레토 최적 곡선에 있어야 하며 7B LLM을 확장해야 한다고 주장한다. 이를 위해 사전 훈련된 기본 모델의 가중치를 활용하여 보다 효율적인 방식으로 더 큰 LLM까지 확장한다. 구체적으로, 잘 수행되는 기본 모델을 선택하고 새로운 DUS 방법을 적용하여 기본 모델에서 사전 훈련된 가중치를 활용하는 ascaled-up 모델을 얻는다.

다른 업-스케일링 방법들, 특히 MoE는 효율적인 추론을 위해 MoE에 최적화된 별도의 트레이닝 프레임워크 및 커스텀 CUDA 커널들과 같은 복잡한 변경들을 요구한다는 점에 유의한다. 대조적으로, DUS로 상향 조정된 모델은 기본 LLM과 동일한 훈련 및 추론 프레임워크를 활용할 수 있으며 여전히 최대 효율성과 효율성을 달성할 수 있다. DUS는 사용하기 쉽고 기존의 광범위한 훈련 및 추론 프레임워크와 호환되는 대규모 LLM 및 기타 변압기 기반 아키텍처를 위한 간단한 방법을 제공한다.

**기본 모델.** 32 계층 Llama 2 아키텍처를 기본 모델로 선택 하 여 강력 하 고 다양 한 프레임워크를 추가 발전을 위한 최적의 기반으로 인식 합니다. 그런 다음 라마 2 아키텍처와 호환되는 상위 성능자 중 하나이기 때문에 미스트랄 7B에서 미리 훈련된 가중치를 사용하여 라마 2 아키텍처를 초기화한다.

기본 모델에 라마 2 아키텍처를 채택함으로써 커뮤니티 리소스의 방대한 풀을 활용하는 동시에 그 기능에 새로운 수정을 도입하는 것을 목표로 한다.

**깊이 상향 조정.** 기본 LLM을 상향 조정하는 한 가지 순진한 방법은 \(32\)에서 \(64\) 계층으로 _즉_ 해당 계층을 한 번 더 반복하는 것입니다. 이것은 층 1 내지 32 및 층 33 내지 64로부터, 이러한 층들이 베이스 LLM으로부터 직접 취해지므로 이질성이 없다는 이점을 갖는다. 즉, 기본 모델에서 레이어 인덱스의 차이인 '레이어 거리'는 레이어 32와 레이어 33이 연결된 1보다 클 뿐이며, _즉, seam에서_이다.

추가적인 이점은 MoE(Komatsuzaki 등, 2022)에서도 관찰되는 바와 같이 업-스케일링이 수행된 후 빠른 성능 회복을 위한 잠재력일 수 있다. 그 이유는 업스케일 모델을 최적화하는 것이 먼저 이음매에서 레이어의 불일치를 줄이는 데 초점을 맞출 수 있기 때문에 업스케일 모델의 성능이 빠르게 향상될 수 있기 때문이다. 그런 다음 모든 레이어의 점진적인 최적화가 시작됩니다.

그러나, 순진한 업-스케일링 접근법에서, 이음매에서의 층 거리는 최대에 도달하여, 잠재적으로 사전 훈련된 가중치를 효과적으로 활용하는 모델의 능력을 저해한다. 잠재적인 해결책은 중간 층을 희생하여 이음매에서의 불일치를 줄이는 것이다. 이 직관에 의해 우리는 그림 1에서 설명하는 깊이 업 스케일링(DUS) 방법을 고안한다.

DUS의 첫 번째 단계에서는 Mistral 7B 사전 훈련된 가중치를 가진 32-layer Llama2 아키텍처인 기본 모델을 취하고 복사본을 만든다. 다음으로 원래 기본 모델에서 마지막 8개 레이어를 잘라내고 복제에서 처음 8개 레이어를 잘라낸다. 이것은 우리에게 두 개의 24층 모델을 남깁니다. 마지막 단계에서, 이들 모델은 48개의 레이어와 107억 개의 파라미터를 갖는 깊이 상향-스케일링된 모델을 형성하도록 연결된다.

각 모델에서 8개의 레이어를 제거하기로 한 결정은 목표 성능 대 크기 절충에 의해 주도되었다. 업-스케일링된 모델에서 중간 층이 되었을 것을 폐기함으로써, 제1 모델의 층(24) 내지 제2 모델의 층(9)이 각각 층(32) 및 층(1) 대신에 연결됨에 따라 이음매에서의 층 거리가 감소된다.

Mixture of Experts(MoE) 접근법과 달리, DUS는 게이팅 네트워크 또는 전문가 선택 프로세스와 같은 추가 모듈을 필요로 하지 않는다. Con

그림 1: 32개의 레이어를 가진 기본 모델을 사용하여 깊이 업스케일링합니다. Llama2 아키텍처를 사용하지만 다른 트랜스포머 아키텍처는 깊이 업스케일링과 호환됩니다. 기본 모델에서는 32개 층 중 24개 층에서 사전 훈련된 가중치를 사용하여 48개 층 모두에 대해 사전 훈련된 가중치를 갖는 깊이 상향 스케일링 모델을 생성한다.

따라서 DUS 모델은 최적의 학습 효율을 위해 별도의 학습 프레임워크를 필요로 하지 않으며 빠른 추론을 위해 특수 CUDA 커널을 필요로 하지 않는다. DUS를 사용하여 상향 스케일링된 LLM은 높은 효율성을 유지하면서 기존의 훈련 및 추론 프레임워크에 원활하게 통합될 수 있다. 다음 섹션에서는 상향 조정된 모델 SOLAR 10.7B의 훈련 과정을 조사할 것이다."

## 3 SOLAR 10.7B Training Details

사전 훈련.DUS가 기본 모델에 적용된 후, 성능은 처음에 기본 LLM보다 떨어진다. 그러나 Sec에서의 우리의 가설과 일맥상통한다 2, 상향 축소 모델을 지속적으로 사전 훈련하면 빠른 성능 회복을 관찰할 수 있다. 연속적인 사전 훈련 후, 우리는 1) 명령어 튜닝과 2) 정렬 튜닝의 두 단계로 미세 조정을 수행한다.

명령어 튜닝.명령어 튜닝 단계에서, 모델은 QA 포맷의 명령어들을 따르도록 트레이닝된다(Zhang 등, 2023). 우리는 주로 오픈 소스 데이터 세트를 사용하지만 모델의 수학적 능력을 향상시키기 위해 수학 QA 데이터 세트를 합성한다. 데이터 세트를 만든 방법에 대한 개요는 다음과 같습니다. 먼저, 시드 수학 데이터는 GSM8K(Cobbe et al., 2021)와 같이 일반적으로 사용되는 벤치마크 데이터세트로의 오염을 피하기 위해, Math(Hendrycks et al., 2021) 데이터세트에서만 수집된다. 그런 다음, Meta-Math(Yu et al., 2023)와 유사한 프로세스를 사용하여, 시드 수학 데이터의 질문 및 답변을 재구성한다. QA 데이터 집합으로 재구성된 질문-답변 쌍을 사용하고 이를 'Synth. Math-Instruct'라고 한다.

얼라인먼트 튜닝.얼라인먼트 튜닝 단계에서, 명령어-튜닝된 모델은 직접 선호도 최적화(DPO)를 사용하여 인간 또는 강한 AI(_예를 들어,_GPT4(OpenAI, 2023)) 선호도와 더 정렬되도록 더 미세 조정된다(Rafailov 등, 2023). 수업 튜닝 단계와 유사하게 대부분 오픈소스 데이터셋을 사용하지만, 수업 튜닝 단계에서 언급된 'Synth. Math-Instruct' 데이터셋을 활용하여 수학 중심의 정렬 데이터셋을 합성하기도 한다.

정렬 데이터 합성 과정은 다음과 같다. 우리는 신스에서 개작된 질문-답변 쌍이라는 사실을 이용한다. 수학-강사 데이터는 모델의 수학적 능력을 향상시키는 데 유익하다(섹 4.3.1 참조). 따라서 우리는 재구성된 질문에 대한 재구성된 답변이 원래 답변보다 더 나은 답변이라고 추측하며, 아마도 중간 재구성 단계 때문일 수 있다. 결과적으로, 재구성된 질문을 프롬프트로 설정하고 재구성된 답변을 선택된 응답으로, 원래 답변을 거부된 응답으로 사용하여 {프롬프트, 선택, 거부} DPO 튜플을 생성한다. 재구문-답변 쌍의 튜플을 집계하고 결과 데이터 세트를 'Synth. Math-Alignment'라고 부른다.

모델 병합.모델 병합은 추가 학습 없이 모델 성능을 향상시키는 효과적인 방법입니다. 우리는 명령과 정렬 조정 단계 모두에서 훈련한 모델 중 일부를 병합한다. 1) 가중치의 단순 평균과 2) SLERP (Shoemake, 1985)의 두 가지 모델 병합 방법을 시도했다.

## 4 실험 결과

### 학습 데이터 집합 및 평가

학습 데이터 세트. Tab의 명령 및 정렬 조정 단계에 대 한 학습 데이터 세트에 대 한 세부 정보를 제시 합니다. 1. 항상 데이터 세트의 전체를 사용 하지 않고 대신 설정 양을 하위 샘플링 합니다. 우리는 대부분의 학습 데이터가 오픈 소스이며 공개되지 않은 데이터 세트가 될 수 있다는 점에 주목한다.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{Properties} & \multicolumn{3}{c}{Instruction} & \multicolumn{3}{c}{Alignment} \\ \cline{2-7}  & Alpaca-GPT4 & OpenOrca & Synth. Math-Instruct & Orca DPO Pairs & Ultrafeedback Cleaned & Synth. Math-Alignment \\ \hline Total \# Samples & 52K & 2.91M & 126K & 12.9K & 60.8K & 126K \\ Maximum \# Samples Used & 52K & 100K & 52K & 12.9K & 60.8K & 20.1K \\ Open Source & O & O & ✗ & O & O & ✗ \\ \hline \hline \end{tabular}
\end{table}
표 1: 각각 명령어 및 정렬 튜닝 단계에 사용되는 트레이닝 데이터세트. 명령어 튜닝 과정은 Alpaca-GPT4(Peng et al., 2023), OpenOrca(Mukherjee et al., 2023), Synth를 활용하였다. Math-Instruct 데이터 세트는 정렬 조정을 위해 Orca DPO 쌍(Intel, 2023), Ultrafeedback Cleaned(Ivison 등, 2023) 및 Synth를 사용했다. 수학 정렬 데이터 세트입니다. 'Total # Samples'는 전체 데이터 세트의 총 샘플 수를 나타냅니다. 사용된 최대 # 샘플'은 훈련에 사용된 실제 최대 샘플 수를 나타내며, 지정된 데이터 세트의 총 샘플 수보다 낮을 수 있습니다. '오픈 소스'는 데이터 세트가 오픈 소스인지 여부를 나타냅니다.

substituted for open-source alternatives, 예컨대 MetaMathQA Yu et al. (2023) dataset.

우리는 알파카 스타일 채팅 템플릿으로 지시 데이터 세트를 재포맷했다. FLAN Longpre 등(2023)으로부터 도출된 OpenOrca와 같은 데이터셋에 대해서는 벤치마크 데이터셋과 중복되는 데이터를 필터링한다(부록 C의 Tab. 8 참조). 정렬 데이터 세트는 {prompt, chosen, rejected} 삼중항 형식입니다. Zephyr Tunstall et al.(2023)에 따른 정렬 데이터셋을 전처리한다.

**평가.** HuggingFace Open LLM Leaderboard Beeching et al. (2023)에서 ARC Clark et al. (2018), HellaSWAG Zellers et al. (2019), MMLU Hendrycks et al. (2020), TruthfulQA Lin et al. (2022), Winogrande Sakaguchi et al. (2021), GSM8K Cobbe et al. (2021). 이러한 데이터 세트를 평가를 위한 벤치마크로 활용하고 _예:_ H6의 6개 작업에 대한 평균 점수를 보고한다.

### Main Results

SOLAR 10.7B 모델과 SOLAR 10.7B-Instruct 모델에 대한 평가 결과를 Tab의 다른 상위 성능 모델과 함께 제시한다. 2. SOLAR 10.7B는 Qwen 14B 및 Mistral 7B와 같은 유사한 크기의 다른 사전 훈련 모델보다 우수한 성능을 보여 DUS가 대규모 기반 LLM에 효과적인 방법임을 보여준다. 또한 더 작은 크기에도 불구하고 SOLAR 10.7B-Instruct는 H6 측면에서 가장 높은 점수를 받아 최근 최고 성능의 오픈 소스 LLM 미스트랄 8x7B-Instruct-0.1 또는 Qwen 72B를 능가한다. 위의 결과는 지속적으로 사전 훈련되고 미세 조정될 때 최신 성능을 달성할 수 있는 DUS 캔 업스케일 모델을 나타낸다. 또한 SOLAR 10.7B-강사의 무결성을 보여주기 위해 부록 C에 데이터 오염 결과를 보고한다.

### Ablation Studies

우리는 지시 단계와 정렬 조정 단계 모두에 대한 절제 연구를 제시한다.

#### 4.3.1 Instruction Tuning

**학습 데이터 세트에 대한 절제.** 탭의 명령 조정 단계에 대해 서로 다른 학습 데이터 세트를 사용하여 절제 연구를 제공합니다. 3. 절제된 모델에는 SFT 또는 감독된 미세 조정이 접두사로 지정되며 다음과 같이 훈련됩니다. 'SFT v1'은 Alpaca-GPT4 데이터 세트만 사용하는 반면, 'SFT v2'는 Alpaca-GPT4 데이터 세트와 함께 OpenOrca 데이터 세트를 사용한다. 'SFT v3'는 Synth를 사용한다. 수학-SFT v2에서 사용되는 데이터셋과 함께 학습 시 데이터를 지도한다. 마찬가지로, 'SFT v4'는 Synth를 사용한다. 수학-SFT v1에 사용된 데이터셋과 함께 학습 시 데이터를 지도한다.

먼저 Alpaca-GPT4와 OpenOrca가 훈련된 모델에 어떤 영향을 미치는지 분석한다. 학습에 Alpaca-GPT4 데이터셋만을 사용한 첫 번째 제거 모델인 'SFT v1'은 H6에 대해 \(69.15\)의 결과를 얻었다. 두 번째 제거 모델인 'SFT v2'를 학습하기 위해 OpenOrca 데이터셋을 추가하면 H6 점수는 \(69.21\)으로 'SFT v1'의 \(69.15\)과 거의 변화가 없다. 그러나 과제 점수는 'SFT v2'가 'SFT v1'의 \(52.24\)에 비해 \(57.32\)의 GSM8K 점수가 훨씬 더 높았지만 ARC, HellaSwag 및 TruthfulQA에서도 전반적으로 눈에 띄게 낮은 점수를 받았다. 이것은 OpenOrca를 사용하면 알파카-GPT4만 사용하는 것과 다르게 행동하는 모델이 생성됨을 나타내는 것으로 판단된다.

둘째, Synth 여부를 조사한다. Math-Instruct 데이터 세트는 유용합니다. 'SFT v3'의 경우, 우리는

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Model & Size & Type & H6 (Avg.) & ARC & HellaSwag & MMLU & TruthfulQA & Winogrande & GSM8K \\ \hline SOLAR 10.7B-Instruct & \(\sim\) 11B & Alignment-tuned & **74.20** & **71.08** & 88.16 & 66.21 & **71.43** & 83.58 & 64.75 \\ Open 72B & \(\sim\) 72B & Pretrained & 73.60 & 65.19 & 85.94 & **77.37** & 60.19 & 82.48 & **70.43** \\ Mistral 8x7B-Instruct-v0.1 & \(\sim\) 47B & Instruction-tuned & 72.62 & 70.22 & 87.53 & 71.16 & 64.58 & 81.37 & 60.73 \\ Yi 34B-200K & \(\sim\) 34B & Pretrained & 70.81 & 65.36 & 85.58 & 76.06 & 53.64 & 82.56 & 61.64 \\ Yi 34B & \(\sim\) 34B & Pretrained & 69.42 & 64.59 & 85.69 & 76.35 & 56.23 & 83.03 & 50.64 \\ Mistral 8x7B-v0.1 & \(\sim\) 47B & Pretrained & 68.42 & 66.04 & 86.49 & 71.82 & 46.78 & 81.93 & 57.47 \\ Llamma 70B & \(\sim\) 70B & Pretrained & 67.87 & 67.32 & 87.33 & 69.83 & 44.92 & 83.74 & 54.06 \\ Falcon 180B & \(\sim\) 180B & Pretrained & 67.85 & 69.45 & **88.66** & 70.50 & 45.47 & **86.90** & 45.94 \\ SOLAR 10.7B & \(\sim\) 11B & Pretrained & 66.04 & 61.95 & 84.60 & 65.48 & 45.04 & 83.66 & 55.50 \\ Open 14B & \(\sim\) 14B & Pretrained & 65.86 & 52.88 & 83.99 & 67.70 & 49.43 & 76.80 & 58.98 \\ Mistral 7B-Instruct-v0.2 & \(\sim\) 78B & Instruction-tuned & 65.71 & 63.14 & 84.88 & 60.78 & 68.26 & 77.19 & 40.03 \\ Yi 34B-Chat & \(\sim\) 34B & Instruction-tuned & 65.32 & 65.44 & 84.16 & 74.90 & 55.37 & 80.11 & 31.92 \\ Mistral 7B & \(\sim\) 7B & Pretrained & 60.97 & 59.98 & 83.31 & 64.16 & 42.15 & 78.37 & 37.83 \\ \hline \hline \end{tabular}
\end{table}
표 2: 다른 최고 성능 모델과 함께 SOLAR 10.7B 및 SOLAR 10.7B-강사에 대한 평가 결과. 우리는 Sec. 4.1에서 언급된 6개 과제에 대한 점수를 H6 점수(6개 과제의 평균)와 함께 보고한다. 우리는 또한 수십억 개의 매개변수 단위로 모델의 크기를 보고한다. 유형은 모델의 학습 단계를 나타내며 {Pretrained, Instruction-tuned, Alignment-tuned}에서 선택됩니다. SOLAR 10.7B 기반 모델은 보라색으로 색상이 있습니다. H6 및 개별 과제에 대한 최상의 점수는 굵게 표시된다.

신스를 추가하십시오. GSM8K 점수를 \(64.14\)으로 부스팅하고 다른 작업에 대해 비교 가능한 점수를 달성하는 Math-Instruct 데이터 세트입니다. 흥미롭게도, 우리가 신스를 추가할 때. SFT v4를 학습하기 위해 수학-강사 데이터셋을 'SFT v1'로 학습하면 모든 작업에서 'SFT v3'보다 높은 점수를 받아 가장 높은 H6 점수 \(70.88\)를 얻을 수 있다. 이상에서 우리는 신트를 추가하는 것을 알 수 있다. Math-Instruct 데이터 세트가 도움이 됩니다.

마지막으로 OpenOrca 유무에 따라 훈련된 모델을 병합하는 것이 성능을 높일 수 있는지 확인한다. 첫 번째 분석에서 우리는 OpenOrca를 사용하면 OpenOrca 없이 훈련된 모델과 다르게 행동하는 모델이 생성됨을 보았다. 이러한 직관을 바탕으로 OpenOrca가 있거나 없는 가장 성능이 좋은 모델인 'SFT v3'과 'SFT v4'를 병합한다. 놀랍게도, 결과 병합 모델 'SFT v3+v4'는 'SFT v4'에서 비 GSM8K 작업에 대한 높은 점수를 유지하지만 'SFT v3' 또는 'SFT v4'보다 더 높은 GSM8K 점수를 달성한다. 따라서 서로 다른 작업에 특화된 모델을 병합하는 것이 일반적으로 잘 수행되는 모델을 얻는 유망한 방법임을 알 수 있다.

#### 4.3.2 Alignment Tuning

이 섹션에서는 신중하게 설계된 정렬 조정 전략에 대해 설명합니다. 실제 정렬 조정을 위해 직접 선호도 최적화(Direct Preference Optimization, DPO)를 사용한다. 구체적으로, 훈련에 사용되는 서로 다른 훈련 데이터 세트, DPO 모델을 초기화하기 위한 서로 다른 SFT 기본 모델, 마지막으로 최종 정렬 조정 모델을 얻기 위한 모델 병합 전략을 보여준다.

학습 데이터 세트에 대한 절제. 탭에서 DPO 동안 사용된 다양한 정렬 데이터 세트에 대해 절제합니다. 4. DPO에 대한 SFT 기본 모델로 'SFT v3'를 사용합니다. 절제된 모델에 대한 설명은 다음과 같다. 'DPO v1'은 Ultrafeedback Clean 데이터셋만을 사용하고 'DPO v2'도 Synth를 사용하였다. Math-Alignment dataset.

먼저 울트라피드백 클린과 신스가 어떻게 작동하는지 테스트합니다. 수학-선형은 모형 성능에 영향을 미칩니다. DPO v1의 경우 H6에서 \(73.06\)을 달성하며, 이는 SFT 기본 모델 점수인 \(70.03\)에서 상당한 부스트이다. 그러나 ARC, HellaSwag 및 TruthfulQA와 같은 작업에 대한 점수는 모두 좋은 마진으로 향상되었지만 GSM8K에 대한 점수는 \(58.83\)으로 SFT 기본 모델 점수인 \(64.14\)보다 낮다. 신스 추가 'DPO v2'를 학습하기 위한 Math-Alignment에서는 GSM8k 점수가 \(60.27\)으로 향상되어 SFT 기본 모델보다 낮지만 여전히 'DPO v1'보다 높다는 것을 알 수 있다. 다른 작업 점수도 신트를 추가해도 부정적인 영향을 받지 않습니다. 수학 정렬 따라서 우리는 신트를 추가하는 것으로 결론지을 수 있다. 수학-정렬은 H6에 유익하다.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Model & Ultrafeedback Clean & Synth. Math-Alignment & H6 (Avg.) & ARC & HellaSwag & MMLU & TruthfulQA & Winogrande & GSM8K \\ \hline DPO v1 & O & ✗ & 73.06 & 71.42 & **88.49** & **66.14** & 72.04 & 81.45 & 58.83 \\ DPO v2 & O & O & **73.42** & **71.50** & 88.28 & 65.97 & 71.71 & **82.79** & **60.27** \\ DPO v1 + v2 & O & O & 73.21 & 71.33 & 88.36 & 65.92 & **72.65** & 82.79 & 58.23 \\ \hline \hline \end{tabular}
\end{table}
표 4: 직접 선호도 최적화(DPO) 단계에서 사용된 상이한 데이터 세트에 대한 절제 연구. 'SFT v3'는 DPO의 SFT 기본 모델로 사용된다. 선형 조정 단계를 나타내기 위해 'DPO' 접두사를 사용하여 절제된 모델의 이름을 지정한다. 'DPO v1+v2'는 단순히 모델 가중치를 평균하여 'DPO v1'과 'DPO v2'로부터 모델이 병합되었음을 나타낸다. H6 및 개별 과제에 대한 최상의 점수는 굵게 표시된다.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline Model & Alexa-GGP4 & OpenOrca & Synth. Math-Instrect & H6 (Avg.) & ARC & HellaSwag & MMLU & TruthfulQA & Winogrande & GSM8K \\ \hline SFT v1 & O & ✗ & ✗ & 69.15 & **67.66** & **86.03** & 65.88 & **60.12** & **82.95** & 52.24 \\ SFT v2 & O & O & ✗ & 69.21 & 63.56 & 83.59 & 65.93 & 58.47 & 82.79 & 57.32 \\ SFT v3 & O & O & 70.03 & 65.87 & 85.55 & 63.31 & 57.93 & 81.37 & 64.14 \\ SFT v4 & O & ✗ & O & 70.88 & 67.32 & 85.87 & 58.97 & 82.48 & 64.75 \\ SFT v3 + v4 & O & O & **71.11** & 67.32 & 85.96 & **65.95** & 58.80 & 2.08 & **66.57** \\ \hline \hline \end{tabular}
\end{table}
표 3: 명령어 튜닝에 사용되는 상이한 데이터 세트에 대한 절제 연구. 'SFT v3+v4'는 단순히 모델 가중치를 평균하여 'SFT v3'와 'SFT v4'로부터 모델이 병합되었음을 나타낸다. H6 및 개별 과제에 대한 최상의 점수는 굵게 표시된다.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Model & Base SFT Model & H6 (Avg.) & ARC & HellaSwag & MMLU & TruthfulQA & Winogrande & GSM8K \\ \hline DPO v2 & SFT v3 & 73.42 & **71.50** & **88.28** & **65.97** & 71.71 & **82.79** & 60.27 \\ DPO v3 & SFT v3 + v4 & **73.58** & 71.33 & 88.08 & 65.39 & **72.45** & 81.93 & **62.32** \\ \hline \hline \end{tabular}
\end{table}
표 5: 직접 선호도 최적화(DPO) 단계에서 사용된 상이한 SFT 기본 모델에 대한 절제 연구. 'SFT v3'는 DPO의 SFT 기본 모델로 사용된다. 선형 조정 단계를 나타내기 위해 'DPO' 접두사를 사용하여 절제된 모델의 이름을 지정한다. 'DPO v1+v2'는 단순히 모델 가중치를 평균하여 'DPO v1'과 'DPO v2'로부터 모델이 병합되었음을 나타낸다. H6 및 개별 과제에 대한 최상의 점수는 굵게 표시된다.

그런 다음 'DPO v1'과 'DPO v2'를 병합하는 것이 유익한지 실험한다. 불행히도 'DPO v1+v2'는 H6에서 \(73.21\)으로 'DPO v2'보다 더 나쁘다. 더 중요한 것은 신스 추가에서 GSM8K 점수 상승이다. 수학 정렬이 사라졌습니다. 바람직하지 않습니다. 그 이유 중 하나는 모델이 장단점이 다른 'SFT v3'과 'SFT v4'를 병합하는 경우와 달리 'DPO v2'가 'DPO v1'에 비해 엄격한 개선이기 때문일 수 있다.

SFT 기본 모델에 대 한 삭제는 DPO를 적용 하는 경우 이미 명령 조정 된 모델 _i.e._ 에서 시작 하 고 다른 SFT 기본 모델을 사용 하 여 제거 합니다. 울트라피드백 클린 앤 신스를 사용합니다. 이 절제에 대한 수학 정렬 데이터 세트입니다. 절제한 모델들 각각은 다음과 같이 트레이닝된다. 'DPO v2'는 'SFT v3'를 기본 SFT 모델로 사용하고, 'DPO v3'는 'SFT v3+v4'를 대신 SFT 기본 모델로 사용한다.

'SFT v3+v4'는 'SFT v3'에 비해 모든 과제에서 점수가 높으며, ARC(\(+1.45\))와 GSM8K(\(+2.43\))의 경우 그 격차가 특히 크다. 놀랍게도, 두 모델은 H6 측면에서 유사하게 수행한다. 개별 작업에 대한 점수를 자세히 살펴보면 GSM8K 점수에서 약간의 마진만 보이고 다른 작업 점수는 거의 차이를 보이지 않는다. 따라서, SFT 베이스 모델들 내의 특정 태스크들에서의 성능 갭들이 정렬-튜닝된 모델들로 항상 이월되는 것은 아니다.

다른 병합 방법에 대한 삭제가 탭에서 수행됩니다. 3, 우리는 서로 다른 강도를 가진 두 모델을 병합하는 것이 성능에 도움이 될 수 있음을 보았습니다. 이를 정렬 조정된 모델에도 활용하기 위해 'Cand. 1'과 'Cand. 2'라는 두 모델을 동일한 학습 데이터 세트와 SFT 기반 모델을 사용하여 'DPO v2'와 'DPO v3'이지만 각 모델의 각 강도를 최대화하기 위해 서로 다른 하이퍼 매개 변수를 사용하여 학습한다. Tab에서 'Cand. 1'과 'Cand. 2'를 비교한다. 6에서 'Cand. 1'은 GSM8K 점수는 높지만 다른 과제에 대해서는 상대적으로 낮은 점수를 받는 반면, 'Cand. 2'는 GSM8K 점수는 낮지만 다른 과제에 대해서는 높은 점수를 받는다는 것을 알 수 있다. 이 두 모델을 다양한 방법을 사용하여 병합하고 결과를 Tab.. 7에서 제거한다.

1) 평균(\(a\), \(b\)), 여기서 a와 b는 가중치의 평균을 구할 때 'Cand. 1'과 'Cand. 2'에 대한 가중치와 2) SLERP (Shoemake, 1985)의 두 가지 병합 방법을 사용한다. 평균값(\(a\), \(b\))은 (\(0.5\), \(0.5\)), (\(0.4\), \(0.6\)), (\(0.6\), \(0.4\))을 사용한다. 탭에서요 도 7을 참조하면, 서로 다른 병합 방법이 H6 점수에 거의 영향을 미치지 않음을 알 수 있다. 개별 태스크의 점수 또한 크게 다르지 않아 머지 후보가 충분히 다른 강도를 갖는 한 정확한 머지 방법이 중요하지 않을 수 있음을 시사한다. 따라서 SOLAR 10.7B-Instruct 모델로 'Merge v1'을 선택했다.

## 5 Conclusion

본 논문에서는 LLM을 효율적으로 확장하기 위한 간단한 방법인 깊이 업스케일링(depth up-scaling, DUS) 방법을 소개한다. MoE와 같은 다른 업-스케일링 기술과 달리, DUS는 최대 효율을 달성하기 위해 전문화된 트레이닝 또는 추론 프레임워크를 필요로 하지 않는다. 더 작은 기본 LLM에서 깊이 상향 조정된 SOLAR 10.7B 및 미세 조정된 변형 SOLAR 10.7B-강사를 훈련하여 DUS의 효과를 보여준다. Apache 2.0 라이선스에 따른 SOLAR 10.7B의 출시가 NLP 분야의 보다 협력적인 연구, 접근성 향상 및 지속 가능한 성장을 촉진할 수 있기를 바란다.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Model & H6 (Avg.) & ARC & HellaSwag & MMLU & TruthfulQA & Winogrande & GSM8K \\ \hline Cand. 1 & **73.73** & 70.48 & 87.47 & 65.73 & 70.62 & 81.53 & **66.57** \\ Cand. 2 & 73.28 & **71.59** & **88.39** & **66.14** & **72.50** & **81.99** & 59.14 \\ \hline \hline \end{tabular}
\end{table}
표 6: 머지 후보들 간의 성능 비교. “캔디 1” 및 캔드 중 적어도 하나를 포함하는 것을 특징으로 하는 방법. 2'는 각각 'DPO v2' 및 'DPO v3'과 동일한 설정을 사용하여 트레이닝되지만, 약간 상이한 하이퍼-파라미터들을 갖는다. H6 및 개별 과제에 대한 최상의 점수는 굵게 표시된다.

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline Model & Merge Method & H6 (Avg.) & ARC & HellaSwag & MMLU & TruthfulQA & Winogrande & GSM8K \\ \hline Merge v1 & Average (0.5, 0.5) & 74.00 & **71.16** & 88.01 & 66.14 & 71.71 & **82.08** & 64.90 \\ Merge v2 & Average (0.4, 0.6) & 73.93 & 71.08 & **88.08** & **66.27** & **71.89** & 81.77 & 64.52 \\ Merge v3 & Average (0.6, 0.4) & **74.05** & 71.08 & 87.88 & 66.13 & 71.61 & **82.08** & **65.50** \\ Merge v4 & SLERP & 73.96 & **71.16** & 88.03 & 66.25 & 71.79 & 81.93 & 64.59 \\ \hline \hline \end{tabular}
\end{table}
표 7: 최종 모델을 얻기 위해 사용된 상이한 병합 방법에 대한 절제 연구. 우리는 'Cand. 1'을 사용한다. 및 “Cand. 2” 탭에서요 6은 병합을 위한 두 가지 모델입니다. 병합된 모델의 이름을 '병합' 접두사로 지정하여 병합되었음을 나타냅니다. H6 및 개별 과제에 대한 최상의 점수는 굵게 표시된다.

## Acknowledgements

포옹 페이스, 특히 클레멘타인 포리에, 루이스 툰스톨, 오마르 산세비에로, 필리프 슈미드의 팀들에게 진심으로 감사드린다. 우리의 감사는 또한 AWS의 팀, 특히 리테스바자리아, 갈 오쉬리, 제이 권, 브랜든 리, 에피 배, 라훌 샤르마까지 확장된다. 한국통신(KT)의 팀들, 특히 이진형, 박정숙, 박성준, 왕홍래, 정경수, 윤선영 등 우리 모델의 폭넓은 호환성을 보장하는 데 큰 힘이 되어준 팀들에게 감사드린다.

## Limitations

첫째, 학습 및 추론을 위한 모델의 실질적인 계산 요구 사항은 제한된 계산 자원을 가진 모델에 대한 접근성을 제한할 수 있다. 둘째, 데이터 오염을 줄이기 위한 노력에도 불구하고 SOLAR 10.7B는 모든 기계 학습 모델과 마찬가지로 학습 데이터에 내재된 편향에 취약하여 잠재적으로 특정 시나리오에서 왜곡된 결과를 초래한다. 셋째, SOLAR 10.7B의 복잡성은 해석 가능성과 설명 가능성 측면에서 어려움을 제기하며, 이는 모델의 의사 결정 과정을 이해하는 것이 중요한 응용 프로그램에서 문제가 될 수 있다.

추가적으로, SOLAR 10.7B가 다양한 자연어 처리 작업에 걸쳐 훌륭하게 수행하지만, 그 효과는 상이한 언어, 특히 리소스가 적은 언어 및 전문화된 도메인에서 달라질 수 있다. 더욱이 SOLAR 10.7B를 훈련하고 실행하는 데 필요한 상당한 에너지 소비는 지속 가능한 AI 개발의 맥락에서 중요한 고려 사항인 환경 영향에 대한 우려를 제기한다.

마지막으로 SOLAR 10.7B-강사 변형은 다음 명령에서 향상된 성능을 보이지만 모델은 여전히 리소스 집약적이고 항상 효과적이지 않을 수 있는 프로세스인 특수 응용 프로그램에서 최적의 성능을 위해 작업별 미세 조정을 필요로 한다. 이러한 한계를 인정하는 것은 제안된 LLM의 능력에 대한 포괄적인 이해와 LLM의 향후 연구 및 개발을 안내하는 데 필수적이다.

## Ethics Statement

우리는 최고 윤리 기준을 유지하는 데 있어 SOLAR 10.7B의 헌신을 양심적으로 다루고 강조한다. 첫째, SOLAR 10.7B-강사가 평가에서 낮은 수준의 데이터 오염을 보여주었으며 이는 엄격한 데이터 처리 및 처리 프로토콜의 증거임을 강조한다. 이 측면은 SOLAR에서 얻은 결과의 신뢰성과 무결성을 뒷받침하기 때문에 중요하다.

또한, 실험 과정에서 모든 설정과 방법론이 잠재적인 윤리적 함정을 피하도록 보장했다. 윤리적으로 의심스러운 관행에 대한 이러한 선제적 고려와 회피는 혁신적일 뿐만 아니라 책임 있는 연구를 수행하는 데 대한 우리의 헌신을 강조한다.

또한 SOLAR이 운영의 모든 측면에서 일반적인 윤리적 고려 사항을 준수하는지 확인한다. 여기에는 개인 정보 보호 규범 준수, 지적 재산 존중, 알고리즘의 편향 부재가 포함된다. 이러한 윤리적 원칙에 대한 우리의 헌신은 확고하며 SOLAR의 신뢰성과 사회적 수용에 크게 기여한다고 믿는다.

결론적으로 SOLAR이 작동하는 윤리적 프레임워크는 강력하고 포괄적이어서 이 분야의 우리의 발전이 과학적으로 건전할 뿐만 아니라 윤리적으로도 책임이 있음을 보장한다.

## References

* L. 알베르츠 메콜리 피카, G 프레노실, K 시, A. 로밍거, A. 아프샤르-오로미에(2023) 대형 언어 모델(llm)과 채팅: 핵의학에 미치는 영향은 무엇인가? European journal of nuclear medicine and molecular imaging50(6), pp. 1549-1552. Cited by: SS1.
*R. 안일아엠다이 피라트 존슨 셰이커리, E 타로파, P 베일리, Z Chen, et al.(2023)Palm 2 기술보고서. arXiv preprint arXiv:2305.10403. Cited by: SS1.
* A. Bahrini, M. Khamoshifar, H. Abbasimehr, R. J. Riggs, M. Esmaeili R. Mastali Majdabadkohne, M. 파세바(2023) 9장: 응용 프로그램, 기회 및 위협입니다. In 2023 Systems and Information Engineering Design Symposium (SIEDS), pp. 274-279. Cited by: SS1.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: Open llm 리더보드 외부 링크: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: Open llm 리더보드 외부 링크: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: Open llm 리더보드 외부 링크: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: Open llm 리더보드 외부 링크: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: Open llm 리더보드 외부 링크: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: Open llm 리더보드 외부 링크: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: Open llm 리더보드 외부 링크: [https://huggingface.co/HuggingFaceH4/open_1lm_leaderboard](https://huggingface.co/HuggingFaceH4/open_1lm_leaderboard) SS1에 의해 인용 됩니다.
* E. Beeching, C. Fourrier, N. 하빕 한남 람베르트 오라자니 산세비에로 Tunstall, T. Wolf(2023) Open llm leaderboard. 참고: llm 리더보드 외부 링크 열기: [https://huggingface.co/spaces/HuggingFaceH4/open](https://huggingface.co/spaces/HuggingFaceH4/open)Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _ Advances in neural information processing systems_, 33:1877-1901.
* Clark 등(2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018년 질문 답변은 해결하셨나요? try arc, the ai2 reasoning challenge. _ arXiv preprint arXiv:1803.05457_.
* Cobbe et al.(2023) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve mathematics word problems. _ arXiv preprint arXiv:2110.14168_.
* Deng 등(2023) Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. 2023. Investigating data contamination in modern benchmarks for large language models. _ arXiv preprint arXiv:2311.09783_.
* Dong et al.(2023) Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. 래프트: 생성 기초 모델 정렬을 위한 보상 순위 피니튜닝. _ arXiv preprint arXiv:2304.06767_.
* Fraiwan and Khasawneh (2023) Mohammad Fraiwan and Natheer Khasawneh. 2023. 교육, 마케팅, 소프트웨어 공학, 및 헬스케어에서의 채팅 애플리케이션 리뷰: 이점, 단점 및 연구 방향 _ arXiv preprint arXiv:2305.00237_.
* Gale 등(2023) Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. 2023. Megablocks: Mixed-of-experts를 사용한 효율적인 sparse training. _ Proceedings of Machine Learning and Systems_, 5.
* 골친 및 수르데아누(2023) 샤히아르 골친 및 미하이 수르데아누. 2023. llms에서의 시간 이동: 대용량 언어 모델에서의 데이터 오염 추적_ arXiv preprint arXiv:2308.08493_.
* Hendrycks 등(2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. massive multitask language understanding 측정. _학습 표현에 대 한 국제 회의_ 에서입니다.
* Hendrycks 등(2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. _ arXiv preprint arXiv:2103.03874_.
* Hernandez 등(2021) Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021. Scaling laws for transfer. _ arXiv preprint arXiv:2102.01293_.
* Hwang et al.(2023) Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, et al. 2023. Tutel: Adaptive mixture-of-experts at scale. _ Proceedings of Machine Learning and Systems_, 5.
* Intel(2023) Intel. 2023. Supervised fine-tuning and direct preference optimization on intel gaudi2.
* Ivison 등(2023) Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. Camels in a changing climate: Enhancing lm adaptation with tulu 2.
* Jiang et al.(2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. _ arXiv preprint arXiv:2310.06825_.
* Kaplan 등(2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. _ arXiv preprint arXiv:2001.08361_.
* Komatskuzaki 등(2022) Aran Komatskuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. 2022. Sparse upcycling: Training mixture-of-experts from dense checkpoints. _ arXiv preprint arXiv:2212.05055_.
* Lin 등(2022) Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Truthfulqa: 모델이 인간의 거짓을 모방하는 방법을 측정한다. _Computational Linguistics Association의 60번째 연차총회 회보(권 1: 장문)_에서, 3214-3252페이지.
* Longpre et al.(2023) Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. _ arXiv preprint arXiv:2301.13688_.
* Mukherjee 등(2023) Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. _arXiv preprint arXiv:2306.02707_.
* OpenAI(2023) OpenAI. 2023. Gpt-4 기술 보고서.
* Peng 등(2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. _arXiv preprint arXiv:2304.03277_.
* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are notsupervised multitask learners. _ OpenAI blog_, 1(8):9.
* Rafailov 등(2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. 2023. 직접 선호도 최적화: 언어 모델은 비밀리에 보상 모델이다. _ arXiv preprint arXiv:2305.18290_.
* Raghavan 등(2020)Oscar Sainz, Jon Ander Campos, Iker Garcia-Ferrero, Julien Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. 2023. Nlp 평가 문제: 각 벤치마크에 대한 llm 데이터 오염을 측정할 필요성에 대해. _ arXiv preprint arXiv:2310.18018_.
* Sakaguchi 등(2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An Adversarial winograd schema challenge at scale. _ Communications of the ACM_, 64(9):99-106.
* Sallam 등(2023) Malik Sallam, Nesreen Salim, Muna Barakat, and Alaa Al-Tammemi. 2023. Chatgpt application in medical, dental, pharmacy, and public health education: A descriptive study highlighting the advantages and limitation. _ Narra J_, 3(1):e103-e103.
* Shazeer 등(2017) Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. beyondragely large neural networks: sparsely-gate mixture-of-experts layer. _ arXiv preprint arXiv:1701.06538_.
* Shen 등(2019) Tianxiao Shen, Myle Ott, Michael Auli, and Marc'Aurelio Ranzato. 2019. Mixture model for various machine translation: Tricks of the trade. _Machine Learning에 관한 International conference_에서, 페이지 5719-5728. PMLR.
* Shi et al.(2023) Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2023. 대형 언어 모델들로부터 사전 트레이닝 데이터를 검출하는 단계. _ arXiv preprint arXiv:2310.16789_.
* Shoemake (1985) Ken Shoemake. 1985. Animating rotation with quaternion curves. [제12회 컴퓨터 그래픽 및 대화형 기술에 관한 연례 회의의 진행사항] _245-254 페이지.
* Touvron et al.(2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_.
* Tunstall et al.(2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, et al. 2023. Zephyr: lm 정렬의 직접 증류. _ arXiv preprint arXiv:2310.16944_.
* Wang 등(2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannanehajishirzi. 2022. 자체 지시: 언어 모델을 자체 생성된 명령어와 정렬. _ arXiv preprint arXiv:2212.10560_.
* Wei 등(2021) Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. _ arXiv preprint arXiv:2109.01652_.
* Wei 등(2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, 등 2022a. 대형 언어 모델의 최신 기능입니다. _ arXiv preprint arXiv:2206.07682_.
* Wei et al.(2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. 생각의 연결 프롬프트는 대규모 언어 모델에서 추론을 이끌어냅니다. _ Advances in Neural Information Processing Systems_, 35:24824-24837.
* Wolf et al.(2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. 2019. Huggingface's Transformers: State-of-the-art natural language processing. _ arXiv preprint arXiv:1910.03771_.
* Yang 등(2023) Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. 2023. Large language models as optimizer. _ arXiv preprint arXiv:2309.03409_.
* Yu 등(2023) Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zheng Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. _ arXiv preprint arXiv:2309.12284_.
* Yuan 등(2023) Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. Rrhf: 눈물 없이 인간 피드백과 언어 모델을 정렬하기 위한 순위 응답. _ arXiv preprint arXiv:2304.05302_.
* Zellers 등(2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019년 헬라스와그: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? _Computational Linguistics Association의 제57차 연차총회 회보_에서, 페이지 4791-4800이다.
* Zhang et al.(2023a) Junwei Zhang, Huamin Feng, Biao Liu, and Dongmei Zhao. 2023a. 네트워크 보안 상황 인식 기술 현황 조사 _ Sensors_, 23(5):2608.
* Zhang 등(2023b) Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, 등 2023b. 대형 언어 모델에 대 한 지침 조정: 설문 조사입니다. _ arXiv preprint arXiv:2308.10792_.
* Zhou 등(2023) Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. 2023. llm을 평가 벤치마크 부정행위로 만들지 마세요. _ arXiv preprint arXiv:2311.01964_.
* Ziegler 등(2019) Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. _ arXiv preprint arXiv:1909.08593_.

Contributions

본 연구의 기여도는 다음과 같다.

* **혁신적인 LLM 상향 조정 방법**: 깊이 상향 조정은 효과적이면서도 효율적이면서도 쉽게 사용할 수 있습니다. 제안된 방법은 효능 또는 효율의 손상 없이 MoE와 같은 다른 업-스케일링 방법에 대한 보다 용이한 대안으로서 기능하며,
* **세계 최초 107억 매개 변수 모델 소개**: SOLAR 10.7B는 언어 모델 개발에서 전례 없는 규모를 보여 주는 이 분야의 새로운 선례를 설정합니다.
* **다양한 벤치마크에 걸친 우수한 성능**: SOLAR 10.7B는 다양한 벤치마크에서 탁월하여 추론, 수학 및 MMLU 프레임워크에서 Llama 2 및 Mistral 7B와 같은 기존 모델을 능가합니다.
* **명령어 추종 기능의 발전**: 향상된 명령어 추종 능력에 대해 미세 조정 된 변형인 SOLAR 10.7B-Instruct의 도입은 복잡한 명령을 이해 하 고 실행할 수 있는 모델의 능력을 크게 향상 시킵니다.
* **상용 실행 가능한 라이선스에 따른 배포**: Apache 2.0 라이선스에 따라 SOLAR 10.7B를 릴리스하면 상용 사용이 가능하여 이러한 모델을 다양한 제품 및 서비스에 통합할 수 있어 학술 연구와 실제 애플리케이션 간의 격차를 해소할 수 있습니다.

김다현, 박찬준, 김상훈, 이원성은 이 논문에 동등하게 기여했다. 김상훈은 김다현, 송원호, 김윤수, 김현우 등이 파운데이션 모델 파트를 이끌었다. 박찬준은 김윤기, 김지후, 안창배, 양성훈, 이수경, 박현병 등과 함께 데이터 및 평가(데이터 중심 LLM) 부분을 주도했다. 이원성은 김경진, 이현주, 차미경과 함께 적응 모델링 파트를 이끌었다. 이활석은 프로젝트 운영 전반의 역할을 수행했다. 이 모든 개인은 SOLAR 10.7B를 만드는 데 기여했다.

## 부록 B 관련 작업 및 배경

대형 언어 모델

컨텍스트 기반 언어 모델의 출현에 따라 다양한 연구에서 "스케일링 법칙" Kaplan et al.(2020); Hernandez et al.(2021); Anil et al.(2023)이 밝혀져 모델의 크기와 학습 데이터 및 모델 성능 사이에 양의 상관 관계가 입증되었다. 이로 인해 LMM(Large Language Models)이 등장하게 되었다. LLM은 이전 언어모델과 달리 Zero-shot learning Radford et al.(2019)과 Few-shot learning Brown et al.(2020)을 포함한 In-context learning의 능력을 보유하여 모델 가중치를 업데이트하지 않고도 새로운 작업을 수행할 수 있다. 더 작은 모델들에서 분명하지 않은 LLM들의 이러한 능력들은 Emergent abilities Wei 등(2022)으로 지칭된다.

### Experts의 혼합

기계 학습 아키텍처의 풍경에서, Shazeer 등(2017), Shen 등(2019), Komatsuzaki 등(2022)과 같은 Mixture of Experts(MoE) 모델은 복잡하고 이질적인 데이터에 의해 제기된 도전들을 해결할 수 있는 능력으로 주목을 받았다. MoE 모델은 향상된 출력 다양성을 포함하여 주목할만한 이점을 제공하여 입력 공간 내에서 복잡한 패턴을 캡처할 수 있다. 더욱이, 특히 희박한 형태로 구현될 때, 이들의 계산 효율은 자원 제약들이 Shazeer 등(2017); Komatsuzaki 등(2022)을 고려하는 시나리오들에서 이들을 가치있게 만들었다.

그러나 MoE 모델은 상당히 복잡하고 일정한 한계를 가지고 있다. 한 가지 중요한 문제는 그들이 어떻게 설정되는지에 매우 민감하다는 것인데, 이는 올바른 설정(하이퍼파라미터라고 함)을 찾기 위해 많은 시간과 노력이 필요하다는 것을 의미한다. MoE 모델의 주요 문제는 "후방 붕괴"라고 불리는 것이다. 이것은 모델의 한 부분이 다른 부분보다 훨씬 더 지배적이 될 때 발생한다. 다른 부분이 덜 효과적이 되거나 모델의 모든 부분이 같아 유용성이 떨어지는 ‘부자가 더 부자가 되는’ 상황과 같다.

또한, MoE 모델의 구현은 주로 동적 라우팅 및 부하-불균형 계산 Gale 등(2023)과 관련된 복잡성으로 인해 상당한 도전을 제기한다. TPU 및 XLA 컴파일러와 같은 딥 러닝을 위한 기존의 하드웨어 및 소프트웨어는 종종 텐서 도형에 대한 정적 지식을 요구하여 TPU에 대한 MoE 구현을 어렵게 만든다.

GPU 구현은 더 많은 유연성을 제공하지만, 희박한 계산 호환성은 장애물이 된다. 효율적인 계산을 용이하게 하고 모델 품질을 유지하기 위해 각 전문가의 크기를 고정하는 것 사이의 올바른 균형을 맞추는 것은 정보 보존과 하드웨어 효율성 사이의 균형을 만든다. 이 절충안은 차례로 하이퍼파라미터 조정 동안 신중한 고려를 필요로 하며 MoE 모델의 구현에 복잡성 계층을 추가하여 잠재적으로 이점을 상쇄한다. MoE 모델 구현에서의 만만치 않은 도전들을 고려할 때, 연구자들 및 실무자들이 Tutel Hwang 등(2023) 또는 Megablocks Gale 등(2023)과 같은 특화된 툴들 및 프레임워크들에 의존하는 것은 거의 불가피하게 된다.

MoE 모델의 수평 확장 특성에서 벗어나, 우리의 새로운 접근법인 DUS는 수직 차원을 도입한다. 이 방법은 MoE에 비해 덜 복잡한 경로를 취한다. MoE가 수평 복잡성과 경쟁하는 동안 DUS는 수직 평면에서 작동하여 복사만 하면 프로세스를 단순화하고 전면 및 후면 레이어를 트리밍한 다음 다시 조립한다. 이러한 접근 방식의 전환은 기존의 MoE 문제에서 벗어나 독특하고 더 간단한 작업 방식을 제공한다.

단순 구현 외에도 사후 붕괴와 같은 문제에 직면할 수 있는 MoE와 달리 DUS는 사용이 의미가 없는 상황이 적다는 점에서 주목할 만한 이점을 제공한다. 이 점은 DUS가 유용하고 신뢰할 수 있는 선택이라는 점을 강조하여 다양한 상황에서 그 강도와 적응성을 보여준다.

### Prompt Engineering

LLM의 새로운 능력을 활용하기 위한 핵심 연구 분야는 신속한 엔지니어링이다. 프롬프트 엔지니어링은 LLM이 특정 작업을 더 잘 수행할 수 있도록 하는 입력(프롬프트)을 설계하는 방법에 대한 연구이다. 본 연구의 대표적인 예로는 CoT( Chain-of-Thought) Wei et al.(2022)를 들 수 있는데, 이는 다단계 문제를 일련의 중간 추론 단계로 분해하는 CoT 프롬프팅을 제안한다. 더욱이, 이러한 신속한 엔지니어링까지도 LLMs Yang 등(2023)으로 대체하기 위한 노력이 진행되고 있다.

### Instruction Tuning

LLM의 조향성을 향상시키기 위해, 명령어 튜닝 Wei 등(2021)이 학습 기법으로서 등장하였다. 이것은 다양한 태스크 Wang 등에 대한 (명령, 입력, 출력)으로서 포맷된 데이터를 사용하여 LLM을 미세 조정하는 것을 포함한다(2022). 지시 튜닝은 목표 조정을 허용하여 모델의 능력에 대해 보다 통제되고 작업 지향적인 개선을 제공한다.

명령어 튜닝 전에, 기존의 방법들은 대형 언어 모델 Zhang 등(2023)의 동작을 효과적으로 안내하고 제어하는 데 어려움을 겪었다. 이러한 모델의 순전한 복잡성으로 인해 정확하고 작업 지향적인 응답을 보장하기 어려웠다. 보다 표적화된 접근법의 필요성은 기존 방법의 한계에서 비롯되어 수업 튜닝의 발전으로 이어졌다. 이 타겟팅된 접근법은 모델의 동작에 대한 더 나은 제어를 가능하게 하여 특정 작업에 더 적합하게 만들고 사용자 정의 목표와 일치하여 전체 성능을 향상시킨다. 따라서, 명령어 튜닝은 계산적으로 효율적이며 광범위한 재교육 또는 아키텍처 변경을 요구하지 않고 LLM을 특정 도메인에 신속하게 적응시키는 것을 용이하게 한다.

### Alignment Tuning

LLM은 사전 훈련 단계 Ziegler 등(2019)에서 인간의 의도가 아니라 다양한 도메인에 걸친 방대한 지식만을 학습했기 때문에 인간 독자에 의해 언어학적으로 일치하지 않는 것으로 인식될 수 있는 문장을 생성하는 것으로 관찰되었다. 이러한 한계를 극복하고 인간의 의도와 일치시키기 위해, 선행 연구 Ziegler 등(2019)은 RLF(Reforcement Learning with Human Feedback)를 제안하였다. RLHF는 인간의 선호도에 기반한 보상 모델을 학습하여 작동하며, 강화 학습을 사용하여 LLM이 가장 높은 보상 점수를 가진 답변의 우선 순위를 지정하는 방향으로 안내한다. 이 프로세스는 생성된 응답의 안전성, 적절성 및 전반적인 품질을 향상시킵니다. 만족스러운 성능을 보여주었음에도 불구하고, RLHF는 수많은 하이퍼파라미터를 관리하고 여러 모델(정책, 가치, 보상 및 참조 모델)을 통합해야 하는 것과 같은 문제에 직면한다.

이러한 과제들에 대한 응답으로, 인간 피드백(RRHF) Yuan 등(2023), Reward rAnked Fine-Tuning(RAFT) Dong 등(2023), 및 Direct와 같은 감독된 미세 조정 기반 접근법들이 제안되었다.

[MISSING_PAGE_FAIL:12]
