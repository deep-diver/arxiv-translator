[MISSING_PAGE_EMPTY:1]

## 1. Introduction

LOM(Large Language Models)은 인간의 언어를 이해하고 생성하도록 설계된 고급 AI 모델의 일종이다. 최근에는 Open AI (GPT-3 [(20)]와 GPT-4 [(20)]에 의해 개발된 것, Google (Gemini [(270)]], GLaM [(69)]], PaLM [(51)]], PaLM-2 [(7)]], Meta (LLaMA-1 [(276)] 및 LLaMA-2 [(277)] 및 BLOOM [(239)]], PanGu-\(\sum\)[(233)] 및 GLM [(336)]과 같은 다른 모델이 LLM이 급증하는 것을 목격하였다. 이 모델은 자연어 이해(NLU), 언어 생성, 복합 추론[(321)], 바이오 의학과 관련된 도메인별 작업[(102; 282; 283)], 법칙[(70)] 및 코드 생성[(35; 302)]과 같은 다양한 작업에 걸쳐 놀라운 성능을 보여주었다. 이러한 성능 돌파구는 다양한 소스로부터 엄청난 양의 데이터에 대해 훈련되는 동안 수십억 또는 심지어 수조 개의 매개변수를 포함하기 때문에 모델 크기와 양의 훈련 데이터의 대규모 규모에 기인할 수 있다.

LLM이 AI 혁명의 다음 물결을 이끌고 있지만, LLM의 놀라운 능력은 상당한 자원 수요를 희생한다[51; 69; 200; 233]. 그림 1은 LLaMA 계열에 대한 GPU 시간 측면에서 모델 성능과 모델 학습 시간의 관계를 나타낸 것으로, 각 원의 크기는 모델 파라미터의 수에 비례한다. 도시된 바와 같이, 더 큰 모델이 더 나은 성능을 달성할 수 있지만, 이들을 훈련시키는데 사용되는 GPU 시간의 양은 모델 크기가 확장됨에 따라 기하급수적으로 증가한다. 훈련 외에도 추론은 LLM의 운영 비용에도 상당히 크게 기여한다. 그림 2는 모델 성능과 추론 처리량 간의 관계를 나타낸다. 유사하게, 모델 크기를 스케일링하는 것은 더 나은 성능을 가능하게 하지만 더 낮은 추론 처리량(더 높은 추론 대기 시간)의 비용으로 제공되어, 이러한 모델들의 범위를 더 넓은 고객 기반 및 비용 효율적인 방식으로 다양한 애플리케이션으로 확장하는데 있어서의 과제를 제시한다.

LLM의 높은 자원 요구는 LLM의 효율성을 향상시키기 위한 기술 개발의 강력한 필요성을 강조한다. 도 2에 도시된 바와 같이, 추론 속도를 높이기 위해 그룹화된 질의 어텐션과 슬라이딩 윈도우 어텐션을 사용하는 LLaMA-1-33B에 비해, Mistral-7B[(123)]는 비교 가능한 성능 및 훨씬 더 높은 처리량을 달성한다. 이러한 우월성은 LLM에 대한 효율성 기술 설계의 타당성과 중요성을 강조한다.

도 1. 서로 다른 스케일에서 LLaMA 모델의 GPU 시간에서의 모델 성능 및 모델 트레이닝 시간의 예시. 보고된 성과는 여러 상식 추론 벤치마크의 평균 점수이다. 학습 시간은 Nvidia A100 80GB GPU를 기반으로 합니다. 각 원의 크기는 모형 모수의 수에 해당합니다. 원본 데이터는 [(276; 277)]에서 찾을 수 있다.

이 조사의 가장 중요한 목표는 효율적인 LLM의 기술 발전에 대한 전체론적 관점을 제공하고 기존 연구 방향을 요약하는 것이다. 그림 3에 예시된 바와 같이, 우리는 각각 **모델 중심**, **데이터 중심** 및 **프레임워크 중심** 관점에서 효율적인 LLMs 주제를 다루는 세 가지 주요 범주로 구성된 분류법으로 문헌을 구성한다. 이 세 가지 범주는 구별되지만 상호 연결된 연구 주제를 포괄하여 효율적인 LLM 연구에 대한 체계적이고 포괄적인 검토를 제공한다. 구체적으로,

* **모델 중심 방법:** 모델 중심 방법은 모델 자체가 초점인 알고리즘 수준 및 시스템 수준 효율적인 기술에 중점을 둡니다. 수십억 또는 수조 개의 매개변수를 가진 LLM은 소규모 모델에 비해 뚜렷한 특성[301]을 나타내므로 새로운 기술의 개발이 필요하다. SS2에서는 모델 압축, 효율적인 사전 훈련, 효율적인 미세 조정, 효율적인 추론 및 효율적인 아키텍처 설계와 관련된 연구 방향을 다루는 효율적인 기술을 조사한다.
* **데이터 중심 방법:** LLM의 영역에서 데이터의 중요성은 모델 자체만큼 중요합니다. 데이터 중심 방법은 LLM의 효율성을 높이는 데 있어 데이터의 품질과 구조의 역할에 중점을 둔다. SS3에서는 데이터 선택 및 신속한 엔지니어링과 관련된 연구 방향을 다루는 효율적인 기술을 조사한다.
* **LLM 프레임워크**: LLM의 출현으로 인해 학습, 추론 및 서비스를 효율적으로 처리하기 위한 전문 프레임워크의 개발이 필요했습니다. 텐서플로우, PyTorch 및 JAX와 같은 주류 AI 프레임워크가 기초를 제공하지만 LLM에 중요한 특정 최적화 및 기능에 대한 기본 지원이 부족하다. SS4에서는 효율적인 LLM을 위해 특별히 설계된 기존 프레임워크를 조사하여 고유한 기능, 기본 라이브러리 및 전문화를 다룬다.

설문 조사 외에도 [https://github.com/AIoT-MLSysLab/Efficient-LLMs-Survey](https://github.com/AIoT-MLSysLab/Efficient-LLMs-Survey)와 동일한 분류로 정리하여 설문 조사에 포함된 논문을 컴파일하는 GitHub 리포지토리를 구축했습니다. 우리는 그것을 적극적으로 유지하고 새로운 연구가 등장함에 따라 통합할 것입니다.

도 2: 성능 점수 _vs._ 다양한 LLM에 대한 추론 처리량. 처리량은 16 비트 부동 소수점 양자화를 사용 하 여 Nvidia A100 80GB GPU에서 측정 됩니다. 각 원의 크기는 배치 크기 1, 프롬프트 크기 256 및 생성 1000 토큰으로 실행할 때 각 모델의 메모리 풋프린트(기가바이트 단위)에 대응한다. 원본 데이터는 [120]에서 찾을 수 있다.

LLM에 대한 몇 가지 조사[26; 128; 299; 354]가 있지만, 이 조사는 LLM의 효율성 측면과 관련된 문헌에 대한 집중 검토와 토론을 제공한다. 또한 효율적인 트랜스포머에 대한 조사[269]와 그 훈련방법[363]도 있다. 대조적으로, 이 조사는 특히 수십억 개 이상의 매개변수의 모델에 대해 설계된 효율성 기술에 중점을 둔다. 우리는 이 조사가 GitHub 리포지토리와 함께 연구자와 실무자들이 문헌을 탐색하는 데 도움이 되고 효율적인 LLM에 대한 추가 연구를 고무하는 촉매 역할을 할 수 있기를 바란다.

도 3: 효율적인 LMM(Large Language Model) 문헌의 분류.

## 2. 모델 중심 방법

### Model Compression

그림 4에 요약된 바와 같이 LLMs에 대한 모델 압축 기술은 양자화, 매개변수 가지치기, 낮은 순위 근사화 및 지식 증류의 네 가지 범주로 그룹화할 수 있다.

#### 2.1.1. **Quantization**

양자화는 32비트 부동 소수점과 같은 고정밀 데이터 형식 \(\mathbf{X}^{\mathrm{H}}\)을 8비트 정수 [(61)] 또는 4비트 정수 [(62)]와 같은 고정밀 데이터 형식 \(\mathbf{X}^{\mathrm{L}}\)으로 모델 가중치 및/또는 활성화로 LLM을 압축합니다.]:

\[\mathbf{X}^{\mathrm{L}}=\mathrm{Round}\left(\frac{\mathrm{absmax}\left( \mathbf{X}^{\mathrm{L}}\right)}{\mathrm{absmax}\left(\mathbf{X}^{\mathrm{H}} \right)}\mathbf{H}^{\mathrm{H}}\right)=\mathrm{Round}\left(\mathcal{K}\cdot\mathbf{X}^{\mathrm{H}}\right),\,\mathrm{and}\,\mathbf{X}^{\mathrm{H}}=\frac{\mathbf{X}^{\mathrm{L}}}{\mathcal{K}} \tag{1}\]

여기서 Round는 부동수를 근사 정수로 매핑하는 것을 의미하고, absmax는 입력 요소의 절대 최대값을 나타내며, \(\mathcal{K}\)은 양자화 상수를 나타낸다.

LLM에 대한 양자화 기술은 PTQ(post-training quantization) 및 QAT(quantization-aware training)로 분류될 수 있다.

**PTQ(학습 후 양자화)** PTQ는 모델이 학습된 후 LLM을 양자화합니다. 정확도 저하를 보상하기 위해, PTQ는 양자화된 가중치 및/또는 활성화를 업데이트하기 위해 작은 교정 데이터세트를 사용한다. LLM에 대한 PTQ는 일반적으로 가중치 전용 양자화 및 가중치 활성화 공동 양자화의 두 가지 범주로 그룹화될 수 있다.

* _가중치만 양자화_ 는 LLM에 대해서만 모델 가중치를 양자화하는 데 중점을 둡니다. 예를 들어, Dettmers 등[(61)]은 완전한 정밀 모델 성능을 유지할 수 있으면서 추론 동안 메모리 사용량을 상당히 감소시키는 LLM.int8()이라는 최초의 수십억-스케일 Int8 가중치 양자화 방법을 소개한다. Frantar 등[(79)]은 한 단계 더 나아가 LLM 가중치를 8비트가 아닌 3 또는 4비트로 압축하는 훈련 후 가중치 양자화 방법인 GPTQ를 제안한다. GPTQ는 역 헤시안 정보로 가중치를 업데이트하기 위해 최적 브레인 양자화(OBQ; Optimal Brain Quantization)[(77)]를 사용한 계층별 양자화를 사용한다. 이 기술은 대략 4개의 GPU에서 175억 개의 매개변수를 가진 GPT 모델의 양자화를 가능하게 한다.

도 4. LLMs에 대한 모델 압축 기술의 요약.

원래 모델에 비해 최소한의 정확도 손실로 시간을 단축할 수 있습니다. 모델 가중치 및 프록시 헤시안 행렬이 인코히어런트일 때 양자화가 더 효과적일 수 있다는 통찰에 힘입어, Chee et al. [27]은 인코히어런트 처리를 적용하여 LLM을 가중치당 2비트로 양자화하는 학습 후 양자화 방법인 QuIP를 제안한다. Lin 등[164]은 양자화 손실을 결정하는 두드러진 가중치로 지칭되는 더 큰 활성화 크기를 갖는 모델 가중치의 작은 부분이 존재함을 관찰한다. 이러한 관찰에 기초하여, 그들은 높은 정밀도로 두드러진 가중치를 유지하면서 LLM을 양자화하기 위해 활성화 인식 가중치 양자화(AWQ)라는 가중치 양자화 접근법을 제안한다. 유사하게, Lee 등[143]은 또한 활성화 이상치가 가중치 양자화 손실을 증폭시킨다는 것을 관찰한다. 그들은 활성화 이상치가 있는 취약 가중치를 식별하고 고정밀도를 할당하기 위해 이상치 인식 가중치 양자화(OWQ)를 제안한다. Dettmers 등[63]은 큰 양자화 오류가 발생하기 쉬운 이상치 가중치를 분리하기 위해 Sparse-Quantized Representation(SpQR)을 제안한다. 이러한 이상치 가중치는 더 높은 정밀도 레벨로 저장되고, 나머지는 3-4 비트로 압축된다. 그런 다음 토큰 단위로 추론 과정을 가속화하는 SpQR 포맷을 위해 설계된 디코딩 기법을 제안한다. Kim et al. [136]은 이상치가 양자화된 가중치의 분포를 왜곡시키는 문제를 해결하고, 경험적으로 만들어진 휴리스틱 기반 접근법을 사용하여 모델 내에서 다양한 수준의 세분성을 다른 가중치 행렬에 할당하는 FineQuant를 제안한다.
* _가중치-활성화 Co-Quantization_ 는 모델 가중치와 활성화를 모두 양자화 합니다. 이상치들의 존재로 인해, 활성화들은 모델 가중치들[19]보다 양자화하기가 더 어렵다. Yao

도 5: LLMs에 대한 모델 압축 기술의 예시.

et al. [329]는 모델 가중치들을 위한 그룹-와이즈 양자화 및 활성화들을 위한 토큰-와이즈 양자화를 이용하는 ZeroQuant를 제안한다. 그러나 ZeroQuant는 1,750억 개 이상의 매개변수를 가진 모델에 대해 정확도를 유지할 수 없었다. 이 문제를 해결하기 위해 Yao et al. [330]과 Wu et al. [307]은 각각 낮은 순위의 행렬을 사용하여 정확도 저하를 복구하는 ZeroQuant-FP와 ZeroQuant-V2를 제안한다. 샤오 등[311]은 최대 5,300억 개의 LLM에 대해 가중치 및 활성화의 무손실 양자화를 8 비트로 달성하기 위해 양자화 난이도를 활성화로부터 가중치로 마이그레이션하는 채널당 스케일링 변환을 도입하는 SmoothQuant를 제안한다. Guo et al. [94] pinpoint 이상치들은 가중치 및 활성화 양자화에서 중요하지만, 그들의 근방의 정규 값들은 그렇지 않다. 이러한 관찰에 기초하여, 그들은 이상치들이 낮은 정밀도로 인코딩될 수 있도록 이상치들에 인접한 정규 값들을 프루닝하는 OliVe를 제안한다. Yuan 등 [334]는 상이한 채널들이 상이한 범위들을 가질 때 활성화들을 양자화하는 도전을 식별한다. 그들은 유사한 값 범위를 표시하는 활성화에서 채널을 그룹화하고 각 그룹의 값에 균일한 양자화 매개변수를 적용하는 RPTQ를 제안한다. Liu 등[168]은 활성화 이상치를 효율적으로 다루고 캘리브레이션 데이터를 활용하여 양자화로부터 발생하는 정보 손실을 상쇄하는 적응형 채널 재조립 방법인 QLLM을 제안한다. Wei 등[303]은 LLM들에서의 활성화 이상치들이 비대칭적이며 특정 채널들에 클러스터링되는 경향이 있음을 관찰한다. 이 관찰을 기반으로 비대칭 이상치를 무력화하기 위해 채널을 개별적으로 이동하고 확장하는 작업을 도입하는 이상치 억제+를 제안한다. 마지막으로, Ahmadian 등 [2]는 52B만큼 큰 스케일에서 큰 활성화 이상치를 억제하는 것이 가능함을 보여준다. 사전 훈련 동안 올바른 최적화 선택이 주어지면, 그들은 최소한의 정확도 저하로 410M에서 52B까지의 크기 범위의 모델을 양자화할 수 있다.

**QAT(Quantization-Aware Training)** QAT는 학습 프로세스 자체에서 LLM을 양자화하여 LLM이 양자화 친화적인 표현을 학습할 수 있도록 합니다. QAT는 PTQ에 비해 정확도 저하를 보완하기 위해 완전한 훈련 세트를 사용한 훈련이 필요하기 때문에 훨씬 더 비싸고 시간이 많이 걸린다. Tao et al. [267]은 균일한 워드 임베딩에 의해 야기되는 GPT-2와 같은 모델에서 양자화 문제를 해결하는 것을 목표로 하고, 자동-회귀 사전 트레이닝 동안 완전-정밀 교사 모델로부터 로짓 증류로부터 양자화된 학생 모델로 대비 증류를 결합하는 QuantGPT를 제안한다. LLM-QAT [176]은 학생 모델을 양자화하는 것을 목표로 LLM 자체에서 생성된 데이터를 사용하여 지식을 증류한다. 특히, 원래 출력 분포를 유지하고 초기 학습 데이터에 관계없이 생성 모델을 양자화할 수 있다. LLM-QAT는 가중치 및 활성화를 양자화하는 것 외에도 처리량을 향상시키고 LLM에서 긴 시퀀스 종속성을 수용하는 중요한 단계인 키 값 캐시의 양자화를 다룬다. 비트넷[287]은 트레이닝 동안 최적화기 상태들 및 구배들을 고정밀도로 유지하면서, 저-정밀도 이진 가중치들 및 양자화된 활성화들을 사용하여, 1-비트 LLM들에 대한 QAT를 개척하며, 1-비트 가중치들을 처음부터 트레이닝하기 위해 nn.선형 레이어의 교체만을 요구한다.

#### Parameter Pruning

파라미터 가지치기는 중복 모델 가중치를 제거하여 LLM을 압축한다. LLM에 대한 매개변수 가지치기 방법은 구조적 가지치기와 비구조적 가지치기로 분류할 수 있다.

**구조적 프루닝.** 구조적 프루닝은 연속 매개 변수 그룹 또는 LLM 가중치 행렬의 행, 열 또는 하위 블록과 같은 계층 구조와 같은 구조적 패턴을 프루닝하는 데 중점을 둡니다. 예를 들어, LLM-Pruner[183]는 그래디언트 정보를 사용하여 비필수적인 상호 연결된 구조를 선택적으로 제거하는 태스크-불가지론적 구조화된 프루닝 전략을 도입한다. 소량의 데이터를 활용하여 LLaMA[276]에 대한 결합 구조의 가중치, 파라미터, 그룹 중요도를 구하고 LoRA[111]을 사용하여 가지치기 후 성능을 회복하여 경쟁적인 제로 샷 성능을 보인다. 전단 LLaMA[310]는 두 가지 기술을 제안한다. 첫 번째 기술은 표적화된 구조화된 가지치기(target structured pruning)로, 레이어, 헤드, 중간 및 숨겨진 차원을 엔드 투 엔드 방식으로 제거하여 지정된 목표 모양으로 더 큰 모델을 가지치기한다. 두 번째 기술은 동적 배치 로딩으로, 다양한 도메인의 손실을 기반으로 각 학습 배치에서 샘플링된 데이터의 구성 요소를 동적으로 변경한다. 이 두 가지 기술을 통해 전단 LLaMA는 LLaMA2-7B 모델을 1.3B 매개변수로 프루닝할 수 있다. LoRAPrune(2017)은 중요도 추정을 위해 미리 훈련된 가중치의 기울기 대신 LoRA의 가중치와 기울기를 이용한 LoRA 기반 가지치기 기준을 소개한다. 초과 채널과 헤드를 제거하기 위해 구조화된 반복 가지치기 과정을 사용함으로써 LoRAPrune은 50% 압축률에서 LLM-Pruner보다 효율 면에서 우수하다.

**비정형 프루닝.** 비정형 프루닝은 반면 모델 가중치를 개별적으로 프루닝하는 데 중점을 둡니다. 따라서 구조적 프루닝에 비해 훨씬 더 유연합니다. Frantar and Alistarh (2018)는 재교육이 필요하지 않은 일회성 LLM 가지치기 접근법인 SparseGPT를 제시한다. 프루닝을 희소 회귀 문제로 공식화하고 헤시안 행렬의 역산을 기반으로 근사 해법을 활용하여 해결한다. 그렇게 함으로써, SparseGPT는 OPT-135B와 같은 모델에서도 60%의 구조화되지 않은 희소성에 도달하면서 약간의 복잡성 감소만을 경험한다. Sun 등(2019)은 가중치 크기들의 곱 값들 및 그들의 각각의 입력 활성화들에 기초하여 가중치들을 프루닝하는 완다를 제안한다. 완다는 SparseGPT에 비해 2차 정보에 의존하거나 가중치 업데이트를 필요로 하지 않으며 SparseGPT에 대해 경쟁적으로 수행한다. Shao et al.(2019)은 재학습 없이 LLM에서 최소 50%의 희소성을 달성하기 위해 헤시안 민감도 인식 혼합 희소성 가지치기를 활용할 것을 제안한다. 이 방법은 전체 희소성의 수준을 유지하면서 가지치기로 인한 오류를 최소화하기 위해 민감도에 따라 희소성을 적응적으로 할당한다.

#### 2.1.3. **낮은 순위 근사**

저순위 근사법은 LLM의 가중치 행렬 \(\mathbf{W}^{m\times n}\)을 저순위 행렬 \(\mathbf{U}\)과 \(\mathbf{V}\)으로 근사하여 LLM을 압축한다. \(\mathbf{W}\approx\mathbf{U}\mathbf{V}^{\top}\), \(\mathbf{U}\in\mathbb{R}^{m\times r}\), \(\mathbf{V}\in\mathbb{R}^{n\times r}\), \(r\)은 일반적으로 \(m,n\)보다 훨씬 작다. 그렇게 함으로써, 낮은 순위의 근사화는 파라미터의 수를 감소시키고 계산 효율을 향상시킨다. 특히, Xu 등(2019)은 Tensor-Train Decomposition(TTD)을 이용하여 LLM의 임베딩 레이어를 압축하는 TensorGPT를 소개한다. 각 토큰 임베딩을 변환 및 분해하여 분산 방식으로 효율적으로 연산할 수 있는 MPS(Matrix Product State)라는 효율적인 임베딩 포맷을 생성한다. LoSparse (2019)는 희소 행렬의 가지치기를 통해 비-표현적 요소와 비-표현적 요소를 제거하면서 낮은 순위 근사화를 통해 뉴런 내의 코히어런트 및 표현적 요소를 압축하는 것을 목표로 한다. 반복 학습을 사용하여 가지치기를 위한 열 뉴런의 중요 점수를 계산하여 기존의 반복 가지치기 방법을 능가한다.

#### 2.1.4. **지식 증류**

지식 증류(KD)는 LLM의 성능을 교사 모델로서 에뮬레이트하기 위해 더 작은 학생 모델을 트레이닝함으로써 LLM들을 압축하여, 학생 모델이 계산적으로 덜 확장적이면서도 교사 모델과 유사한 높은 수준의 성능을 유지한다. LLM에 대한 KD는 화이트박스 KD 방법과 블랙박스 KD 방법으로 분류할 수 있다.

**White-Box Knowledge Distillation.** White-box KD는 증류 공정에서 교사 LLM의 매개 변수 또는 로짓이 사용되는 KD 기술을 나타냅니다 (Zhou et al., 2018). 예를 들어, Baby LLaMA(Zhou et al., 2018)는 10M 단어들의 BabyLM 데이터세트를 이용하여 GPT-2의 앙상블과 더 작은 LLaMA-1 모델들의 컬렉션을 트레이닝한다. 이 앙상블은 5,800만 매개변수가 있는 컴팩트 LLaMA 모델로 증류되어 원래 교사 모델과 증류를 사용하지 않고 훈련된 비교 가능한 모델 모두에 비해 성능이 우수하다. Gu 등(2018)은 KLD(Kullback-Leibler divergence)와 같은 종래의 KD 목표가 분류 작업에 비해 더 복잡한 출력 공간으로 인해 오픈 텍스트 생성 작업에 적합하지 않을 수 있음을 관찰한다. 이러한 문제를 해결하기 위해, 이들은 정책 그래디언트 기법[264]을 통해 목적 함수의 그래디언트를 이용하여 역 KLD를 최소화하는 MiniLLM을 제안한다. 이 접근법은 130억 매개변수 LLaMA-1 모델에서 표준 KD 벤치마크의 성능을 능가한다[276]. 유사하게, 일반화된 지식 증류(GKD) [1]은 트레이닝 동안 학생 모델로부터 출력 시퀀스를 드로잉함으로써 분포 불일치 문제를 다룬다. GKD는 역 KL과 같은 다양한 발산 측정을 최적화하여 모델 과소 지정 문제를 해결한다. 이 접근법은 교사 모형의 분포 내에서 개연성이 있는 학생 모형으로부터 표본을 생산하는 것을 목적으로 한다. KPTD [202]는 KD 방법이 엔티티 정의에서 미리 훈련된 언어 모델의 매개변수로 지식을 성공적으로 전달하고 전파할 수 있음을 보여준다. 구체적으로, 그것은 엔티티의 정의에 기초하여 텍스트를 생성하도록 언어 모델을 프롬프트함으로써 전송 세트를 생성한다. 그런 다음 모델의 매개변수를 업데이트하여 학생 언어 모델의 분포를 교사 모델의 분포와 정렬한다. TED [163]은 층별 작업 증류를 위한 기술을 소개한다. 특별히 설계된 필터를 사용하여 각 레이어에서 학생과 교사 모델의 내부 상태를 정렬합니다. 이러한 필터들은 특정 작업에 유익한 내부 상태들로부터 관련 지식을 추출한다. TED는 지속적인 사전 훈련과 미세 조정 모두에서 상당한 그리고 꾸준한 성능 향상을 보여준다. TSLD [134]는 토큰 수준 증류를 활용하여 QAT를 향상시켰으며, 이는 중간 표현을 개혁하여 토큰 예측 복구에서 계층 간 KD의 한계를 극복하고 LLM에 QAT를 성공적으로 적용했다. 마지막으로 MiniMA [339]는 LLM을 증류할 때 용량 격차에 대한 뷰포트를 제안하고 분석을 통해 원리로 변환하고 계산-성능 파레토 프론티어의 새로운 벤치마크를 설정하는 3B 언어 모델을 도입한다.

**Black-Box 지식 증류.** White-box KD와 달리 Black-box KD에서는 교사 LLM에서 생성된 출력만 증류 공정에 사용됩니다. MetaICL 및 MetalICL[43, 189]에서 영감을 받아, 언어 모델이 인-컨텍스트 학습 목표를 사용하여 광범위한 태스크에서 메타 트레이닝된 다음 인-컨텍스트 학습을 통해 보이지 않는 태스크에 대해 미세 조정된 멀티태스크-ICT[116]는 인-컨텍스트 학습 증류로 알려진 개념을 도입한다. 이 방법은 LLM 교사로부터 학생 모델로 소샷 학습 역량을 이양하는 것을 목적으로 한다. 유사하게, LI 등[151]은 GPT-3 텍스트-다빈치-002 버전[201]에 의해 생성된 설명과 함께 멀티-태스크 학습을 채용하는 하이브리드 프롬프트 기술을 소개한다. 이 방법은 설명을 더 작은 모델로 증류하는 데 사용되어 다양한 시나리오에서 강력한 단일 작업 미세 조정 벤치마크에 비해 일관되고 상당한 개선을 달성한다. 라이온[125]은 학생 모델의 기술 수준을 점진적으로 향상시켜 지식 전달의 효율성을 높이는 것을 목표로 하는 적대적 증류 아키텍처를 소개한다. 특히 LLM이 도전적인 지침을 인식하고 학생 모델에 대한 새로운 복잡한 지침을 생성하도록 유도하여 모방, 구별 및 생성을 포함하는 3단계 적대적 주기를 설정한다. DISCO [44]는 일반적인 LLM이 구동 섭동을 생성하도록 유도하는 것을 포함한다. 그런 다음 이러한 생성된 섭동은 전문화된 교사 모델에 의해 필터링되어 고품질 반사실적 데이터를 더 작은 학생 모델로 증류하여 더 작은 모델이 인과적 표현을 더 안정적으로 학습할 수 있다. 최근 일부 연구에서는 CoT(chain-of-thought) 프롬프트가 LLM에서 블랙박스 KD를 통해 더 작은 모델로 이 능력을 전달하기 위해 복잡한 추론 작업을 단계적으로 해결하기 위해 언어 모델을 이끌어낼 수 있음을 보여주었다. 예를 들어, Fu 등[82]은 더 작은 모델의 CoT 수학 추론 능력을 향상시키는 것을 목표로 한다. 구체적으로, 이들은 LLM 교사(GPT-3.5 코드-다빈치-002 [35])로부터 GSM8K 데이터세트에서 발견된 추론 경로들을 증류함으로써 학생 모델(FlanT5)을 지시하는 방법을 채용한다. 그런 다음 작은 모델은 세 개의 개별 보류된 수학 추론 데이터 세트에 대한 평균 성능을 기반으로 선택되어 새로운 배포 외 시나리오로 잘 일반화하는 능력을 확인한다. 마찬가지로, Distilling Step-by-Step [110]은 LLM들의 성능에 부합하기 위해, 더 작은 모델들을 미세 조정하고 Distilling하는 것은 상당한 양의 트레이닝 데이터를 필요로 한다고 주장한다. 이를 해결하기 위해 CoT 프롬프팅을 사용하여 다중 작업 설정 내에서 더 작은 모델을 훈련할 때 추가 안내를 위한 LLM 근거를 추출하여 소수의 샷 프롬프트 LLM에 비해 더 나은 성능을 달성하는 기술을 제안한다. Fine-tune-CoT[104]는 LLM들로부터 근거들을 생성하기 위해 기존의 제로 샷 CoT 프롬프트 기법들[139]을 활용한다. 그런 다음 이러한 근거는 더 작은 학생 모델을 미세 조정하는 데 사용된다. 또한 확률적 샘플링을 사용하여 교사 모델로부터 다양한 추론 솔루션을 생성하는 방법인 다양한 추론을 도입하여 학생 모델에 대한 학습 데이터를 풍부하게 하는 역할을 한다. SOCRATIC CoT[251]는 원래의 문제를 일련의 작은 작업으로 분해하고 이 분해를 활용하여 추론의 중간 단계를 지시하는 방법을 사용한다. 이 접근법은 한 쌍의 더 작고 증류된 모델을 훈련하는 데 사용되며, 하나는 문제를 전문적으로 해부하는 모델이고 다른 하나는 이러한 하위 문제를 해결하는 데 중점을 둔다. SCOTT [292]는 반사실적 추론 프레임워크 하에서 학생 모델을 트레이닝하기 위해 LLM들에 의해 생성된 유리들을 사용한다. 이 접근법은 학생 모델이 제공된 근거를 간과하지 않도록 하여 일관성 없는 예측을 하는 것을 방지한다. ScoTD[150]은 상징적 CoT 증류라고 불리는 방법을 제시한다. 여기에는 레이블이 지정되지 않은 데이터 인스턴스를 사용하여 LLM에서 CoT 근거를 그리는 것이 포함된다. 그런 다음 더 작은 모델이 샘플링된 유리와 관련 레이블을 모두 예측하도록 훈련된다. 마지막으로 Peng et al. [207]은 LLaMA와 같은 학생 LLM을 정교화하기 위해 영어 및 중국어 명령어 기반 데이터 세트를 생성하기 위해 GPT-4를 교사 모델로 활용한다. 그 결과 GPT-4에 의해 생성된 52K 데이터 포인트는 이전 최신 모델에서 생성된 명령어 추적 데이터에 비해 제로 샷 성능을 향상시킬 수 있음을 보여준다.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline
**Model** & **Parameter Size** & **Data Scale** & **GPUs Cost** & **Training Time** \\ \hline GPT-3 [20] & 175B & 300B tokens & - & - \\ GPT-NeX-20B [17] & 20B & 825GB corpus & 96 A100-40G & - \\ OPT [346] & 175B & 180B tokens & 992 A100-80G & - \\ BLOOM [239] & 176B & 366B tokens & 384 A100-80G & 105 days \\ GLM [336] & 130B & 400B tokens & 786 A100-40G & 60 days \\ LLaMA [276] & 65B & 1.4T tokens & 2048 A100-80G & 21 days \\ LLaMA-2 [277] & 70B & 2T tokens & A100-80G & 71,680 GPU days \\ Gopher [223] & 280B & 300B tokens & 1024 A100 & 13.4 days \\ LaMDA [273] & 137B & 768B tokens & 1024 TPU-v3 & 57.7 days \\ GLaM [69] & 1200B & 280B tokens & 1024 TPU-v4 & 574 hours \\ PanGu-\(a\)[337] & 13B & 1.1TB corpus & 2048 Ascend 910 & - \\ PanGu-\(\Sigma\)[233] & 1085B & 329B tokens & 512 Ascend 910 & 100 days \\ PaLM [51] & 540B & 780B tokens & 6144 TPU-v4 & - \\ PaLM-2 [7] & - & 3.6T tokens & TPUv4 & - \\ WeLM [258] & 10B & 300B tokens & 128 A100-40G & 24 days \\ Flan-PaLM [52] & 540B & - & 512 TPU-v4 & 37 hours \\ AlexaTM [254] & 20B & 1.3 tokens & 128 A100 & 120 days \\ Codegeex [357] & 13B & 850 tokens & 1536 Ascend 910 & 60 days \\ MPT-7B [272] & 7B & IT tokens & - & - \\ \hline \hline \end{tabular}
\end{table}
표 1: 대표적인 LLM의 사전 훈련 비용.

### Efficient Pre-Training

표 1과 같이 사전 훈련 LLM은 높은 비용을 발생시킨다. 효율적인 사전 훈련은 LLM 사전 훈련 과정의 효율성을 높이고 비용을 줄이는 것을 목표로 한다. 그림 6에 요약된 바와 같이, 효율적인 사전 훈련 기법은 혼합 정밀 가속도, 스케일링 모델, 초기화 기법, 최적화 전략의 4가지 범주로 그룹화할 수 있다.

**혼합 정밀 가속도.** 혼합 정밀 가속도는 전방 및 후방 전파에 대한 저정밀 모델을 사용하고 계산된 저정밀 기울기를 원래 고정밀 가중치를 업데이트하기 위한 고정밀 기울기로 변환함으로써 사전 훈련 효율성을 향상시킵니다. 예를 들어, Micikevicius 등(2018)은 업데이트를 위해 전체-정밀 FP32에서 가중치의 마스터 사본을 유지하기 위해 자동 혼합 정밀(AMP)을 제안하는 반면, 가중치, 활성화 및 그라디언트는 산술 연산을 위해 FP16에 저장된다. 특히, 개선된 버전의 AMP(Shen 등, 2017) 옵티마이저는 FP32 가중치의 카피를 제거했지만, 옵티마이저(AdamW)는 여전히 내부적으로 FP32를 사용한다. 그러나, Rae 등(2016)은 FP16이 정확도 손실을 초래한다는 것을 입증한다. 이러한 성능 저하에 대응하기 위해, 뇌 부동 소수점(Brain Floating Point; BF16)이 제안되었으며(Rae et al., 2016; Rae et al., 2016), 이는 지수에 더 많은 비트를 할당하고 유의미한 비트에 더 적은 비트를 할당함으로써 더 나은 성능을 달성한다. 마지막으로, 최근의 연구들(Shen et al., 2017; Wang et al., 2018)은 혼합-정밀 가속도와 활성화 압축 트레이닝(ACT)을 결합하는 것이 메모리-효율적인 트랜스포머 사전 트레이닝을 더욱 용이하게 할 수 있다는 것을 보여주었다.

**크기 조정 모델.** 크기 조정 모델을 기반으로 하는 기술은 작은 모델의 가중치를 사용하여 큰 모델까지 확장함으로써 사전 훈련 수렴을 가속화하고 훈련 비용을 줄입니다. 예를 들어, Gong 등(2018)은 Progressive Stacking을 도입하여 보다 간단한 모델에서 보다 복잡한 모델로 지식을 전달한 후 Progressive Stacking을 이용하여 모델의 훈련 효율과 수렴 속도를 향상시킨다. Yang et al. (2019)는 점진적 적층을 통해 모델의 깊이가 증가할수록 학습 속도는 감소함을 관찰한다. 이 문제를 해결하기 위해, 그들은 이전에 훈련된 계층을 변경하지 않고 출력과 새로 도입된 상위 인코더 계층만 업데이트하는 다단계 계층 훈련(MSLT)을 제안한다. 모든 계층이 훈련되면 MSLT는 각 계층을 전체 단계의 20%로 업데이트하여 전체 모델을 미세 조정하여 기존의 점진적 적층 접근법보다 시간 효율적이다. Gu et al.(2019)은 컴파운드그로우(CompoundGrow)를 소개하는데, 컴파운드그로우(CompoundGrow)는 작은 모델의 훈련에서 시작하여 입력 길이, 모델 폭 및 깊이를 증가시키는 것을 포함하여 모델 성장 기술의 혼합을 사용하여 점진적으로 확장하여 사전 훈련 과정에서 최대 82.2%의 가속화를 유도한다. Qin 등(2020)은 사전 훈련 동안 보조 감독으로서 지식 증류를 사용하는 지식 상속을 제안한다. 이는 더 작은 교사 모델로부터 더 큰 모델을 효과적으로 훈련하는 데 도움을 주어 사전 훈련의 속도와 일반화 능력을 모두 향상시킨다. Shen 등(2019)은 작은 모델로 시작하여 성장 연산자를 통해 그 깊이와 폭을 점진적으로 증가시키는 Staged Training을 소개하며, 이는 모델 파라미터, 최적화기의 상태를 포함하고,

도 6. LLMs에 대한 효율적인 사전-훈련 기술의 요약.

및 학습률 스케줄 중 적어도 하나를 포함하는 것을 특징으로 하는 방법. 이전 단계의 결과로 각 단계를 시작함으로써 계산을 효과적으로 재사용하여 보다 효율적인 학습 과정으로 이어진다. Chen 등 [31]은 더 작은 사전 훈련된 모델의 지식을 큰 모델에 전달하여 큰 모델의 사전 훈련 효율을 향상시키기 위해 기능 보존 초기화(FPI) 및 고급 지식 초기화(AKI)를 제안한다. 특히, FPI는 더 큰 모델에 더 작은 모델과 유사한 동작을 제공하여 최적화를 위한 강력한 기반을 마련하며, AKI는 더 높은 계층에서 가중치를 복제하여 더 빠른 수렴을 촉진한다. Wang et al. [291]은 아키텍처 지식을 포착하기 위해 Kronecker 인수분해로 더욱 강화된 너비 및 깊이-성장 연산자의 구성을 사용하여 더 큰 것을 개시하기 위해 더 작은 모델의 파라미터를 선형적으로 매핑하는 선형 성장 연산자(LiGO)를 제안한다. 망고[204]는 가속 능력을 부스팅하기 위해 타겟 모델의 각각의 가중치와 사전 훈련된 모델의 모든 가중치 사이의 선형 관계를 확립하는 기술을 도입한다. 또한 사전 훈련 시 계산량과 공간 복잡도를 줄이기 위해 다중 선형 연산자를 사용한다. 이러한 스케일링 기술과 점진적인 사전 훈련[327]을 바탕으로 최근의 FLM-101B[158]와 같은 LLM은 오프라인에서 모델 구조를 확장하고 이전 단계의 더 작은 모델 체크포인트에서 다시 시작하여 LLM 훈련 비용을 줄이는 성장 전략을 소개한다.

**초기화 기술.** 초기화는 LLM 사전 교육의 효율성을 높이는 데 중요한 역할을 합니다. 좋은 초기화는 모델의 수렴을 가속화할 수 있기 때문입니다. 대부분의 LLM은 [103; 141]과 같은 종래의 초기화 기술과 같이 더 작은 규모의 모델을 훈련하는 데 채택된 초기화 기술을 사용한다. 예를 들어, Kumar[141]에 의해 소개된 초기화 방법은 입력 및 출력 분산의 균형을 맞추는 것을 목표로 한다. 픽업[340] 및 ZerO[352]는 잔여 스템을 제로로 설정하여 신호 동일성을 보존한다. SkipInit [60]은 배치 정규화를 0-값 승수로 대체합니다. ReZero [11]은 동일성을 유지 하기 위해 0 값 매개 변수를 추가 하 여 더 빠른 수렴을 유도 합니다. T-Fixup[115]은 Fixup에 후속하여 트랜스포머 모델의 잔여 블록의 초기화를 위해 리스케일링 스킴을 채택한다. DeepNet[288]은 Post-LN-init를 사용하여 딥 트랜스포머에서 잔여 연결을 조정하여 안정적인 최적화를 위해 Layer-Normalization에 대한 안정적인 입력을 보장하고 gradient vanishing을 완화한다.

도 7: LLM에 대한 효율적인 사전-훈련 기술의 예시.

**최적화 전략.** GPT-3 [20], OPT [346], BLOOM [239] 및 Chinchilla [105]와 같은 인기 LLM은 주로 Adam [137] 또는 AdamW [178]을 최적화자로 사용하여 사전 훈련됩니다. 그러나, Adam과 AdamW 둘 다 메모리에 대한 엄청난 수요를 가지며 계산 비용이 많이 든다. 일부 연구[40; 165]는 LLM의 사전 훈련을 가속화하기 위한 새로운 최적화기를 제안한다. Chen 등[40]은 모델 훈련을 위한 최적화자들을 발견하기 위해 크고 희박한 프로그램 공간을 횡단하기 위해 탐색 기법들을 레버리지할 것을 제안한다. 라이온이라는 이름의 발견된 최적화기는 운동량만 추적하기 때문에 애덤보다 기억력이 더 뛰어나다. Liu et al. [165]는 사전 훈련 속도를 배가시키면서 Adam을 능가하는 경량 2차 최적화기로서 Sophia를 제안한다. 소피아는 기울기의 이동 평균과 추정된 헤시안(Hessian)을 계산하여 전자를 후자로 나누고 요소별 클리핑을 적용한다. 업데이트 크기를 효과적으로 조정하고 비볼록성 및 빠른 헤시안 변화를 해결하여 메모리 활용률과 효율성을 모두 향상시킵니다.

**시스템 수준 사전 훈련 효율성 최적화.** 메모리 및 컴퓨팅 리소스에 대한 높은 요구로 인해 LLM은 일반적으로 분산 방식으로 여러 컴퓨팅 노드에서 사전 훈련됩니다. 따라서, 시스템 차원에서 사전 훈련 효율을 향상시키기 위한 대부분의 기법들은 분산 훈련에 초점을 맞추고 있다. 일반적인 인공지능 모델 학습에 사용되는 기존의 효율적인 분산 학습 방법은 LLM 사전 학습에도 적용될 수 있다. 예를 들어, 데이터 병렬성[155; 241]은 트레이닝 데이터세트를 별개의 노드들 상의 다수의 서브세트들로 분할하는 것을 포함한다. 각 노드는 기울기를 독립적으로 계산한 다음 다른 노드와 공유하여 모델 매개변수를 업데이트합니다. 파이프라인 병렬성[117;196]은 입력 미니배치를 몇 개의 더 작은 배치들로 분할한 다음, 이러한 마이크로배치들의 실행을 다수의 GPU들에 걸쳐 분배한다. 텐서 병렬성[16; 197; 285; 317]은 다수의 노드들에 걸쳐 모델의 가중치 매트릭스들을 분할한다. 각 노드는 모델의 가중치 세그먼트를 사용하여 순방향 및 역방향 패스를 실행하는 역할을 하며 계산된 결과는 집계된다. 이러한 병렬화 기술들은 LLM들을 트레이닝하기 위한 컴퓨팅 및 메모리 제약들을 다루지만, 그라디언트들, 최적화기 상태들 및 활성화 상태들을 포함한 모든 런타임 상태들을 제한된 메모리에 피팅할 때 계산, 통신 및 개발 효율을 유지하는 데 여전히 제한된다. 이러한 갭을 메우기 위해, ZeRO(Zero Redundancy Data Parallelism) [226]은 상이한 노드들에 걸쳐 사전 트레이닝 동안 중간 상태들을 분할하기 위한 세 단계의 최적화를 제공한다. 구체적으로, ZeRO-1은 최적화기 상태만을 파티셔 닝하고, ZeRO-2는 최적화기 상태 및 그라디언트를 모두 파티셔 닝한다. ZeRO-1과 ZeRO-2는 모두 데이터 병렬성에 비해 런타임 메모리를 줄이는 반면, 데이터 병렬성과 동일한 통신 볼륨만 소비한다. ZeRO-3는 ZeRO-1과 ZeRO-2에 비해 모델 파라미터를 노드 간에 분할하는 보다 적극적인 분할을 제공한다. 런타임 메모리는 ZeRO-3를 통해 더욱 감소하지만, 이 단계에서는 통신 오버헤드가 50% 정도 증가한다. 따라서 노드 간에 ZeRO-1과 ZeRO-2를 사용하는 동안 통신 시간을 최소화하기 위해 노드 내에서 ZeRO-3를 사용하는 것이 좋다. 완전 샤딩 데이터 병렬(Fully Shard Data Parallel; FSDP)[355]은 최적화를 위한 유사한 아이디어를 공유하며, 사용자가 구배, 파라미터 및 최적화기 상태를 서로 다른 노드에 걸쳐 분할할 노드 또는 프로세스를 정의할 수 있도록 하이브리드 샤딩 전략을 설계한다. 가중치 메모리가 모든 컴퓨트 노드에 의해 제공될 수 있는 집계된 메모리를 초과하는 경우, ZeRO-Offload[231]은 ZeRO의 임의의 스테이지에 대해 CPU로의 오프로딩을 가능하게 하고, ZeRO-Infinity[227]은 CPU 메모리 외에 NVMe 드라이브로 오프로딩하는 방법을 제공한다. 그러나 CPU와 GPU 사이의 데이터 이동이 느리기 때문에 이 두 가지 대안을 사용하여 성능을 유지하는 것은 상당히 어렵다.

### Efficient Fine-Tuning

효율적인 미세 조정은 LLM에 대한 미세 조정 프로세스의 효율성을 향상시키는 것을 목표로 한다. 도 8에 도시된 바와 같이, 효율적인 미세 조정 방법들은 파라미터-효율적인 미세 조정(PEFT) 및 메모리-효율적인 미세 조정(MEFT)으로 그룹화될 수 있다.

#### 2.3.1. **파라미터 효율적인 Fine-Tuning**

파라미터 효율적인 미세 조정(PEFT)은 전체 LLM 백본을 동결하고 작은 추가 매개변수 세트만 업데이트하여 LLM을 다운스트림 작업에 적응시키는 것을 목표로 한다. 일반적으로 PEFT 방법은 어댑터 기반 튜닝, 하위 순위 적응, 프리픽스 튜닝, 프롬프트 튜닝의 네 가지 범주로 그룹화할 수 있다.

**어댑터 기반 튜닝.** 어댑터는 LLM에 통합 된 병목 같은 훈련 가능한 모듈로, 먼저 입력 특징 벡터를 다운 프로젝션 한 다음 비선형 계층을 사용 하 고 원래 크기로 다시 업 프로젝션 합니다. [108]. 어댑터 기반 튜닝에는 직렬 어댑터와 병렬 어댑터가 모두 포함됩니다. 직렬 어댑터에서 각 LLM 레이어에는 주의력 및 피드포워드 모듈 뒤에 추가된 두 개의 어댑터 모듈이 있으며 병렬 어댑터는 LLM의 각 레이어 내에서 주의력 및 피드포워드 모듈과 함께 두 개의 어댑터 모듈을 배치한다. 특히, Hu 등 [113]은 서로 다른 작업에 대한 미세 조정을 위해 직렬 또는 병렬 어댑터를 LLM에 통합하는 LLM-Adapters를 제안한다. Karimi Mahabadi 등[130]은 어댑터, 로우 랭크 기술을 통합하는 Compacter를 제안하며,

도 8. LLMs에 대한 효율적인 미세 조정 방법의 요약.

도 9. 파라미터-효율적인 Fine-Tuning(a)-(d) 및 메모리-효율적인 Fine-Tuning(e)의 예시.

그리고 훈련 가능한 파라미터의 양과 태스크 성능 사이의 균형 잡힌 균형을 달성하기 위해 최신 하이퍼 복소 곱셈 계층을 포함한다. (IA)\({}^{3}\)[166]에서는 학습된 벡터들을 이용하여 활성화들을 스케일링하는 기법을 도입하였는데, 이는 정확도와 계산 효율 면에서 적은 수의 문맥 내 학습(ICL)보다 우수한 성능을 보인다. 메타-학습 원리에 따라, 메타-어댑터[12]는 메타-학습된 어댑터 계층들을 사전-학습된 모델에 통합하고, 고정된 사전-학습된 모델을 효율적인 소수의-샷 학습 프레임워크로 변환하는 소수의-샷 시나리오에 대한 자원 효율적인 미세 조정 기술을 설계한다. AdaMix[298]는 희소하게 활성화된 MoE(mixture-of-experts) 모델 [365]로부터 영감을 받아 주어진 태스크의 다수의 뷰를 학습하기 위한 적응 모듈의 혼합물을 제안한다. 마지막으로 OpenDelta [112]는 다양한 어댑터 기반 기술을 구현하기 위한 다기능 및 플러그 앤 플레이 프레임워크를 제공하는 오픈 소스 소프트웨어 라이브러리로 다양한 LLM 아키텍처와 호환되도록 설계되었다.

**Low-Rank Adaptation.** Low-Rank Adaptation (LoRA)[111]은 LLM에 대해 널리 사용 되는 PEFT 접근 방식입니다. LoRA는 가중치 행렬 \(\mathbf{W}\in\mathbb{R}^{m\times n}\)을 \(\mathbf{W}\leftarrow\mathbf{W}+\Delta\mathbf{W}\)으로 직접 조정하는 대신 훈련 가능한 저순위 행렬 \(\mathbf{A}\in\mathbb{R}^{m\times r}\)과 \(\mathbf{B}\in\mathbb{R}^{r\times n}\)을 도입하여 \(\Delta\mathbf{W}\)을 \(\Delta\mathbf{W}=\mathbf{A}\cdot\mathbf{B}\)로 표현한다. 이와 같이 미세조정 시 작은 행렬 \(\mathbf{A}\)과 \(\mathbf{B}\)만 갱신되는 반면, 원래 큰 가중치 행렬은 동결 상태로 유지되어 미세조정 과정이 보다 효율적이다. 효과적이지만 LoRA는 모든 단일 미세 조정 반복에서 LLM의 모든 레이어에 대한 하위 순위 행렬의 모든 매개변수를 업데이트해야 한다. LoRA의 효율성을 높이기 위해 LoRA-FA [342]는 각 LoRA 어댑터에 \(\mathbf{A}\)의 프로젝션-업 가중치를 갱신하면서 \(\mathbf{B}\)의 프로젝션-다운 가중치를 일정하게 유지하여 미세 조정 시 가중치 수정이 낮은 순위 공간에 국한되도록 하여 전체 순위 입력 활성화를 저장할 필요가 없게 한다. 로라허브[114]는 서로 다른 작업에 걸쳐 일반화하기 위한 목적으로 LoRA의 합성성을 탐구한다. 기존에 볼 수 없었던 작업에 대해 좋은 성능을 얻는 것을 목표로 다양한 작업에 대해 훈련된 LoRA 모듈을 결합한다. LongLoRA[42]는 LoRA를 Long-context fine-tuning 시나리오까지 확장한다. 이는 문맥 확장을 효과적으로 촉진하는 shift short attention (S\({}^{2}\)-Attn)을 도입하여 LoRA가 훈련 가능한 임베딩과 정규화를 활용할 때 긴 문맥에 효과적임을 보여준다. MHR(Multi-Head Routing) [23]은 LoRA를 MoE(Mixture-of-Experts) 아키텍처로 확장한다. 유사한 매개변수 할당으로 작동할 때 폴리트로폰[213]보다 성능이 뛰어납니다. 특히, 어댑터를 조정하지 않고 라우팅 기능만 미세 조정하는 데 초점을 맞추면서 경쟁력 있는 성능을 달성하여 현저한 매개변수 효율성을 보여준다. Zhang 등[344]은 많은 PEFT 기술들이 다양한 가중치 파라미터들의 상이한 중요성을 무시한다는 것을 관찰한다. 이를 해결하기 위해, 이들은 증분 업데이트를 파라미터화하기 위해 특이값 분해를 사용하고 각 가중치 행렬의 중요도 점수를 기반으로 파라미터 예산을 적응적으로 분배하는 AdaLoRA를 제안한다. Valipour 등[279]은 LoRA에서의 랭크가 정적이고 미세 조정 동안 적응적으로 조정될 수 없음을 식별한다. 이 문제를 해결하기 위해, 그들은 어댑터 모듈이 학습한 표현을 랭크에 따라 정리하여 하나가 아닌 여러 랭크에 LoRA 블록을 훈련시키는 동적 하위 랭크 적응 방법을 도입하는 딜로라를 제안한다. PEFT를 주로 풀-사이즈 LLM들에 적용하는 전술한 방법들과는 달리, CEPT[353]는 압축된 LLM들을 활용하는 새로운 프레임워크를 도입한다. 구체적으로, LLM 압축 방법이 PEFT 성능에 얼마나 널리 영향을 미치는지 평가하고 후속적으로 이러한 압축 기술에 의해 유도된 지식 손실을 방지하기 위해 지식 유지 및 복구를 위한 전략을 구현한다. 또한, Tied-LoRA[234]는 LoRA의 파라미터 효율을 더욱 증가시키기 위해 가중치 묶음과 선택적 훈련을 사용한다.

**접두사 조정.** 접두사 조정 [159]는 LLM의 각 계층에 접두사 토큰으로 알려진 일련의 훈련 가능한 벡터를 추가합니다. 이러한 프리픽스 토큰은 특정 작업에 맞게 조정되며 가상 단어 임베딩으로 취급될 수 있다. LLaMA-어댑터[345]는 트레이닝가능한 적응 임베딩들의 세트를 통합하고, LLM들의 상위 계층들에서의 워드 임베딩들에 그것들을 부착한다. 또한 제로 게이팅(zero gating)을 이용한 제로-초기화 어텐션(zero-initialized attention) 기법을 소개한다. 미리 훈련된 지식을 유지하면서 새로운 안내 신호를 LLaMA-1에 동적으로 통합한다.

**프롬프트 튜닝.** 접두사 튜닝과 달리 프롬프트 튜닝은 입력 계층에서 훈련 가능한 프롬프트 토큰을 통합합니다. 이러한 토큰은 접두사로 삽입되거나 입력 토큰 내의 모든 위치에 삽입될 수 있습니다. Soft Prompt [146]은 각 다운스트림 작업에 대한 입력 텍스트의 시작 부분에 추가 \(k\) 훈련 가능한 토큰을 추가하면서 사전 훈련된 모델 전체를 고정 상태로 유지합니다. 전체 모델 미세 조정에 비해 적은 샷 프롬프트를 능가하고 성능 격차를 좁힙니다. P-튜닝[173]은 프롬프트로서 소수의 파라미터들을 활용하는데, 프롬프트 인코더에 의해 처리되고, 프롬프트 인코더는 미리 트레이닝된 LLM들에 대한 입력으로서 사용된다. 개별 프롬프트를 검색하는 대신 P-튜닝은 기울기 하강을 통해 이러한 프롬프트를 미세 조정하고 광범위한 NLU 태스크에서 성능을 향상시킨다. Liu 등[171]은 프리픽스 튜닝의 이전 버전들이 복잡한 시퀀스 라벨링 작업들과 투쟁한다는 것을 관찰한다. 이를 해결하기 위해 입력 계층에만 있는 것이 아니라 사전 훈련된 모델의 각 계층에 연속 프롬프트를 도입하여 프리픽스 튜닝을 향상시키는 P-튜닝 v2를 제안한다. 이 수정은 자연 언어 이해와 관련된 작업에 대해 다양한 파라미터 크기에 걸쳐 성능을 향상시키는 데 효과적인 것으로 입증되었다. Tam 등 [265]는 텍스트 검색을 위한 효율적인 프롬프트 튜닝을 도입하고, 단지 0.1%의 파라미터를 업데이트하며, 다양한 도메인에서 전통적인 풀-파라미터 업데이트 방법을 능가한다. Sun et al. [261]은 프롬프트 튜닝이 소수의 샷 학습 시나리오에서 어려움을 겪는 경향이 있다고 주장하며, 따라서 멀티태스크 학습을 사용하여 모듈러 프롬프트 컬렉션을 미리 트레이닝하는 MP\({}^{2}\)을 제안한다. 그런 다음 이러한 프롬프트는 특정 작업에 대해 훈련 가능한 라우팅 메커니즘에 의해 선택적으로 트리거되고 조립된다. 결과적으로 MP\({}^{2}\)는 미리 훈련된 모듈러 프롬프트를 병합하고 재사용하는 방법을 학습함으로써 다운스트림 작업에 빠르게 적응할 수 있다. PPT [93]은 MP\({}^{2}\)과 달리, 적은 샷 학습에서 프롬프트 튜닝의 성능 저하를 소프트 프롬프트의 초기화가 불량한 탓으로 보고, 더 나은 초기화를 위해 사전 훈련 단계에 소프트 프롬프트를 추가하는 것을 제안한다. 멀티태스크 프롬프트 튜닝[300]은 멀티태스크 학습 설정에서 프롬프트 벡터의 사용을 통해 다양한 태스크의 지식을 활용한다. 특히 처음에는 다양한 작업별 원본 프롬프트에서 지식을 추출하여 전달 가능한 단일 프롬프트를 학습한 다음, 이 프롬프트에 곱셈 하위 순위 업데이트를 적용하여 각 다운스트림 작업에 대해 효과적으로 조정합니다. 이를 통해 멀티태스크 프롬프트 튜닝은 완전 미세 튜닝 방법에 비해 경쟁력 있는 성능 수준을 달성할 수 있다.

#### 2.3.2. **메모리 효율적인 미세 조정**

LLM의 파라미터가 확장됨에 따라 미세 조정에 필요한 메모리의 크기도 증가하여 미세 조정에 메모리가 중요한 장애물이 되었다. 결과적으로, 효율 향상을 위한 미세 조정에서 메모리 사용을 최소화하는 것 또한 중요한 화두로 떠올랐다. Dettmers 등[62]은 먼저 모델을 4-비트 NormalFloat 데이터 타입으로 양자화하는 QLoRA를 제안하고, 이어서 이 양자화된 모델을 LoRA(low-rank adapter) 가중치들을 부가하여 미세 조정한다[111]. 그렇게 함으로써, QLoRA는 표준 풀-모델 미세-튜닝에 비해 성능 저하 없이 미세-튜닝 동안 메모리 사용을 감소시킨다. QA-LoRA[318]는 적응 파라미터들을 감소시키면서 양자화 유연성(각 그룹은 개별적으로 양자화됨)을 향상시키는 그룹-와이즈 연산자들을 도입함으로써 QLoRA를 개선한다(각 그룹은 공유된 적응 파라미터들을 이용한다). 유사하게, LoftQ[160]는 모델 양자화와 특이값 분해(SVD)를 결합하여 원래의 고정밀 사전 훈련된 가중치를 근사화한다. 결과적으로, 후속 LoRA 미세 조정에 유리한 초기화 포인트를 제공하여 QLoRA보다 향상된다. PEQA[133]는 양자화 인식 미세 조정을 위한 2단계 접근법을 소개한다. 첫 번째 단계에서는 각 완전 연결 계층에 대한 파라미터 행렬을 스칼라 벡터와 함께 저비트 정수의 행렬로 양자화한다. 두 번째 단계에서는 하위 비트 행렬은 변경되지 않고 유지되는 반면 미세 조정은 각 특정 다운스트림 작업에 대한 스칼라 벡터에만 집중된다. 이 2단계 접근법을 사용하여 PEQA는 미세 조정 동안 메모리 사용을 최소화할 뿐만 아니라 가중치를 낮은 비트 양자화된 형태로 유지함으로써 추론 시간을 빠르게 한다. Simoulin 등(2019)은 계산된 그라디언트가 0이 아닌 순방향 패스로부터 중간 액티베이션들의 서브세트를 구체적으로 보존함으로써 메모리 사용을 최소화하는 선택적 Fine-Tuning을 제안한다. 특히, 이 접근법은 그렇지 않으면 필요한 GPU 메모리의 최대 3분의 1만 사용하는 동안 완전한 미세 조정과 동등한 성능을 제공한다. Lv 등(2019)은 기울기 계산과 파라미터 갱신을 하나의 단계로 결합하여 미세 조정 시 메모리 소모를 최소화하는 LOMO를 소개한다. 이와 같이 LOMO는 최적화기 상태의 모든 구성 요소를 제거하여 기울기 텐서에 대한 메모리 요구 사항을 \(O(1)\)으로 낮춘다. MeZO(2019)는 두 번의 전진 패스만을 사용하여 기울기 추정을 위해 영차법(Zhou et al., 2019)을 개선한다. 이는 추론과 유사한 메모리 요구 사항을 갖는 LLM의 효율적인 미세 조정을 가능하게 하고 LoRA(2019) 및 프리픽스 튜닝(Luo et al., 2020)과 같은 풀-파라미터 및 PEFT 방법 모두를 지원하여, MeZO가 단일 A100 80GB GPU 상에서 300억 파라미터 모델을 트레이닝할 수 있게 한다.

### Efficient Inference

효율적인 추론은 LLM에 대한 추론 과정의 효율성을 높이는 것을 목표로 한다. <그림 10>에 정리된 바와 같이 효율적인 추론 기법은 알고리즘 수준과 시스템 수준의 기법으로 그룹화할 수 있다.

**알고리즘 수준 추론 효율성 최적화** 알고리즘 수준에서 LLM 추론 효율성을 향상 시키는 기술에는 추측 디코딩 및 KV-캐시 최적화가 포함 됩니다.

* _Speculative Decoding._ 투기적 디코딩(즉, 투기적 샘플링)(Srivastava et al., 2017)은 더 큰 타겟 모델에 대한 투기적 프리픽스를 생성하기 위해 더 작은 드래프트 모델을 사용하는 것을 통해 병렬 토큰 계산에 의해 샘플링을 고속화하는 자기회귀 언어 모델에 대한 디코딩 전략이다. Chen et al. (2019)는 더 빠른 자기회귀모형 \(K\)을 실행한 후 큰 목표 LLM으로 예비 출력을 평가할 것을 제안한다. 상기 맞춤형 거절 샘플링 전략은,

도 11. LLM 추론을 위한 알고리즘-레벨 효율 최적화 기술의 예시.

도 10. LLMs에 대한 효율적인 추론 기술의 요약.

취업자는 초안 토큰을 왼쪽에서 오른쪽으로 선택하는 것을 승인하여 절차 중에 대상 모델의 분포를 다시 포착한다. 스테이지 추측 [256]은 추측 배치를 잠재적인 토큰 시퀀스를 나타내는 트리 구조로 변환합니다. 이 구조조정은 더 크고 개선된 투기 배치의 생성을 촉진하는 것을 목표로 한다. 초기 모델의 추측 디코딩을 위한 추가 단계를 도입하여 전체 성능을 향상시킨다. BiLD[135]는 두 가지 혁신적인 전략을 통해 추측 디코딩을 최적화한다 : 더 작은 드래프트 모델이 충분한 신뢰도가 부족할 때 더 큰 타겟 모델에 대한 제어를 포기할 수 있게 하는 폴백 정책 및 더 작은 드래프트 모델에 의해 이루어진 임의의 부정확한 예측을 재방문하고 수정할 수 있게 하는 롤백 정책. SpecInfer [187]은 추측 추론 기술과 토큰 트리 유효성 검사를 사용 하 여 추론을 가속화 합니다. 핵심 아이디어는 LLM의 출력을 공동으로 예측하기 위해 집합적으로 미세 조정된 다양한 소규모 추측 모델을 병합하는 것을 포함하며, 이는 모든 예측을 검증하는 데 사용된다. LLMA[323]는 밀접하게 관련된 참조로부터 텍스트 세그먼트를 선택하고 그 토큰들을 디코더로 복제한다. 그런 다음 단일 디코딩 단계 내에서 디코딩 출력으로서 이들 토큰의 적합성을 동시에 평가한다. 이 접근법은 전통적인 그리디 디코딩과 동일한 생성된 결과를 유지하면서 LLM에 대해 2배 이상의 속도 증가를 초래한다. 메두사 [24]는 LLM 백본을 동결하고 추가 헤드를 미세 조정하고 트리 기반 주의 메커니즘을 사용하여 예측을 병렬 처리하여 디코딩 프로세스를 가속화하는 것을 포함한다. 마지막으로, Santilli 등[238]은 추측 디코딩을 위한 Jacobi와 Gauss-Seidel 고정 소수점 반복 방법을 포함하는 병렬 디코딩을 제안한다. 이 중 Jacobi 디코딩은 LLM의 효율성을 높이기 위해 Lookahead 디코딩[81]으로 확장되었다.
* _KV-Cache Optimization._ Minimizing the repeated computation of Key-Value (KV) pairs during the inference process of LLMs is also key to enhancing the inference efficiency. Corro et al. [53] propose SkipDecode, a token-level early exit approach that utilizes a unique exit point for each token in a batch at every sequence position, and skips the lower and middle layers to accelerate the inference process. Zhang et al. [350] point out that KV-cache is scaling linearly with the sequence length and batch size. They propose a KV cache eviction strategy that formulates the KV cache eviction as a dynamic sub-modular problem and dynamically retains a balance between recent and important tokens, reducing the latency for LLMs inference. Dynamic Context Pruning [6] utilizes a learnable mechanism to identify and remove non-informative KV-cache tokens. In doing so, it not only enhances efficiency but also improves interpretability. Liu et al. [175] underscore the Persistence of Importance Hypothesis, suggesting that only tokens that were crucial at an earlier phase will have a significant impact on subsequent stages. Based on this theory, they propose Scissorhands that introduces a streamlined algorithm for LLM inference using a compact KV-cache.

**시스템 수준 추론 효율성 최적화** LLM 추론의 효율성은 시스템 수준에서도 최적화될 수 있습니다. 예를 들어, FlexGen[248]은 제한된 메모리를 갖는 GPU들 상에서 LLM들의 실행을 가능하게 하는 고처리량 추론 엔진이다. 선형 프로그래밍 기반 검색 접근법을 사용하여 GPU, CPU 및 디스크의 메모리와 계산을 결합하여 다양한 하드웨어를 조정한다. 또한 FlexGen은 가중치 및 주의 캐시를 4비트로 양자화하여 단일 16GB GPU에서 OPT-175B [346]의 추론 속도를 높인다. Deja Vu[177]는 밀집 모델과 동일한 결과를 생성하지만 더 적은 컴포넌트를 갖는 MLP 및 주의 모듈의 집합인 문맥 희소성의 개념을 제시한다. 이 기법은 예측자를 학습시켜 희소성을 식별한 후 커널 융합과 메모리 병합을 이용하여 추론 과정을 빠르게 한다. Pope 등[214]은 애플리케이션 요건에 기초하여 TPU v4 슬라이스에 최적화된 최상의 다차원 분할 방법을 선택하기 위한 간단한 분석 프레임워크를 개발한다. 이를 기존의 일부 하위 수준 최적화와 결합하여 FasterTransformer [199] 표준에 비해 PaLM [51]에서 더 큰 효율성을 달성했다. S\({}^{3}\)[126]은 출력 시퀀스를 미리 알고 있는 시스템을 만들었다. 시퀀스의 길이를 예측하고 그에 따라 생성 요청을 배열하여 장치 자원의 활용을 최적화하고 생산 속도를 높일 수 있다. Orca [332]는 반복 수준 스케줄링을 사용 하 여 배치 크기를 결정 합니다. 배치 내의 시퀀스가 완료되면 새로운 것으로 대체되어 정적 배칭에 비해 GPU 활용도가 향상된다. DeepSpeed-Inference [5]는 집합 GPU 메모리 내에 포함될 때 조밀 및 희소 트랜스포머 모델의 효율성을 모두 향상시키도록 설계된 다중 GPU 추론 접근법이다. 또한 GPU 메모리와 연산 외에도 CPU와 NVMe 메모리를 활용하는 혼합 추론 기법을 제공하여, 너무 커서 결합된 GPU 메모리에 적합하지 않은 모델에도 높은 처리량 추론을 보장한다. 플래시-디코딩[59]은 키들/값들을 더 작은 조각들로 분해하고, 이들 조각들에 대한 주의를 병렬로 컴퓨팅한 다음, 이들을 조합하여 최종 출력을 생성함으로써 긴-맥락 추론의 속도를 향상시키는 기술이다. FlashDecoding++[106]은 비동기 소프트맥스, 플랫 GEMM 최적화를 위한 이중 버퍼링, 휴리스틱 데이터 플로우를 통해 주류 언어 모델 및 하드웨어 백엔드를 지원하여 허깅페이스 구현과 비교하여 NVIDIA 및 AMD GPU에서 각각 최대 4.86배 및 2.18배 가속을 초래한다.

효율적인 아키텍처 설계

LLM에 대한 효율적인 아키텍처 설계는 자원 소비를 최소화하면서 성능 및 확장성을 향상시키기 위한 모델 아키텍처 및 계산 프로세스의 전략적 최적화를 의미한다. 그림 12는 LLM에 대한 효율적인 아키텍처 설계를 요약한 것이다.

도 12. LLMs에 대한 효율적인 아키텍처 설계의 요약.

#### 2.5.1. **효율적인 주의**

어텐션 모듈의 2차 시간 및 공간 복잡도는 LLM의 사전 훈련, 추론 및 미세 조정을 상당히 느리게 한다[132]. 보다 효율적인 실행을 위해 주의를 가볍게 하기 위해 많은 기술들이 제안되었다. 이러한 기법들은 일반적으로 공유 기반 주의력, 특징 정보 감소, 커널화 또는 하위 순위, 고정 패턴 전략, 학습 가능한 패턴 전략 및 하드웨어 지원 주의력으로 분류될 수 있다.

**공유 기반 주의** 공유 기반 주의는 서로 다른 KV 헤드 공유 방식을 통해 추론하는 동안 주의 계산을 가속화하는 것을 목표로 합니다. 예를 들어, LLaMA-2[277]는 다중-쿼리 주의(MQA)[245] 및 그룹화된-쿼리 주의(GQA)[3]를 사용하여 자기회귀 디코딩 프로세스를 최적화한다. 질의, 키, 값 및 출력에 대해 별개의 선형 변환과 동시에 여러 주의 계층(헤드)을 사용하는 멀티 헤드 주의와 대조적으로, MQA는 하나의 키 및 값 세트를 공유하는 모든 헤드를 갖는다. MQA는 단 하나의 키-값 헤드를 사용하여 디코더 추론을 가속화하지만, 이는 품질을 손상시킬 수 있다. 이를 해결하기 위해, GQA는 추론 품질을 향상시키기 위해 두 개 이상의 키-값 헤드를 사용하지만 전체 쿼리 헤드의 수보다 적은 수의 MQA를 수정하여 제공한다.

**특성 정보 감소.** Funnel-Transformer [55], Nystromformer [314], Set Transformer [144]와 같은 모델에서 알 수 있듯이 특성 정보 감소의 원리는 시퀀스 내에서 특성 정보를 감소시켜 계산 요구를 줄이는 것이며, 이는 필요한 계산 리소스의 비례적인 감소로 이어집니다. 예를 들어, Funnel-Transformer[55]는 은닉 상태들의 시퀀스 길이를 감소시켜 계산 비용을 감소시키는 반면, 그의 디코더는 이 압축된 시퀀스로부터 각각의 토큰에 대한 깊은 표현들을 재구성할 수 있다.

**커널화 또는 낮은 순위.** Summermer [4], FluRKA [96], Scatterbrain [28], Low-Rank Transformer (LRT) [304], Performer [50], Random Feature Attention (RFA) [209], Linear Transformer [131], Linformer [293]와 같은 모델에 채택 된 커널화 또는 낮은 순위 기술은 자체 주의 매트릭스의 낮은 순위 표현을 활용 하거나 주의 커널화 기술을 채택 하 여 계산 효율성을 향상 합니다. 구체적으로, 저순위 메소드는 어텐션 키 및 값의 치수를 압축하는 데 중점을 둔다. 예를 들어, Linformer[293]는 선형 투영을 통해 스케일링된 도트-곱 주의를 더 작은 단위로 분할할 것을 제안한다. 저순위 기법의 변형인 커널화는 주의행렬의 근사화에 초점을 맞춘다[49]. 예를 들어, 연주자[50]

도 13. 주의 최적화들의 예시.

양의 직교 랜덤 피쳐를 사용하여 소프트맥스 주의-커널을 응축합니다. Summermer [4]는 등변수 수열 대 수열 함수에 근사하여 Linformer와 Performer 모두에 대한 보편적인 해를 제공한다.

**고정 패턴 전략.** [203], Big Bird [335], Poolingformer [341], Longformer [13], Blockwise Transformer [221], Sparse Transformer [48]와 같은 모델이 채택한 고정 패턴 전략은 주의 매트릭스를 희소화하여 효율성을 향상시킵니다. 이는 주의 범위를 로컬 윈도우들 또는 고정-스트라이드 블록 패턴들과 같은 미리결정된 패턴들에 한정함으로써 달성된다. 예를 들어, 종래의 자기 주의력의 대안으로 설계된 Longformer [13]의 주의력 메커니즘은 특정 작업에 맞춤화된 글로벌 지향 주의력과 로컬 윈도우 주의력을 병합한다. Pagliardini 등[203]은 플래시어텐션[58]을 확장하여 키-쿼리 드롭핑 및 해싱 기반 어텐션 기술을 포함하는 광범위한 스펙트럼의 어텐션 희소성 패턴을 지원하였다.

**학습 가능한 패턴 전략.** HyperAttention [99], Reformer [138], Sparse Sinkhorn Attention [268], Clustered Attention [281], ClusterFormer [290], Routing Transformer [235]와 같은 모델에 의해 채택 된 학습 가능한 패턴 전략은 토큰 관련성을 학습 하 고 토큰을 버킷 또는 클러스터로 그룹화 하 여 효율성을 향상 합니다. 예로서, 하이퍼어텐션[99]은 스펙트럼 근사화를 위한 파라미터화를 제안하며, 정규화된 어텐션 매트릭스에서 최대 열 노름과 큰 엔트리 제거 후 정규화되지 않은 매트릭스에서 행 노름 비율의 두 가지 주요 메트릭을 채용한다. 또한 학습 가능한 정렬 지역성 민감 해싱(sortLSH) 기법과 행 norm 샘플링을 통한 빠른 행렬 곱셈을 활용한다. 그들의 실험 결과는 하이퍼어텐션이 최소한의 성능 저하만으로 LLM에 대한 추론과 훈련 속도를 모두 향상시킨다는 것을 보여준다.

**하드웨어 지원 주의** 주의를 희소화하여 주의 매트릭스 계산을 간소화하는 알고리즘 접근법 외에도 여러 연구는 하드웨어 측면에서 효율적이고 가벼운 주의 메커니즘을 실현하는 데 중점을 둡니다. 예를 들어, FlashAttention[58]과 FlashAttention-2[57]는 LLM에서 주의 모듈을 계산할 때 GPU HBM(high-bandwidth memory)과 GPU on-chip SRAM 간의 통신 시간을 줄이는 것을 목표로 한다. 플래시어텐션은 표준 어텐션 메커니즘에서와 같이 HBM과 SRAM 사이에 값과 결과를 여러 번 전송하는 대신 모든 어텐션 연산을 하나의 커널로 결합하고 가중치 행렬을 작은 블록으로 타일링하여 작은 SRAM에 더 잘 맞도록 한다. 그 결과, 각각의 어텐션 블록을 처리하기 위해 오직 하나의 통신만이 요구되어, 전체 어텐션 블록을 처리하기 위한 효율을 상당히 증가시킨다. 가상 메모리 및 페이징 기술에 의해 영감을 받은 PagedAttention[142]는 연속된 키 및 값을 비연속적인 메모리 공간에 저장할 수 있게 한다. 구체적으로, PagedAttention은 각각의 시퀀스의 KV 캐시를 블록들로 분할하며, 각각은 고정된 수의 토큰들에 대한 키들 및 값들을 포함한다. 어텐션 계산 동안, 페이지드 어텐션 커널은 메모리 단편화를 감소시키기 위해 블록 테이블을 유지함으로써 이러한 블록들을 효율적으로 관리한다. 구체적으로, 시퀀스의 연속적인 논리 블록들은 테이블을 통해 비연속적인 물리 블록들에 매핑되고, 테이블은 새로 생성된 모든 토큰에 대해 새로운 물리 블록을 자동으로 할당한다. 이것은 새로운 토큰들을 생성할 때 낭비되는 메모리의 양을 감소시키며, 따라서 그 효율성을 향상시킨다. A\({}^{3}\)[97]은 키의 수를 줄이는 혁신적인 후보 선택 프로세스를 소개하고 병렬 처리를 통해 근사화된 주의 기술을 빠르게 하여 효율성을 더욱 향상시키는 맞춤형 하드웨어 파이프라인을 제공한다. ELSA[98]는 Kronecker 분해를 사용하여 어텐션 모듈을 근사화하는데, 이는 그 복잡도를 감소시킬 뿐만 아니라 하드웨어 상의 병렬화에 더 적합하게 하여 추론에 사용될 때 더 효율적이다.

#### 2.5.2. **MoE(전문가 혼합)**.

혼합 전문가(MoE)는 LLM과 같은 대규모 모델에서 두드러지게 활용되는 희소 방법론을 나타낸다. 지정 된 작업을 여러 하위 작업으로 분할 한 다음 _전문가_ 라고 하는 여러 개의 더 작고 전문화된 모델을 개발 하 고 각각 고유한 하위 작업에서 연마 하는 원리로 작동 합니다. 그 후, 이 전문가들은 통합된 산출물을 제공하기 위해 협력합니다. 사전 학습 또는 미세 조정을 위해 MoE는 계산 및 메모리 요구 사항을 비교적 관리 가능하게 유지하면서 모델의 용량 및 잠재적으로 성능을 향상시키면서 막대한 수의 매개변수를 효율적으로 관리하는 데 도움이 된다. 추론을 위해 MoE는 모든 전문가를 동시에 참여시키는 것이 아니라 선택된 소수의 전문가만을 활성화함으로써 추론 시간을 감소시킨다. 추가적으로, MoE는 각각의 전문가를 개별 가속기에 할당함으로써 모델-분산 시나리오에서 장치들 사이의 통신을 최소화할 수 있다; 통신은 라우터를 호스팅하는 가속기들과 관련 전문가 모델 사이에서만 필요하다[128].

**MoE 기반 LLMs.** 여러 MoE 기반 LLMs가 제안 되었습니다. 예를 들어, GShard [145]는 MoE 기반 LLM으로서, 기존의 모델 코드에 약간의 수정을 가하여 다양한 병렬 계산 프레임워크를 명료화하는 정제된 방법을 제공한다. 또한 자동 샤딩을 통해 6,000억 개의 파라미터를 넘어 Sparsely-Gated MoE로 다국어 신경망 기계 번역 Transformer 모델을 증폭한다. 스위치 트랜스포머[75]는 스위치 라우팅 알고리즘을 제시하고 직관적으로 향상된 모델을 수정하여 통신 및 계산 비용을 낮춘다. 최대 1조 개의 파라미터를 포괄하여 최대 2,048명의 전문가들 사이에서 작업을 나누어 MoE 프레임워크의 확장성과 효율성을 보여준다. Artetxe 등 [8]은 희소 언어 모델을 1.1T 파라미터로 스케일링하여, 밀집 모델에 비해 언어 모델링, 제로 샷 및 소수의 샷 학습에서 이 스케일까지 우수한 성능을 식별한다. 이는 희소 MoE 모델이 전통적으로 사용되는 고밀도 아키텍처를 계산적으로 효율적으로 대체한다는 것을 시사한다. BASE Layer[148]는 토큰 대 전문가 할당을 선형 할당 문제로 정의하여, 각 전문가가 동일한 수의 토큰을 획득하는 최적의 할당을 허용한다. PanGu-\(\Sigma\)[233]은 1.085T 파라미터를 갖는 MoE 기반 LLM으로, dense Transformer 모델에서 Random Routed Experts(RRE)를 갖는 sparse 모델로 전환하고, Expert Computation and Storage Separation(ECSS)을 이용하여 329B 토큰 이상의 모델을 효과적으로 학습한다. 마지막으로, 믹스트랄 8x7B [123]은 46.7B의 총 파라미터를 갖는 MoE이다. Mixtral 8x7B는 MoE 구조의 장점을 활용하여 MMLU, MBPP, GSM-8K와 같은 대부분의 벤치마크에서 LLaMA-2 70B보다 추론을 위해 토큰당 모델의 12.9B 파라미터만을 사용하여 6배 빠른 추론을 수행한다.

도 14. 혼합 전문가(MoE) 및 롱 컨텍스트 LLM의 예시.

**알고리즘 수준 MoE 최적화** MoE 기반 LLM의 효율성을 알고리즘 수준에서 개선할 수 있습니다. 전문가 선택(Expert Choice; 360)이라는 기법은, 전문가들이 토큰들이 상위 k개의 전문가들을 선택하도록 하는 대신에 상위 k개의 토큰들을 선택할 수 있게 하며, 이는 각각의 전문가가 고정된 버킷 사이즈를 유지하는 동안 각각의 토큰이 가변적인 수의 전문가들로 향할 수 있음을 암시한다. 이 방법은 GLUE 및 SuperGLUE 벤치마크에서 더 높은 성능을 보여주며 11개의 태스크 중 7개에서 T5 밀집 모델보다 성능이 우수하다. StableMoE[54]는 훈련 중 동일한 입력에 대한 타겟 전문가의 변경 문제를 식별하고, 두 개의 훈련 단계를 생성함으로써 이를 해결한다. 처음에는 균형 잡힌 라우팅 전략을 배양한 다음 분리 경량 라우터로 증류합니다. 다음 단계에서는 이 증류 라우터를 고정 토큰 대 전문가 할당에 사용 하 여 안정적인 라우팅 전략을 보장 합니다. X-MoE[46]는 초기 라우팅 메커니즘이 전문가 중심 중심으로 토큰 클러스터링을 촉진하여 표현 붕괴 경향을 나타낸다는 점에 주목한다. 저차원 하이퍼스피어에서 토큰과 전문가 간의 라우팅 점수를 추정하는 것을 제안한다. 평생-MoE[39]는 MoE가 단순히 추가적인 전문가 계층 및 적합한 전문가 정규화를 통합함으로써, 추가적인 계산 비용 없이 온라인 데이터 스트림에서 상이한 코퍼스 분포에 적응하기 위해 모델의 용량을 증가시킨다는 것을 발견한다. 이는 이전의 지식을 잃지 않고 순차적인 데이터 분포들에 대한 MoE 기반 LLM의 연속적인 사전-훈련을 용이하게 한다. 마지막으로 Flan-MoE [246]은 MoE와 명령어 튜닝의 결합을 촉진하여 MoE 모델이 밀집 모델에 비해 명령어 튜닝에서 더 많은 이득을 얻는 것을 관찰한다. 특히, Flan-MoE는 계산 자원 또는 메모리 요구 사항의 증가를 요구하지 않고 언어 모델을 효과적으로 확대한다.

**시스템 수준 MoE 최적화** MoE 기반 LLM의 학습 및 추론을 가속화하기 위해 여러 시스템 수준 최적화 기술이 개발되었습니다. 예를 들어, FastMoE[100]은 PyTorch에 구축된 분산 MoE 트레이닝 시스템으로서, 공통 가속기와 호환된다. 이 시스템은 유연한 모델 설계와 Transformer-XL 및 Megatron-LM과 같은 다양한 응용 프로그램에 쉽게 적응할 수 있는 계층적 인터페이스를 제공한다. FasterMoE[101]는 지연 시간을 예측하고 루프라인과 같은 방법론을 통해 종단 간 성능을 분석하는 성능 모델을 소개한다. 이 모델을 활용하여 부하 분산을 위한 동적 쉐도잉 기법, 운영을 위한 동시 세밀한 스케줄, 모델 학습을 위한 전문가 선택을 조정하여 네트워크 혼잡을 완화하는 전략을 제시한다. DeepSpeed-MoE[225]는 MoE 모델 파라미터의 트레이닝 및 추론 효율을 모두 향상시키기 위해 피라미드-잔여 MoE(PR-MoE)를 설계하였다. PR-MoE는 잔류 연결을 사용하여 전문가를 최적으로 활용하는 고밀도-MoE 하이브리드이며, 품질이나 계산 요구 사항을 희생하지 않고 매개변수 크기를 최대 3배까지 줄일 수 있도록 관리한다. 또한 품질을 유지하면서 모델 크기를 최대 3.7배까지 트리밍할 수 있는 증류 변형인 Mixture-of-Students(MoS)를 제안한다. TA-MoE[30]는 현재의 MoE 디스패치 패턴이 근본적인 이종의 네트워크 환경을 완전히 활용하지 않는다는 점을 강조하며, 따라서 네트워크 토폴로지에 기초하여 MoE 디스패치 패턴을 동적으로 수정하는 대규모 MoE 훈련을 위한 토폴로지 인식 라우팅 전략을 도입하여 FastMoE, FasterMoE 및 DeepSpeed-MoE보다 성능이 우수하다는 것을 강조한다. EdgeMoE[331]는 MoE 기반 LLMs에 맞춘 온-디바이스 추론 엔진을 제시한다. 다양한 스토리지 수준에 모델을 분산하여 추론을 위한 메모리 및 계산을 최적화합니다. 구체적으로, 비전문가 모델 가중치는 에지 디바이스에 직접 저장되는 반면, 전문가 가중치는 외부에 유지되고 필요할 때 디바이스의 메모리에만 로딩된다. Tutel [119]는 훈련과 추론을 가속화하기 위해 적응 병렬성과 파이프라이닝 기능을 가진 MoE를 위한 확장 가능한 스택이다. MoE 파라미터와 입력 데이터에 대해 일관된 레이아웃을 사용하여 수학적 불일치나 텐서 마이그레이션 비용 없이 전환 가능한 병렬성과 동적 파이프라이닝을 지원하여 자유로운 런타임 최적화를 가능하게 한다. SmartMoE[338]는 MoE에 대한 분산 훈련에 초점을 맞춘다. 오프라인 단계에서 SmartMoE는 하이브리드 병렬화 전략의 탐색 공간을 구성한다. 온라인 단계에서는 경량 알고리즘을 통합하여 최적의 병렬 전략을 식별합니다. 마지막으로, MegaBlocks[83]은 MoE-지향 연산을 블록-희소 연산으로 변환하고 블록-희소 GPU 커널을 생성하여 하드웨어에서 MoE 연산을 최적화한다. 이는 Megatron-LM으로 훈련된 밀집 DNN보다 Tutel에 비해 최대 40% 빠르고 2.4배 빠른 훈련 시간으로 이어진다.

#### 2.5.3. **Long Context LLMs**.

멀티턴 대화 및 회의 요약과 같은 많은 실제 응용에서 기존의 LLM은 미리 훈련된 것보다 훨씬 더 긴 컨텍스트 시퀀스를 이해하거나 생성하는 것이 종종 필요하며 긴 컨텍스트에 대한 잘못된 암기로 인해 정확도의 저하를 초래할 수 있다. 이 문제를 해결하기 위한 가장 명확하고 직접적인 방법은 시간이 많이 걸리고 계산량이 많은 유사한 긴 시퀀스 데이터로 LLM을 미세 조정하는 것이다. 최근, LLM이 외삽 및 보간, 순환 구조, 윈도우 세그먼트 및 슬라이딩 구조, 메모리-검색 증강을 포함하여 보다 효율적인 방식으로 더 긴 컨텍스트 길이에 적응할 수 있도록 다양한 새로운 방법이 개발되었다.

**외삽 및 보간.** 절대 위치 임베딩(APE) [280], 학습 위치 임베딩(LPE) [286], 상대 위치 임베딩(RPE) [244], 상대 위치 편향(224) 및 회전 위치 임베딩(RoPE) [259]과 같은 표준 위치 인코딩 방법은 LLM에서 위치 정보의 통합을 향상시켰습니다. 예를 들어, LPE는 GPT-3[20] 및 OPT[346]에 의해 사용되었고; RPE는 고퍼[223] 및 친칠라[105]에 의해 사용된 반면, RoPE는 LLaMA-1 및 GLM-130B에 의해 사용되었다. 그러나 추론 중에 훨씬 더 긴 시퀀스에 잘 일반화되도록 보장하면서 제한된 최대 길이를 가진 시퀀스에서 LLM을 훈련시키는 것은 여전히 어렵다. 그 점을 감안할 때, 위치 외삽[32; 215; 263] 및 위치 내삽[36; 154; 208]에 기초한 기술들이 제안되었다.

위치 외삽 전략은 모델이 훈련 중에 명시적으로 학습한 것 이상으로 위치 정보의 인코딩을 확장한다. 예를 들어, ALBi[215]는 훈련 중에 보이는 최대 길이를 초과하는 시퀀스에 대한 외삽을 달성하기 위해 선형 편향으로 주의를 적용한다. 위치 임베딩을 사용하는 것과 달리 관련 키와 쿼리 사이의 거리에 따라 패널티가 선형적으로 감소하면서 부정적인 편향된 주의 점수를 적용함으로써 효율적인 길이 외삽을 용이하게 할 수 있다. ALBi[215]와는 달리, xPOS[263]는 어텐션 해상도를 외삽을 위한 마커로서 특성화하고, 상대적 위치 임베딩을 활용하여 어텐션 해상도를 향상시킴으로써, 길이 외삽을 개선한다. 그러나, 이러한 기술들은 GPT-4[200], LLaMA[276], 또는 LLaMA-2[277]와 같은 최근의 LLM들 중 일부에서 구현되지 않았다. CLEX [32]는 길이 스케일링 인자에 대한 연속 동역학을 모델링하기 위해 상미분 방정식으로 위치 임베딩 스케일링을 일반화하는 것을 제안한다. 이렇게 함으로써 CLEX는 기존의 위치 외삽 스케일링 방법의 한계를 제거하여 긴 시퀀스 생성을 가능하게 한다.

반면에 위치 보간 전략은 입력 위치 인덱스의 규모를 줄이고 컨텍스트 윈도우 크기를 확장하여 LLM이 더 긴 텍스트 시퀀스에 비해 성능을 유지할 수 있다. 예를 들어, Chen 등[36]은 훈련된 컨텍스트 길이를 초과하여 확장하는 것이 자기-주의 메커니즘을 손상시킬 수 있음을 강조한다. 그들은 사전 훈련 단계에서 직면하는 이전 컨텍스트 윈도우 한계와 최대 위치 인덱스를 정렬하는 선형 보간을 통해 위치 인덱스를 줄이는 방법을 제안한다. NTK 보간 [18]은 RoPE의 베이스를 수정하여 각 RoPE 차원의 회전 속도를 효과적으로 변경한다. YaRN 보간[208]은 램프 함수를 사용하여 차원에 걸쳐 다양한 비율로 선형 및 NTK 보간을 혼합하고 긴 입력으로 인해 주의 행렬의 분포 이동을 상쇄하기 위해 온도 인자를 통합한다. FIRE[154]는 길이 일반화를 가능하게 하기 위해 모든 시퀀스 길이에 걸쳐 기능을 인코딩하기 위한 바운딩된 입력을 보장하는, 편향 및 점진적 보간에 대한 입력 위치의 학습가능한 매핑을 사용하는 기능적 상대 위치 인코딩을 제안한다. PoSE[362]는 고정된 컨텍스트 윈도우를 사용하여 긴 입력들을 스마트하게 시뮬레이션하고, 각각의 청크의 위치 인덱스들을 조작하기 위해 별개의 스킵 바이어스 용어들을 설계하는 위치 스킵-와이즈 트레이닝을 제안한다. 이 전략은 전체 길이의 미세 조정에 비해 메모리와 시간 오버헤드를 줄입니다.

**반복 구조.** LLM의 긴 시퀀스 관리 기능은 반복 구조를 통해 향상 될 수도 있습니다. 예를 들어, Transformer-XL[56]은 세그먼트 레벨 재발 메커니즘을 제시하고, 향상된 상대적 위치 인코딩을 활용하여 장기 의존성을 캡처하고 장기 문맥 단편화 문제를 해결한다. Memformer[305]는 과거의 정보를 인코딩하고 검색하기 위해 외부 동적 메모리를 활용하여 긴 시퀀스에 대해 선형 시간 및 일정한 메모리 공간 복잡도를 달성한다. 또한 메모리 요구량이 현저히 낮은 시간을 통해 장거리 역전파를 용이하게 하기 위해 메모리 재생 역전파(MRBP)를 제안한다. \ (\infty\)-former [185]는 비경계 장기 기억(LTM)으로 확장된 Transformer 모델을 제시하며, 연속 공간 주의 프레임워크를 사용하여 메모리에 수용된 정보 단위의 양과 표현의 세분성을 균형 있게 한다. RMT(Recurrent Memory Transformer) [21]은 특별한 메모리 토큰을 입력 또는 출력 시퀀스에 통합함으로써 과거 세그먼트 레벨로부터의 정보를 유지하는 재발 메커니즘을 사용하여 긴 컨텍스트 모델링에서 Transformer-XL에 비해 우수한 성능을 보여준다. 블록-순환 트랜스포머[118]는 병렬 계산을 통해 긴 시퀀스를 모델링하기 위해 광범위한 상태 벡터 및 토큰 세트에 걸쳐 순환 함수를 실행하기 위해 자기-집중 및 교차-주의를 이용한다. 마지막으로, Retentive Network[262]는 멀티-헤드 주의의 대안으로서 멀티-스케일 보유 메커니즘을 도입한다. 병렬 및 청크 단위의 반복 표현을 포함함으로써 효과적인 스케일링을 초래하고, 병렬 학습을 가능하게 하며, 훈련 병렬화 및 일정한 추론 비용을 달성하면서, 다른 Transformer 모델에 비해 선형 긴 시퀀스 메모리 복잡도를 제공한다.

**세그먼트화 및 슬라이딩 창.** 분할 및 슬라이딩 창 기술은 입력 데이터를 더 작은 세그먼트로 분할 하거나 이동 창을 적용 하 여 긴 시퀀스를 통해 슬라이딩 하 여 긴 컨텍스트 처리의 문제를 해결 합니다. 예를 들어, 미스트랄 [123]은 감소된 추론 비용으로 임의의 길이의 시퀀스를 효과적으로 처리하기 위해 슬라이딩 윈도우 주의를 사용한다. 스트리밍LLM[312]은 초기 토큰들의 Key-Value를 유지하는 것이 윈도우 어텐션의 성능을 상당히 회복시킨다는 점에 주목하여 어텐션 싱크 현상을 식별한다. 이러한 관찰에 기초하여, 윈도우 컨텍스트와 첫 번째 토큰을 병합하여 유한 길이 주의 윈도우로 훈련된 LLM을 허용하면서도 미세 조정 없이 무한 시퀀스 길이로 일반화할 수 있는 효율적인 프레임워크를 제안한다. 병렬 컨텍스트 윈도우(PCW)(230)는 긴 컨텍스트를 청크로 세그먼트화하여, 주의 메커니즘이 각 윈도우 내에서만 기능하도록 제한한 다음, 이들 윈도우에 걸쳐 위치 임베딩을 재분배한다. LongNet[65]은 확장된 어텐션을 제안하며, 이는 거리가 증가함에 따라 어텐션 필드를 기하급수적으로 확장하여 10억 개 이상의 토큰의 시퀀스 길이를 처리할 수 있게 한다. LongNet은 시퀀스 차원을 분할하여 학습을 병렬화함으로써 구현될 수 있다. SLED [121]은 LLM에서 사용하기 위해 잘 검증된 단문 언어 모델을 용도 변경하고 활용하는 긴 시퀀스를 처리하는 간단한 방법이다.

**메모리 검색 확장** 여러 연구에서 메모리 검색 확장 전략을 사용 하 여 매우 긴 텍스트의 추론을 해결 합니다. 주목할 만한 예는 KNN-증강 트랜스포머 [308]로서, KNN(k-nearest-neighbor) 룩업을 활용하여 이전에 유사한 컨텍스트 임베딩을 인출함으로써 주의 컨텍스트 크기를 확장한다. 랜드마크 어텐션[191]은 랜드마크 토큰을 사용하여 입력의 각 블록을 표현하고, 이를 관련 블록을 선택하는 데 활용하기 위해 어텐션 메커니즘을 훈련시킨다. 이를 통해 이전 컨텍스트의 랜덤 액세스 유연성을 유지하면서 주의 메커니즘을 통해 블록을 직접 검색할 수 있어 긴 컨텍스트 모델링을 위한 LLaMA-1에서 인상적인 성능을 보여준다. LongMem[294]는 메모리 인코더로서 원래의 백본 LLM과 메모리 리트리버 및 판독기로서 적응적인 잔류 측 네트워크를 갖는 디커플드 네트워크 아키텍처를 제안하며, 지식 충실성을 방지하기 위해 장기 과거 컨텍스트를 효율적으로 캐싱 및 업데이트한다. Unlimiformer[14]는 주의점-곱 점수를 KNN 거리로 출력함으로써 KNN 증강 트랜스포머를 향상시켜 사실상 무제한 입력 시퀀스의 인덱싱을 가능하게 한다. FoT(Focused Transformer) [278]은 문맥 길이가 증가함에 따라 관련 키 대 관련 키의 비율이 감소함을 강조하고, 키-값 공간의 구조를 정제하기 위해 대조적 학습을 통해 최적화된 솔루션을 제안한다. 마지막으로, Xu 등 [316]은 4K 컨텍스트 윈도우를 갖는 LLM이, 생성 동안 간단한 검색으로 증강될 때, 긴 컨텍스트 태스크에 대한 위치 보간 [36]을 사용하여 16K 컨텍스트 윈도우를 갖는 미세 조정된 LLM의 성능을 일치시킬 수 있는 반면, 상당히 적은 계산을 필요로 한다는 것을 발견한다.

#### Transformer-Alternate Architectures

현재 변압기 기반 아키텍처가 LLM의 최전선에 있는 반면, 일부 연구에서는 변압기 기반 아키텍처를 대체할 새로운 아키텍처를 제안한다.

**상태 공간 모델** 입니다. 주의 메커니즘을 대체하는 것을 목표로 하는 유망한 접근법은 상태 공간 모델(SSM)이다. SSM은 \(x^{\prime}(t)=Ax(t)+Bu(t)\), \(y(t)=Cx(t)+Du(t)\)로 공식화되며, 이는 단일차원 입력신호 \(u(t)\)를 N차원 잠재상태 \(x(t)\)로 매핑한 후 단일차원 출력신호 \(y(t)\), 여기서 \(A\), \(B\), \(C\), \(D\은 기울기 강하 [90]에 의해 학습된 파라미터이다. 2차 복잡도를 갖는 주의와 비교하여 SSM은 시퀀스의 길이에 비해 거의 선형적인 계산 복잡도를 제공한다. 이러한 이점을 감안할 때 SSM을 개선하기 위한 일련의 기술이 제시되었다. 예를 들어, Structured State Space 시퀀스 모델(S4)[90]은 낮은 순위 보정으로 매트릭스 \(A\)을 컨디셔닝하여 SSM을 정제한다. 이것은 안정적인 대각화를 가능하게 하고 SSM을 코시 커널의 잘 연구된 계산으로 단순화한다. 대각 상태 공간(DSS)[95]은 대각 플러스 낮은 랭크 구조 대신에 상태 공간의 완전 대각 파라미터화를 제안함으로써 SSM을 개선하여 더 큰 효율성을 입증한다. 최신 하드웨어에 적응하면서 SSM과 주의 사이의 격차를 메우기 위해 H3 [80]은 두 개의 SSM을 스택하여 출력 및 입력 프로젝션과 상호 작용하여 토큰을 기록하고 시퀀스 전체 비교를 동시에 용이하게 한다. Mehta 등[186]은 GSS(Gated State Space)라는 보다 효율적인 계층을 도입하는데, 이는 다중 언어 모델링 벤치마크에 대한 복잡성을 유지하면서 이전의 전략 [95]보다 2-3배 더 빠른 것으로 경험적으로 입증되었다. Block-State Transformer(BST)[211]는 확장 범위 문맥화를 위한 SSM 부계층과 단기 시퀀스 표현을 위한 Block Transformer 부계층을 결합한 하이브리드 계층을 설계한다. Gu와 Dao[89]는 상관없는 데이터를 제거하기 위한 선택 메커니즘을 설계하여 SSM을 향상시키는 Mamba를 제안하고, 반복 연산을 위한 하드웨어 인식 병렬 알고리즘을 개발하여 트랜스포머보다 5배 높은 처리량을 달성하였다. Ren et al. [232]는 일반적인 모듈화 활성화 메커니즘인 SMA(Sparse Modular Activation)를 제안하며, 이는 MoE, 적응 계산, 동적 라우팅 및 희박 주의에 대한 이전 작업을 통합하고 SMA를 추가 적용하여 최신 품질 효율성 절충을 달성하기 위한 새로운 아키텍처 SeqBot을 개발한다.

**기타 순차 모델**. 마지막으로, 트랜스포머 층을 대체하기 위해 몇몇 다른 아키텍처들이 제안되었다. 수용 가중 키 값(RWKV) 모델[206]은 반복 신경망(RNN) 및 변압기의 장점을 통합한다. 이 조합은 RNN의 효율적인 추론 능력과 결합된 트랜스포머의 효과적인 병렬화 훈련 기능을 활용하도록 설계되어 자동 회귀 텍스트 생성을 관리하는 데 능숙한 모델을 단조하고 긴 시퀀스 처리와 관련된 문제를 효과적으로 해결한다. Poli et al. [212]는 긴 시퀀스에서 이차 비용을 완화하면서 주의 메커니즘에 대한 하위 이차 대안인 Hyena를 제안한다. 이 연산자는 두 개의 효율적인 하위 2차 프리미티브를 포함한다: 음함수 긴 콘볼루션과 입력의 곱셈 요소-와이즈 게이팅. 이를 통해 하이에나는 긴 시퀀스에 대해 더 크고 효율적인 컨볼루션 언어 모델의 개발을 용이하게 한다. 마지막으로, MEGABYTE[333]는 긴 바이트 시퀀스를 토큰과 유사한 고정된 크기의 패치로 분해하는데, 인코딩을 위한 패치 임베더, 패치 표현을 위한 큰 자기회귀 트랜스포머로서 작용하는 글로벌 모듈, 및 패치 내의 바이트를 예측하기 위한 로컬 모듈로 구성된다.

## 3. 데이터 중심 방법

### Data Selection

LLM들에 대한 데이터 선택은 모델이 필수적인 패턴들 및 특징들을 효율적으로 캡처할 수 있도록 가장 유익하고 다양한 예들을 신중하게 선택하는 것을 수반하여, 학습 프로세스를 가속화한다[85; 237; 313; 326]. 그림 15는 효율적인 LLM 사전 훈련 및 미세 조정을 위한 최신 데이터 선택 기법을 요약한 것이다.

#### 3.1.1. **효율적인 사전 학습을 위한 데이터 선택**

데이터 선택은 모델이 훈련 중에 가장 유익하고 관련된 예시에 집중할 수 있도록 함으로써 LLMs 사전 훈련 효율성을 향상시킨다. 대표 데이터의 하위 집합을 신중하게 큐레이션함으로써 모델은 필수 패턴과 특징을 추출할 수 있어 일반화된 지식을 보다 효율적으로 획득할 수 있다. 예를 들어, SSPT[85]는 독해력 원리를 기반으로 한 사전 학습 과제이다. 이는 문맥적으로 관련된 텍스트 구절에서 답변을 선택하는 것을 포함하며, 이는 다양한 기계 판독 이해(MRC) 벤치마크에서 성능이 눈에 띄게 향상되었음을 보여준다. Yao et al. [326]은 기계-생성 번역들의 품질을 현저하게 상승시키는 언어학적으로 유익한 문장들의 선택을 위한 메타-학습 기반 방법을 제안한다. Xie 등 [313]은 범용 LLM 및 특수 LLM 모두에 대한 중요도 재샘플링에 기초한 데이터 선택 방법인 DSIR을 제안한다. 그것은 상이한 데이터 조각들이 더 간단한 특징들 세트 내에 얼마나 중요한지를 계산하고, 이러한 중요도 계산들에 기초하여 데이터를 선택한다.

#### 3.1.2. **효율적인 미세 조정을 위한 데이터 선택**

데이터 선택은 또한 모델을 정제하기 위해 예제의 큐레이트된 부분 집합만이 사용되기 때문에 미세 조정 효율성을 높일 수 있다. 이러한 접근법은 적응 프로세스가 수행되는 것을 보장한다

도 16. LLMs에 대한 데이터 선택 기술의 예시.

도 15. LLMs에 대한 데이터 선택 기술의 요약.

목표 도메인 또는 작업에 고유한 특정 뉘앙스에 초점을 맞추어 미세 조정 프로세스를 보다 효율적으로 만든다. 예를 들어, Instruction Mining(Liu et al., 2019)은 명령어 추종 태스크에서 데이터 품질을 평가하기 위한 선형 평가 방법을 제시한다. 이는 고품질 데이터의 중요성을 강조하여 명령어 마이닝 큐레이트 데이터 세트로 학습된 모델이 42.5%의 사례에서 일반 데이터 세트로 학습된 모델보다 성능이 우수함을 보여준다. 이는 데이터 품질의 중요성을 강조하고 수업 추적 모델 효능의 향후 개선을 위한 토대를 마련한다. Ivison 등(2019)은 더 큰 멀티태스크 데이터세트로부터 유사한 라벨링된 것들을 검색하기 위해 몇 개의 라벨링되지 않은 예들을 사용하는 것을 제안하여, 태스크-특정 모델 트레이닝을 개선한다. 이 방법은 미세 조정을 위한 표준 멀티태스크 데이터 샘플링보다 성능이 우수하고 적은 샷 미세 조정을 향상시켜 현재 모델에 비해 2-23%의 상대적 개선을 제공한다. TS-DShapley (2019)는 Shapley 기반 데이터 가치 평가를 미세 조정 LLM에 적용하는 계산 문제를 해결하기 위해 도입되었다. 전체 훈련 세트를 평가하기 위해 부분 집합에서 계산된 샤플리 값을 집계하는 효율적인 샘플링 기반 방법을 사용한다. 또한, 목표 언어 모델의 표현을 사용하여 훈련된 단순 분류기의 정보를 활용하는 값 전달 방법을 통합한다. Low Training Data Instruction Tuning (LTD Instruction Tuning) (Kumar et al., 2019)은 Fine-tuning에서 큰 데이터 세트의 필요성에 도전하며, 원래 데이터 세트의 0.5% 미만이 성능을 손상시키지 않으면서 태스크-특정 모델을 효과적으로 트레이닝할 수 있음을 보여준다. 이 접근법은 데이터 부족 환경에서 보다 자원 효율적인 관행을 가능하게 하며, 선택적 데이터 전략과 최적의 데이터 효율성을 위한 맞춤형 훈련 프로토콜을 결합한다. AlpaGasus(Kumar et al., 2019)는 52k의 더 큰 데이터 세트로부터 꼼꼼하게 필터링되는, 단지 9k의 고품질 데이터 포인트 상에서 미세 조정된 모델이다. 전체 데이터 세트에 대해 훈련된 원래 모델보다 성능이 우수하고 훈련 시간이 5.7배 감소하여 명령어 미세 조정에서 고품질 데이터의 성능을 보여준다. LIMA(LMA, 2019)는 작고 선택된 일련의 예들로 LLM들을 미세 조정하며, 강력한 성능을 보여주고 광범위한 튜닝의 필요성에 도전한다. 그것은 새로운 작업에 잘 일반화되며 비교에서 43%의 사례에서 GPT-4와 일치하거나 초과하여 LLM이 사전 훈련에서 대부분의 지식을 획득하여 최소한의 명령 튜닝을 필요로 함을 시사한다.

### Prompt Engineering

프롬프트 엔지니어링(Von et al., 2019)은 원하는 출력을 생성할 때 LLM을 안내하기 위해 효과적인 입력(즉, 프롬프트)을 설계하는 데 중점을 둔다. 특정 언어 모델의 기능과 뉘앙스에 더 잘 맞도록 입력 프롬프트 또는 쿼리를 조정하여 추론 효율성을 향상시킨다. 의미 분류와 같은 일부 간단한 작업에 사용될 때, 프롬프트 엔지니어링은 높은 정확도를 달성하기 위해 미세 조정을 대체할 수도 있다(Von et al., 2019). 그림 17에 요약된 바와 같이, 프롬프트 엔지니어링 기술은 소수의 프롬프트, 프롬프트 압축 및 프롬프트 생성으로 그룹화될 수 있다.

도 17. LLMs에 대한 프롬프트 엔지니어링 기술의 요약.

#### 3.2.1. **Few-Shot Prompting**

소수의 샷 프롬프트는 LLM에 제한된 세트의 예들(즉, 시연)을 제공하여 실행을 위해 요구되는 작업에 대한 이해를 유도하는 것을 포함한다[301]. 이러한 데모는 테스트 예와의 유사성에 기초하여 LLM의 트레이닝 코퍼스로부터 선택되며, LLM은 이러한 유사한 데모로부터 얻은 지식을 사용하여 정확한 예측을 할 것으로 예상된다[67]. 소샷 프롬프트는 추가 훈련이나 미세 조정 없이도 LLM이 매우 다양한 작업을 수행하도록 안내함으로써 LLM을 사용하는 효율적인 메커니즘을 제공한다. 또한, 효과적인 소수의 샷 프롬프트 접근법은 생성된 프롬프트를 여분의 컨텍스트의 약간의 증가만으로 LLM이 높은 정확도로 작업에 신속하게 적응할 수 있도록 충분히 간결하게 할 수 있고, 따라서 추론 속도를 상당히 향상시킬 수 있다. 도 18에 예시된 바와 같이, 소수의 샷 프롬프트 기법은 일반적으로 시연 선택, 시연 순서화, 명령어 생성 및 다단계 추론으로 그룹화될 수 있다.

**시연 조직.** 시연 조직은 추론을 위한 적절한 프롬프트를 구성하도록 적절한 방식으로 시연을 조직하는 것을 말합니다. 실증조직은 추론속도에 유의한 영향을 미친다. 부적절한 조직은 상당한 양의 불필요한 정보의 처리를 초래하여 상당한 둔화를 초래할 수 있다. 실증 조직의 주요 과제는 실증 선택과 실증 명령이라는 두 가지 관점에서 나온다.

* _Demonstration Selection._ 시위 선택은 몇 발에 대한 좋은 예제를 선택하는 것을 목표로 합니다[67]. 만족스러운 결과를 생성하기 위해, 양호한 시연의 선택은 프롬프트에 사용되는 소수의 시연만을 필요로 할 수 있으며, 따라서 프롬프트는 보다 효율적인 추론을 위해 간결하고 간단해진다. 기존의 시연 선택 기법들은 비감독 방법[157, 169, 190, 218, 257, 296, 309, 349] 및 감독 방법[156, 181, 236, 289]으로 그룹화될 수 있다. 감독되지 않은 방법들은 L2 거리, 코사인 거리, 및 최소 기술 길이(MDL)와 같은 미리 정의된 유사성 함수를 사용하여 트레이닝 세트로부터 가장 가까운 예들을 선택하는 것을 목표로 한다[309]. 예를 들어, KATE[169]는 주어진 테스트 샘플의 가장 가까운 이웃을 대응하는 데모로 직접 사용하는 감독되지 않은 선택 방법이다. VoteK[257]은 양호한 성능을 달성하기 위해 많은 예들의 세트를 필요로 하는 그것의 제한을 해결하기 위해 KATE의 개선된 버전이다. KATE와 달리, VoteK는 시위의 다양성을 증가시킨다.

도 18. LLMs에 대한 소수의 샷 프롬프트 기술의 예시.

이미 선택한 예제와 유사한 예제를 처벌합니다. 이에 비해 감독 방법은 훈련 세트에서 도메인별 검색기를 훈련하고 시연 선택에 사용해야 한다. 예를 들어, EPR[236]은 훈련 시체로부터 BM25와 같은 비감독 리트리버에 의해 초기화된 후보들의 작은 세트로부터 데모를 선택하도록 훈련된다. UDR[156]은 상이한 작업들에 걸쳐 데모 선택을 통합하기 위해 통합된 데모 리트리버를 채택함으로써 EPR을 더욱 향상시킨다. 감독되지 않은 방법에 비해 감독된 방법은 종종 더 만족스러운 생성 결과를 가져오지만 도메인 외 데이터를 처리하기 위해 검색기를 자주 조정해야 하므로 추론에 덜 효율적이다.
* _Demonstration Ordering._ After selecting representative samples from the training set, the next step is ordering these samples in the prompt. The order of the demonstrations also has a significant impact on the performance of the model. Therefore, selecting the right order of demonstrations can help the model quickly reach a good generation quality with fewer samples, thus improving the inference efficiency. To date, only a few studies have delved into this area. For example, Liu et al. [169] suggest arranging demonstrations based on their distance from the input, placing the closest demonstration furthest to the right. Lu et al. [179] propose to develop both global and local entropy metrics and use the entropy metrics to set up the demonstration order.

**템플릿 형식.** 템플릿 형식은 프롬프트를 형성하는 데 적합한 템플릿을 설계하는 것을 목표로 합니다. 좋은 템플릿은 일반적으로 LLM에 의해 필요한 모든 정보를 간단한 문장으로 컴파일하여 프롬프트와 전체 입력 컨텍스트를 가능한 간결하게 만들어 더 높은 추론 효율성을 보장한다. 템플릿 형식 설계는 명령어 생성과 다단계 추론의 두 부분으로 나눌 수 있다.

* _Instruction Generation._ The instruction of the template refers to a short description of the task. By adding instructions to the prompt, LLMs can quickly understand the context and the task they are currently performing, and thus may require fewer demonstrations to create a desirable prompt. The performance of a given task is highly affected by the quality of the instructions. The instructions vary not only between different datasets for the same task but also between different models. Unlike demonstrations that are usually included in traditional datasets, the generation of instructions is heavily dependent on human efforts. To enhance the efficiency of instruction generation, automatic instruction generation techniques have been proposed. For example, Instruction Induction [107] and Automatic Prompt Engineer [361] have demonstrated that LLMs can generate task instructions. Wang et al. [297] propose Self-Instruct, an approach that allows LLMs to align with self-generated instructions, highlighting their inherent adaptability. Yang et al. [320] also discover that LLMs can be treated as an optimizer to iteratively generate better instructions for the target LLM and have applied this technique to various LLMs. Chen et al. [41] develop TeGit for training language models as task designers, which can automatically generate inputs and outputs together with high-quality instructions to better filter the noise based on a given human-written text for fine-tuning LLMs. Despite the promise of automatic instruction generation methods, their complexity is still a major bottleneck for their real-world adoption.
* _다중 단계 추론_ 최종 답변을 출력하기 전에 중간 단계들의 시퀀스를 생성하도록 LLM들을 안내하는 것은 세대의 품질을 크게 향상시킬 수 있다. 이 기술을 CoT(Chain-of-Thought) 프롬프트 [302]라고도 한다. 컨텍스트 및 태스크를 LLM들에 더 이해할 수 있게 하기 위해 몇 개의 예시적인 예들을 반복적으로 선택하는 것보다, CoT는 제한된 개수에만 집중하고 고려를 위한 세부사항들을 컨텍스트에 추가함으로써, 프롬프트를 더 포괄적이고 효과적이고 보다 효율적인 추론을 보장한다. 그러나, CoT의 장점에도 불구하고, 매 중간 단계[67]의 정확성을 보장하는 것은 여전히 어렵다. 이를 감안할 때, 이 문제를 해결하기 위해 많은 기술들이 제안되어 왔다. 예를 들어, Auto-CoT[351]는 LLM들로부터 CoT를 단계적으로 생성할 것을 제안한다. 자기 질문[216]은 각 단계의 자기 생성 질문을 CoT에 통합한다. ReAct [325]는 추론에 추가 정보를 통합하기 위해 외부 환경과 상호작용하면서 연기에 대한 상위 레벨 계획을 생성, 유지 및 조정하는 동적 추론을 수행한다. 최소-최대 프롬프팅[359]은 복잡한 질문을 더 작은 질문으로 분해하고 이전의 질문 및 답변의 맥락 내에서 반복적으로 응답한다. Tree-of-Thinking(ToT)[324]는 텍스트의 일관성 있는 단위에 대한 탐구를 포함하도록 CoT를 확장하고 의사 결정 프로세스를 심의한다. CoT-SC[295]는 CoT 프롬프트에서 단순한 그리디 디코딩을 대체하기 위해 "셀프-일관성"이라는 새로운 디코딩 접근법을 소개한다. 탐욕스러운 경로 대신 다양한 추론 경로를 샘플링하는 것으로 시작하여 샘플링된 모든 경로를 고려하여 가장 일관된 답변을 결정한다. 사상의 그래프(Graph of Thoughts, GoT)[15]는 LLM에 의해 생성된 정보를 일반 그래프로 나타내며, "LLM 생각들"은 정점들로서 그리고 에지들은 이들 정점들 사이의 종속성을 나타낸다. 대비적 CoT[47]는 유효 및 무효 추론 데모를 모두 제공함으로써 언어 모델 추론을 향상시키기 위한 대비적 사고 사슬을 제안한다. 마지막으로, XoT [66]은 사전 훈련된 강화 학습 및 몬테카를로 트리 탐색(MCTS)을 활용하여 외부 도메인 지식을 LLM의 사고 프로세스에 통합함으로써 새로운 보이지 않는 문제로 효율적으로 일반화하는 능력을 향상시킨다.

#### 3.2.2. **Prompt Compression**

프롬프트 압축(도 19(a))은 긴 프롬프트 입력들을 응축하거나 콤팩트 프롬프트 표현들을 학습하는 것을 통해 LLM 입력들의 프로세싱을 가속화한다. Mu 등[195]은 프롬프트를 gist 토큰으로 지칭되는 보다 간결한 토큰 세트로 증류하도록 LLM을 트레이닝할 것을 제안한다. 이러한 요지 토큰은 원래 프롬프트의 지식을 캡슐화하며 향후 사용을 위해 저장될 수 있다. 그렇게 함으로써, 프롬프트를 최대 26배까지 압축할 수 있어, 초당 부동 소수점 동작(FLOP)이 최대 40%까지 감소된다. Chevalier 등[45]은 긴 텍스트 컨텍스트를 요약 벡터로 알려진 컴팩트 벡터로 응축하기 위해 AutoCompressor를 제안하며, 이는 이후 언어 모델에 대한 소프트 프롬프트로서 사용될 수 있다. 이러한 요약 벡터는 모델의 컨텍스트 창을 확장하여 훨씬 적은 계산 비용으로 더 긴 문서를 처리할 수 있다. Jung과 Kim[127]은 정책 네트워크를 사용하여 프롬프트를 직접 편집하는 PCRL(Prompt Compression with Reinforcement Learning)을 제안하여 성능을 유지하면서 토큰 수를 줄이는 것을 목표로 한다. 다양한 명령 프롬프트에서 토큰 카운트의 평균 24.6% 감소를 달성합니다. Ge 등[84]은 학습 가능한 인코더와 학습 가능한 인코더로 구성된 ICAE(In-context Autoencoder)를 제안한다.

도 19. LLMs에 대한 프롬프트 압축(a) 및 프롬프트 생성(b)의 예시.

고정 디코더. 인코더는 긴 컨텍스트를 제한된 수의 메모리 슬롯들로 압축하고, 이 메모리 슬롯들은 이후 타겟 언어 모델이 컨디셔닝할 수 있다. 이러한 설계로 ICAE는 4x 컨텍스트 압축을 얻을 수 있다. 너겟 2D[(219)]는 재구성을 가능하게 하도록 트레이닝되는 컴팩트한 "너겟"으로서 이력 컨텍스트를 나타낸다. 또한 LLaMA와 같이 쉽게 구할 수 있는 모델을 사용하여 초기화할 수 있는 유연성을 가지고 있다. 마지막으로 LongLLMLingua [(124)]는 LLM의 핵심 정보 인식을 향상시키기 위해 질문 인식 거친 대 세 압축, 문서 재정렬, 동적 압축 비율 및 압축 후 하위 시퀀스 복구를 포함하는 신속한 압축 기술을 소개한다.

#### 3.2.3. **Prompt Generation**.

프롬프트 생성(그림 19(b))은 수동 주석이 달린 데이터 대신 특정 및 관련 응답을 생성할 때 모델을 안내하는 효과적인 프롬프트를 자동으로 생성함으로써 효율성을 향상시킵니다. AutoPrompt[(249)]는 그래디언트-유도 탐색에 기초하여 다양한 태스크 세트에 대한 프롬프트를 생성하는 자동화된 방법을 제안한다. 이는 LLM 성능을 최적화하는 데 중추적인 역할을 강조하면서 데이터의 품질과 진정성을 정제하는 데 있어 인간이 쓴 텍스트의 중요성을 강조한다. TempLM[(347)]은 생성 및 템플릿 기반 방법론을 결합하여 LLM을 템플릿 기반 생성기로 증류하는 것을 제안하며, 데이터 대 텍스트 작업에 대한 조화 솔루션을 제공한다. 프롬프트젠[(348)]은 사전 학습된 LLM을 기반으로 지식 프로빙을 위한 동적 프롬프트 생성을 고려한 첫 번째 작업이다. 입력 문장을 조건으로 프롬프트를 자동으로 생성할 수 있으며 LAMA 벤치마크에서 자동 프롬프트를 능가합니다.

## 4. Llm Frameworks

**DeepSpeed.** Microsoft에서 개발한 DeepSpeed [(229)]는 LLM을 교육 및 배포하기 위한 통합 프레임워크입니다. 메가트론 튜링 NLG 530B[(253)] 및 BLOOM[(239)]과 같은 대형 모델을 훈련하는 데 사용되었다. 이 프레임워크 내에서 DeepSpeed-Inference는 기초 라이브러리이다. 이 모듈의 중추적인 특징은 대규모 모델 추론을 위해 GPU 메모리 제약을 해결하기 위해 만들어진 최적화 기법인 ZeRO-Inference[(226; 228)]이다. ZeRO-추론은 모델 상태들을 다수의 GPU들 및 CPU들에 분산시켜, 개별 노드들의 메모리 제약들을 관리하기 위한 접근법을 제공한다. DeepSpeed-Inference의 또 다른 양태는 그것의 딥 퓨전 메커니즘이며, 이는 반복 공간 차원들에 걸쳐 계산들을 타일링함으로써 글로벌 동기화를 위한 필요 없이 동작들의 퓨전을 허용한다[(149; 266; 180; 231)]. 이를 기반으로 DeepSpeed Model Implementations for Inference (DeepSpeed MII) 모듈은 인기 있는 딥러닝 모델의 배치 및 관리를 위한 전략을 제공한다. 성능, 유연성 및 비용 효율성을 강조하는 딥스피드 MII는 모델 추론을 개선하기 위해 진보된 최적화 기술을 통합한다[(228; 306; 329). 나아가 DeepSpeed-Chat[(328)]의 도입으로 생태계에 채팅 지원이 추가된다. 이 모듈은 딥스피드 트레이닝 시스템과 RLF(Human Feedback) [(88)]로부터의 강화 학습의 기법들을 통합하는, 상이한 스케일들에 걸쳐 챗봇 모델들을 트레이닝하는 것에 초점을 맞춘다. 특히, ZeRO-Offload 최적화기[(231)]의 통합은 메모리 용량에 관계없이 CPU와 GPU 모두에 대한 교육을 용이하게 한다.

**메가트론.** 메가트론 [(250)]은 GPT [(222)] 및 T5 [(224)]와 같은 LLM의 교육 및 배포를 간소화하기 위한 Nvidia의 노력을 구성합니다. 이것은 Nvidia의 메가트론 모델에 사용되는 기본 프레임워크이다[(140; 197; 250)] 메가트론은 Nvidia GPU에 대한 다양한 특수 도구와 프레임워크를 포함한다. 메가트론의 설계의 핵심은 처리 속도와 메모리 활용도를 모두 최적화하기 위해 여러 GPU에 분산되어 있는 모델의 텐서 연산을 전략적으로 분해하여 모델 충실도를 손상시키지 않으면서 훈련 처리량을 향상시키는 것이다[(250). 메가트론은 또한

\begin{table}
\begin{tabular}{p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt}} \hline \hline
**Framework** & **Training Fine-Tuning** & **Inference** & **Features** \\ \hline DeepSpeed & & & Data Parallelism, Model Parallelism, Pipeline Parallelism, Prompt Batching, Quantisation, Kernel Optimizations, Compression, Mixture of Experts. \\ Megatron & & & Data Parallelism, Model Parallelism, Pipeline Parallelism, Prompt Batching, Automatic Mixed precision, Selective activation Recomputation \\ Alpa & & & Data Parallelism, Model Parallelism, Pipeline Parallelism, Operator Parallelism, Automated Model-Parallel Training, Prompt Batching \\ Colossal AI & & & Data Parallelism, Model Parallelism, Pipeline Parallelism, Mixed Precision Training, Gradient accumulation, heterogeneous Distributed Training, Prompt Batching, Quantization \\ FairScale & & & Data Parallelism, Model Parallelism, Pipeline Parallelism, Activation Checkpointing, Model Offloading, Model scaling, Adascale Optimization \\ Pax & & & Data Parallelism, Model Parallelism, Kernel Optimization \\ Composer & & & Fully Sharded Data Parallelism, Elastic sharded checkpointing, Flash Attention \\ VLAM & & & Data Parallelism, Model Parallelism, Tensor Parallelism, Efficient management via PagedAttention, Optimized CUDA kernels, Dynamic Batching, Quantization \\ OpenLLM & & & Distributed Finetuning and Inference, Integration with BentoML, LangChain, and Transformers Agents, Prometheus Metrics, Token Streaming \\ Ray LLM & & & Distributed Inference, Integration with Alpa, Prompt Batching, Quantization, Prometheus Metrics \\ MLC LLM & & & Distributed Inference, Compiler Acceleration, Prompt Batching, Quantization \\ Sax & & & Distribute Inference, Serves PaxML, JAX, and PyTorch models, Slice Serving, Prometheus Metrics \\ Mosec & & & Distribute Inference, Dynamic Batching, Rust-based Task Coordinator, Prometheus Metrics \\ LLM Foundry & & & Distribute Inference, Dynamic Batching, Prompt Batching \\ \hline \hline \end{tabular}
\end{table}
표 2: LLM 프레임워크의 비교.

대형 트랜스포머 모델에 대한 추론 프로세스를 최적화하기 위한 FasterTransformer[199]. 또한, FasterTransformer는 FP16 및 INT8과 같은 다양한 정밀 모드를 처리하는 데 사용되어 다양한 운영 요구를 충족한다. 이 시스템은 또한 튜링과 볼타와 같은 특정 GPU 아키텍처에 맞춘 알고리즘을 통합하여 성능 최적화를 강조한다[199]. 마지막으로, 메가트론은 개발자에게 LLM에 맞게 특별히 조정된 고급 도구와 최적화를 제공하는 텐서RT-LLM을 사용하여 실시간 응용 프로그램의 대기 시간을 크게 줄이고 처리량을 향상시킨다. 특히, TensorRT-LLM은 FasterTransformer [275]에서 최적화된 커널을 통합하고 텐서 병렬성을 사용하여 개발자의 개입이나 모델 변경 없이 여러 GPU 및 서버에 걸쳐 규모에서 효율적인 추론을 용이하게 한다.

**Alpa.** Alpa [356]은 대규모 신경망을 학습 하 고 서비스 하기 위한 라이브러리입니다. 알파는 분산 딥러닝 성능의 총체적인 향상을 목표로 운영자 간 및 운영자 내 병렬성을 전략적으로 다룬다. GPT-2[222], BLOOM[239], OPT[346], CodeGen[198]의 예시적인 구현들을 갖는다. 알파의 방법론의 핵심은 자동 병렬화이다. 자동 조정 프레임워크를 배치하여 알파는 특정 딥러닝 모델과 하드웨어 구성에 맞춘 최적의 병렬화 전략을 동적으로 식별한다. 또한, 알파는 데이터와 모델 병렬성을 모두 결합한 통합 설계를 보여준다[162; 364]. 그렇게 함으로써, 알파는 이러한 병렬화 기술의 집단적 이점을 활용함으로써, 서비스 동안 최적화된 리소스 활용 및 향상된 트레이닝 처리량을 유도한다.

**ColossalAI.** ColossalAI [152]는 대규모 분산 훈련의 문제를 해결하기 위해 조정된 프레임워크입니다. [284]. 콜로살AI는 확장성, 효율성, 다재다능성을 조화롭게 하는 통일된 솔루션을 제공합니다. LLaMA[277], GPT-3[20], GPT-2[222], BERT[64], PaLM, OPT[346], ViT[68]에 대한 구현들을 갖는다. 거대 인공 지능의 설계의 핵심은 전체론적 통합에 대한 강조이다. 데이터 전처리에서 모델 학습 및 검증에 이르기까지 딥러닝 파이프라인의 다양한 구성 요소를 통합함으로써, ColossalAI는 단편화를 줄이고 워크플로우 효율성을 향상시키는 간소화된 플랫폼을 제공한다[16]. 이 통합 접근법은 분산 환경에서 대규모 훈련을 조정하는 것과 종종 관련된 복잡성을 완화한다. 나아가, 딥 러닝 연구 및 응용의 동적 풍경을 인식함으로써, 시스템은 본질적으로 모듈식인 것으로 설계된다[38]. 또한, 프레임워크는 몇 가지 다른 진보된 최적화 기술[16; 73; 74; 153; 174; 285] 및 양자화, 구배 축적 및 혼합 정밀도와 같은 특징을 통합한다. 최첨단 알고리즘과 방법론을 활용함으로써, Colossal-AI는 병렬 훈련에 내재된 계산 및 통신 오버헤드를 모두 최적화하여 훈련 시간을 줄이고 모델 성능을 향상시키려고 한다.

**FairScale.** 메타에서 개발한 FairScale [72]는 고성능 및 대규모 교육 이니셔티브에 전념하는 PyTorch의 확장 라이브러리입니다. 페어스케일의 에토스는 사용자의 인지 오버헤드를 최소화하기 위해 페어스케일의 API에 대한 이해와 활용의 용이성을 강조하는 사용성, 사용자의 훈련 루프 내에서 여러 페어스케일 API의 원활한 통합을 보증하여 유연성을 촉진하는 모듈성, 그리고 페어스케일의 API를 통해 최적의 스케일링과 효율성을 제공하는 것을 중심으로 하는 성능의 세 가지 기본 원칙에 뿌리를 두고 있다. 추가적으로, FairScale은 광범위한 신경망들의 트레이닝 동작들을 스케일링하기 위한 바람직한 방법으로서 완전 샤드드 데이터 병렬(Fully Sharded Data Parallel; FSDP)에 대한 지원을 제공한다. 따라서 분산 훈련 및 추론을 위한 강력한 도구입니다. 또한, 활성화 체크포인팅, 효율적인 모델 오프로딩 및 스케일링을 지원하는 자원 제약 시스템에서 훈련을 위한 주요 기능을 가지고 있다.

**Pax.** Google에서 개발한 Pax [9]는 JAX 기반 효율적인 분산 교육 프레임워크입니다. Pax는 PaLM-2[7] 및 Bard[109]를 훈련시키는데 사용되었다. 확장성을 목표로 하며 다양한 양식(텍스트, 비전, 음성 등)을 포함한 대규모 모델 교육을 위한 참조 예를 가지고 있다. JAX와 많이 통합되어 있으며 JAX 생태계의 많은 라이브러리를 사용한다. 팩스에는 순차적인 데이터 처리를 처리하기 위한 SeqIO, 최적화를 위한 Optax, 구성을 위한 Fiddle, 체크포인팅을 위한 Orbax, 자동 분화를 위한 PyGLove, 고성능 신경망을 생성하기 위한 Flax 등 많은 핵심 구성 요소가 포함되어 있다.

**Composer.** Mosaic ML에 의해 설계된 Composer [193]은 신경망의 훈련을 더 빠르고 효율적으로 만드는 것을 목표로 합니다. Mosaic ML의 MPT 7B 및 MPT 30B 모델과 Replit의 Code V-1.5 3B를 훈련하는 데 사용되었다. 라이브러리는 PyTorch 위에 빌드되며 사용자가 자체 훈련 루프에 통합하거나 더 나은 경험을 위해 Composer 트레이너와 함께 사용할 수 있는 속도 향상 방법의 컬렉션을 제공합니다. 효율적인 병렬 처리를 위한 FSDP, 강력한 간헐적 훈련을 위한 탄력적인 공유 체크포인팅 및 훈련 중에 클라우드 Blob 스토리지에서 데이터 세트를 즉시 다운로드할 수 있는 데이터 세트 스트리밍 구현을 지원합니다. 따라서 합성기는 훈련 루프에 직접 방법을 통합하기 위한 함수 API와 PyTorch 기반 훈련 루프를 자동으로 구현하는 트레이너 API를 사용하여 다재다능하도록 설계되어 ML 개발자의 작업량을 줄인다.

**vLLM.** vLLM [142]는 LLM을 서비스하는 접근 방식의 방법론적 변화를 나타냅니다. vLLM의 설계의 핵심은 설정된 토큰 수에 대한 주의 키 및 값(KV) 캐시를 세그먼트화하는 메커니즘인 PagedAttention입니다. 연속 공간 저장소와 달리 KV 캐시에 대한 PagedAttention의 블록은 가상 메모리 관리와 유사하게 유연하게 저장됩니다. 이는 동일한 요청 또는 심지어 상이한 요청들에 묶여 있는 다양한 시퀀스들에 걸쳐 블록 레벨에서의 메모리 공유를 용이하게 하고, 따라서 주의 메커니즘들을 처리하는데 있어서 메모리 관리 효율을 향상시킨다. 또한, 온-디맨드 버퍼 할당을 허용하는 한편, 블록들의 크기가 균일하게 됨에 따라 외부 단편화를 제거한다. 또한, vLLM은 적응 로딩 기술을 통합한다. 휴리스틱 방법론들에 뿌리를 둔 이 기법은 입력에 기초하여 메모리에 로딩될 페이지들의 수를 식별한다. 이를 보완하여 vLLM은 매개변수 압축 전략도 통합한다. 모델 파라미터를 압축된 상태로 저장하고 실시간 서빙 동안 압축을 해제함으로써, vLLM은 메모리 사용을 더욱 최적화한다. 또한, vLLM은 최신의 양자화 기법과 빠른 모델 실행을 지원하는 최적화된 CUDA 커널을 지원한다. 라이브러리는 AMD의 ROCm GPU에 대한 지원도 추가했다. 따라서 vLLM은 분산 훈련에 유용한 도구일 뿐만 아니라 워크로드를 제공하는 효율적인 고처리량 모델을 처리할 수 있다.

**OpenLLM.** OpenLLM [210]은 프로덕션 환경 내에서 LLM의 배포 및 운영에 대한 포괄적인 접근 방식을 설명합니다. 벤투ML 생태계 내에 고정된 OpenLLM은 LLM의 훈련과 실제 애플리케이션으로의 원활한 통합 사이의 격차를 해소하기 위해 만들어졌다. OpenLLM의 정의적 특성은 모듈성과 확장성에 중점을 둔다. OpenLLM은 생산 환경의 다양한 요구를 인식하고 컴포넌트 기반 아키텍처를 촉진한다. 가치 제안을 더욱 강화한 OpenLLM은 고급 캐싱 메커니즘을 통합합니다. 이러한 메커니즘을 활용함으로써, 시스템은 반복적인 질의를 최적화하여 운영 비용을 줄이고 응답 시간을 향상시키는 것을 목표로 한다. 또한 OpenLLM의 설계에는 강력한 모니터링 및 로깅 도구가 포함되어 있어 성능 조정 및 문제 해결에 운영 통찰력을 쉽게 사용할 수 있습니다.

**Ray-LLM.** Ray-LLM [217]은 LLM의 배포 및 운영을 최적화 하는 것을 목표로 하는 Ray 에코 시스템 [192]과 LLM의 전략적 융합을 나타냅니다. 최첨단 모델 아키텍처와 확장 가능한 인프라의 교차점에 위치한 Ray-LLM은 LLM 활용의 패러다임을 재정의하려고 한다. Ray-LLM 접근법의 핵심은 Ray의 고유한 분산 컴퓨팅 능력을 활용하는 것이다. 레이-LLM은 LLM의 계산 요구 사항을 인식하여 Ray의 분산 태스크 스케줄링 및 실행 메커니즘을 통합하여 LLM 태스크가 가용 리소스에 걸쳐 효율적으로 분산되도록 한다. 이러한 원활한 통합은 잠재적으로 향상된 모델 성능, 지연 시간 감소 및 최적화된 리소스 활용으로 이어집니다. 레이-LLM은 레이 생태계 위에 구축되기 때문에 대규모 모델을 클러스터에서 빠르게 프로토타입, 훈련 및 배치하기에 좋은 라이브러리이다. 고급 모니터링 지원도 함께 제공되어 서빙에 사용할 수 있습니다.

**MLC-LLM.** MLC-LLM (Zheng et al., 2019)은 개인이 다양한 장치 배열에서 AI 모델을 개발, 최적화 및 배포할 수 있도록 권한을 부여하고자 합니다. MLC-LLM의 접근법의 핵심은 디바이스 네이티브 AI의 개념이다. 오늘날 사용되는 디바이스의 방대한 스펙트럼을, 고급 서버로부터 스마트폰에 이르기까지, MLC-LLM은 모델을 컴파일하고 각각의 디바이스의 특정 능력 및 제약에 본질적으로 맞춤화된 프로세스로 전개한다(Zheng et al., 2019; Wang et al., 2020; Wang et al., 2020). 이 장치 고유 초점은 AI 모델이 효율적일 뿐만 아니라 작동하는 환경에 매우 최적화되도록 합니다. 엣지 디바이스에서 프로토타이핑을 위한 모델을 컴파일하고 최적화하는 데 중점을 둔 MLC-LLM은 온 디바이스 AI 모델을 배포하기 위한 강력한 도구이며 다양한 디바이스에 걸쳐 처리량 측면에서 최첨단 성능을 나타낸다.

**Sax.** Sax(Sax 등, 2019)는 추론 작업을 위해 Pax, JAX 및 PyTorch 모델을 배포하기 위해 Google에서 설계한 플랫폼입니다. 색스 내에는 여러 모델 서버와 연결된 관리 서버로 구성된 색스 셀(또는 색스 클러스터)이라고 하는 단위가 있습니다. 관리자 서버의 역할은 다양 합니다. 모델 서버를 모니터링 하 고, 추론을 위해 게시 된 모델을 이러한 서버에 할당 하 고, 클라이언트가 특정 게시 된 모델에 적절 한 모델 서버를 찾을 수 있도록 안내 합니다. 색스는 기본적으로 팩스 프레임워크에 보완적이며, 팩스는 대량 분산 워크로드에 초점을 맞추는 반면, 색스는 모델 서빙에 맞춰져 있다.

**Mosec.** Mosec(Mosec, 2019)는 특히 클라우드 환경에서 대규모 딥러닝 모델을 제공하기 위해 설계되었습니다. 머신 러닝 모델의 서빙을 백엔드 서비스와 마이크로 서비스로 간소화하기 위해 제작되었습니다. 주요 기능에는 Rust 기반 웹 계층 및 작업 조정으로 인한 고성능, 사용하기 쉬운 Python 인터페이스, 동적 배치, 혼합 워크로드를 처리하기 위한 파이프라인 단계, 모델 워밍업, 우아한 종료 및 프로메테우스 모니터링 메트릭을 사용하여 Kubernetes 또는 기타 컨테이너 오케스트레이션 시스템에서 쉽게 관리할 수 있습니다. 모섹은 클라우드 생태계를 중심으로 웹 계층으로 모델을 효율적으로 서비스하는 데 매우 적합하여 개발자가 모델 최적화 및 백엔드 로직에 집중할 수 있다.

**LLM Foundry.** LLM Foundry(Foundry, 2019)는 Composer 및 MosaicML 플랫폼을 사용하여 추론을 위해 LLM을 조정, 평가 및 배포하기 위한 라이브러리입니다. 효율적인 배치를 위해 분산 추론, 동적 배칭 및 신속한 배칭을 지원합니다. 무료 교육 프레임워크 작성기와 유사하게 LLM 파운드리(Foundry)는 사용하기 쉽고 효율적이며 유연하도록 설계되어 LLM의 최신 기술로 신속한 실험을 가능하게 한다. 또한 Mosaic의 Prerained Transformers (MPT) (FlashAttention (Mosaic, 2019) 및 ALBi (MosaicML, 2019)와 같은 기능을 지원 하는 GPT 스타일 모델)에 대 한 간단한 인터페이스를 제공 합니다. 이것은 MosaicML의 Composer 프레임워크와 보완적이며 Composer는 분산 훈련에 중점을 두고 있지만 LLM Foundry는 이러한 모델을 배포하고 최신 기술로 신속한 실험을 가능하게 하는 지원을 제공한다.

## 5. 비고 요약

본 조사에서는 LLMs의 민주화를 목표로 하는 연구의 중요한 영역인 효율적인 LLMs에 대한 체계적인 검토를 제공한다. 우리는 효율적인 LLM의 필요성에 대한 동기 부여부터 시작한다. 분류법에 따라 모델 중심 및 데이터 중심 관점에서 LLM에 대한 알고리즘 수준 및 시스템 수준 효율적인 기술을 각각 검토한다. 또한, 효율적인 LLM에 중요한 특정 최적화 및 기능을 가진 LLM 프레임워크를 검토한다. 우리는 효율성이 LLM 및 LLM 지향 시스템에서 점점 더 중요한 역할을 할 것이라고 믿는다. 이 조사를 통해 연구자와 실무자가 이 분야에서 빠르게 시작하고 효율적인 LLM에 대한 새로운 연구를 고무하는 촉매 역할을 할 수 있기를 바란다.

## References

* (1)
* Agarwal 등(2023) Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. 2023. Generalized Knowledge Distillation for Auto-regressive Language Models. arXiv:2306.13649 [cs.LG]
* Ahmadian 등(2023) Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Stephen Gou, Phil Blunsom, Ahmet Ustin, and Sara Hooker. 2023. Intriguing Properties of Quantization at Scale. arXiv:2305.19268 [cs.LG]
* Ainslie 등(2023) Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sangghai. 2023. QGA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. arXiv:2305.13245 [cs.CL]
* Alberti 등(2023) Silas Alberti, Niclas Dem, Laura Thesing, and Gitta Kutyniok. 2023. Summermer: Universal Approximation for Efficient Transformers. arXiv:2307.02301 [cs.LG]
* Aminabadi 등(2022) Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, and Yuxiong He. 2022. DeepSpeed-Inference: 전례 없는 규모에서 변압기 모델의 효율적인 추론을 가능하게 한다. _Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis (SC '22)_ 에서. IEEE Press, Dallas, Texas, Article 46, 15 페이지.
* Anagnostidis 등(2023) Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, and Thomas Hofmann. 2023. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. arXiv:2305.15805 [cs.CL]
* Anil 등(2021) Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Paul Barham, Jan Diaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Josh 그래서 다니엘 손, 사이먼 토쿠민, 다샤 발터, 비제이 바수데반, 키란 보드라할리, 쉬에지 왕, 피동 왕, 지루이 왕, 도왕, 존 위팅, 유화이 우, 켈빈 쉬, 윤한 쉬, 린팅 쉬, 펑청 인, 자후이 유, 차오 장, 스티븐 정, 세 정, 웨이강 저우, 데니 저우, 슬라브 페트로프, 용후이 우가 있다. 2023. PaLM 2 Technical Report. arXiv:2305.10403 [cs.CL]
* Artetxe 등(2022) Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Ves Stoyanov. 2022. 전문가 혼합을 이용한 효율적인 대규모 언어 모델링. arXiv:2112.10684 [cs.CL]
* 작성자(2023) 팩스 작성자. 2023. Pax: A Java-based Machine Learning Framework for Large Scale Models. [https://github.com/google/paxml] (https://github.com/google/paxml). GitHub 리포지토리입니다.
* 저자(2023) Sax Authors. 2023. Sax. [https://github.com/google/saxml] (https://github.com/google/saxml). 2023-10-07
* Bachlechner 등(2021) Thomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley. 2021. ReZero가 필요한 전부입니다. 큰 깊이에서 빠른 수렴입니다. _Proceedings of the 30-7th Conference on Uncertainty in Artificial Intelligence (Proceedings of Machine Learning Research)_, Cassio de Campos and Marloes H. Maathuis (Eds.), Vol. 161. PMLR, online, 1352-1361. [https://proceedings.mlr.press/v161/bachlechner21a.html](https://proceedings.mlr.press/v161/bachlechner21a.html)
* Bansal 등(2022) Trapit Bansal, Salaheddin Alzubi, Tong Wang, Jay-Yoon Lee, and Andrew McCallum. 2022. 메타-어댑터: 메타-러닝을 통한 파라미터 효율 적인 미세 조정. [자동화된 기계 학습에 대한 첫 번째 회의(메인 트랙)]에서요. PMLR, Baltimore, US, -. [https://openreview.net/forum?id=BCGNf-prLg5] (https://openreview.net/forum?id=BCGNf-prLg5)
* Beltagy 등(2020) Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-Document Transformer. arXiv:2004.05150 [cs.CL]
* Bertsch 등(2023) Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. 골리 2023. Unlimformer: Long-Range Transformers with Unlimited Length Input. arXiv:2305.01625 [cs.CL]
* Besta 등(2023) Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstavski, Hubert Niewaidomski, Piotr Nyczyk, and Torsten Hoefler. 2023. 사상의 그래프: 대형 언어 모델을 이용한 정교한 문제 해결. arXiv:2308.09687 [cs.CL]
* Bian 등(2021) Zhengda Bian, Qifan Xu, Boxiang Wang, Yang You. 2021. Maximizing Parallelism in Distributed Training for Huge Neural Networks. arXiv:2105.14450 [cs.DC]
* Bansal et al. (2021) - Workshop on Challengees & Perspectives in Creating Large Language Models_, Angela Fan, Suzana Ilic, Thomas Wolf, and Matthias Galle (Eds.). Association for Computational Linguistics, virtual+Dublin, 95-136. [https://doi.org/10.18653/v1/2022.higcscience-1.9](https://doi.org/10.18653/v1/2022.higcscience-1.9)
* [18] bloc97. 2023. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation. [https://www.reddit.com/r/LocalLLaMA/comments/14lx7j5/ntkaware_scaled_rope_allows_llam_models_to_have/](https://www.reddit.com/r/LocalLLaMA/comments/14lx7j5/ntkaware_scaled_rope_allows_llam_models_to_have/). Accessed: 2023-12-19.
* [19] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. 2021. Understanding and Overcoming the Challenges of Efficient Transformer Quantization. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 7947-7969. [https://doi.org/10.18653/v1/2021.emnlp-main.627](https://doi.org/10.18653/v1/2021.emnlp-main.627)
* [20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
* [21] Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. 2022. Recurrent memory transformer. _Advances in Neural Information Processing Systems_ 35 (2022), 11079-11091.
* [22] Neil Burgess, Jelena Milanovic, Nigel Stephens, Konstantinos Monachopoulos, and David Mansell. 2019. Bifloat16 Processing for Neural Networks. In _2019 IEEE 26th Symposium on Computer Arithmetic (ARITH)_. IEEE Press, Kyoto, 88-91. [https://doi.org/10.1109/ARITH.2019.00022](https://doi.org/10.1109/ARITH.2019.00022)
* [23] Lucas Caccia, Edoardo Ponti, Zhan Su, Matheus Pereira, Nicolas Le Roux, and Alessandro Sordoni. 2023. Multi-Head Adapter Routing for Cross-Task Generalization. arXiv:2211.03831 [cs.AI]
* [24] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, and Tri Dao. 2023. Medusa: Simple framework for accelerating llm generation with multiple decoding heads.
* [25] Yihan Cao, Yanbin Kang, Chi Wang, and Lichao Sun. 2023. Instruction Mining: When Data Mining Meets Large Language Model Finetuning. arXiv:2307.06290 [cs.CL]
* [26] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A survey on evaluation of large language models. _arXiv preprint arXiv:2307.03109_ (2023).
* [27] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. 2023. QuIP: 2-Bit Quantization of Large Language Models With Guarantees. arXiv:2307.13304 [cs.LG]
* [28] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Re. 2021. Scatterbrain: Unifying sparse and low-rank attention. _Advances in Neural Information Processing Systems_ 34 (2021), 17413-17426.
* [29] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. 2023. Accelerating Large Language Model Decoding with Speculative Sampling. arXiv:2302.01318 [cs.CL]
* [30] Chang Chen, Min Li, Zhihua Wu, Dianhai Yu, and Chao Yang. 2022. TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training. _Advances in Neural Information Processing Systems_ 35 (2022), 22173-22186.
* [31] Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and Qun Liu. 2021. Bert2BERT: Towards Reusable Pretrained Language Models. arXiv:2110.07143 [cs.CL]
* [32] Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Lidong Bing. 2023. CLEX: Continuous Length Extrapolation for Large Language Models. arXiv:2310.16450 [cs.CL]
* [33] Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yanggong, and Junbo Zhao. 2023. Maybe Only 0.5Exploration of Low Training Data Instruction Tuning. arXiv:2305.09246 [cs.AI]
* [34] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. 2023. AlpaGasus: Training A Better Alpaca with Fewer Data. arXiv:2307.08701 [cs.CL]
* [35] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.

2021. 코드 교육을 받은 대규모 언어 모델 평가 arXiv:2107.03374 [cs.LG]
* [36] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. Extending Context Window of Large Language Models via Positional Interpolation. arXiv:2306.15595 [cs.CL]
* [37] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: An Automated End-to-End Optimizing Compiler for Deep Learning. In _13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)_. USENIX Association, Carlsbad, CA, 578-594. [https://www.usenix.org/conference/osdi18/presentation/chen](https://www.usenix.org/conference/osdi18/presentation/chen)
* [38] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training Deep Nets with Sublinear Memory Cost. arXiv:1604.06174 [cs.LG]
* [39] Wuyang Chen, Yanqi Zhou, Nan Du, Yanping Huang, James Laudon, Zhifeng Chen, and Claire Cui. 2023. Lifelong Language Pretraining with Distribution-Specialized Experts. In _Proceedings of the 40th International Conference on Machine Learning (ICML'23)_. JMLR.org, Honolulu, Hawaii, USA, Article 213, 13 pages.
* [40] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V. Le. 2023. Symbolic Discovery of Optimization Algorithms. arXiv:2302.06675 [cs.LG]
* [41] Yongrui Chen, Haiyun Jiang, Xinting Huang, Shuming Shi, and Guilin Qi. 2023. TeGit: Generating High-Quality Instruction-Tuning Data with Text-Grounded Task Design. arXiv:2309.05447 [cs.CL]
* [42] Yukawa Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023. LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. arXiv:2309.12307 [cs.CL]
* [43] Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022. Meta-learning via Language Model In-context Tuning. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, Smaranda Muresan, Preslav Nakov, and Alime Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 719-730. [https://doi.org/10.18653/v1/2022.acl-long.53](https://doi.org/10.18653/v1/2022.acl-long.53)
* [44] Zeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, and Kyle Richardson. 2023. DISCO: Distilling Counterfactuals with Large Language Models. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 5514-5528. [https://doi.org/10.18653/v1/2023.acl-long.302](https://doi.org/10.18653/v1/2023.acl-long.302)
* [45] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting Language Models to Compress Contexts. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 3829-3846. [https://doi.org/10.18653/v1/2023.emnlp-main.232](https://doi.org/10.18653/v1/2023.emnlp-main.232)
* [46] Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xin Song, Xian-Ling Mao, et al. 2022. On the representation collapse of sparse mixture of experts. _Advances in Neural Information Processing Systems_ 35 (2022), 34600-34613.
* [47] Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, and Lidong Bing. 2023. Contrastive Chain-of-Thought Prompting. arXiv:2311.09277 [cs.CL]
* [48] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating Long Sequences with Sparse Transformers. arXiv:1904.10509 [cs.LG]
* [49] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, et al. 2020. Masked language modeling for proteins via linearly scalable long-context transformers. _arXiv preprint arXiv:2006.03555_ (2020).
* [50] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. 2021. Rethinking Attention with Performers. [https://openreview.net/forum?id=UaZuk0WRH](https://openreview.net/forum?id=UaZuk0WRH)
* [51] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselin Levskaya, Sanjay Ghemawat, Sunjeng Dev, Henry Michaelswaki, Xavier Garcia, Vedant Misra, Kevin Robinson, Lian Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontelac Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thammalamayan Sankaranarayanan Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways. arXiv:2204.02311 [cs.CL]
* [52] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, GauravMishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling Instruction-Finetuned Language Models. arXiv:2210.11416 [cs.LG]
* [53] Luciano Del Corro, Allie Del Giorno, Sahaj Agarwal, Bin Yu, Ahmed Awadallah, and Subhabrata Mukherjee. 2023. SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference. arXiv:2307.02628 [cs.CL]
* [54] Damai Dai, Li Dong, Shuming Ma, Bo Zheng, Zhifang Sui, Baobao Chang, and Furu Wei. 2022. StableMoE: Stable Routing Strategy for Mixture of Experts. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 7085-7095. [https://doi.org/10.18653/v1/2022.acl-long.489](https://doi.org/10.18653/v1/2022.acl-long.489)
* [55] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing. _Advances in Neural Information Processing Systems_ 33 (2020), 4271-4282.
* [56] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive Language Models beyond a Fixed-Length Context. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, Anna Korhonen, David Traum, and Lluis Marquez (Eds.). Association for Computational Linguistics, Florence, Italy, 2978-2988. [https://doi.org/10.18653/v1/P19-1285](https://doi.org/10.18653/v1/P19-1285)
* [57] Tri Dao. 2023. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. arXiv:2307.08691 [cs.LG]
* [58] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. 2022. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. _Advances in Neural Information Processing Systems_ 35 (2022), 16344-16359.
* [59] Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. 2023. Flash-Decoding for long-context inference. [https://pytorch.org/blog/flash-decoding/](https://pytorch.org/blog/flash-decoding/). Accessed: 2023-12-13.
* [60] Soham De and Sam Smith. 2020. Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks. _Advances in Neural Information Processing Systems_ 33 (2020), 19964-19975.
* [61] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. arXiv:2208.07339 [cs.LG]
* [62] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314 [cs.LG]
* [63] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuzmedelev, Elias Frantz, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. 2023. SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression. arXiv:2306.03078 [cs.CL]
* [64] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Minneapolis, Minnesota, 4171-4186. [https://doi.org/10.18653/v1/N19-1423](https://doi.org/10.18653/v1/N19-1423)
* [65] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. 2023. LongNet: Scaling Transformers to 1,000,000,000 Tokens. arXiv:2307.02486 [cs.CL]
* [66] Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. 2023. Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation. arXiv:2311.04254 [cs.AL]
* [67] Qingxitun Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A Survey on In-context Learning. arXiv:2301.00234 [cs.CL]
* [68] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In _International Conference on Learning Representations_. ICLR, Addis Ababa, Ethiopia. [https://openreview.net/forum?id=YichFdNTTY](https://openreview.net/forum?id=YichFdNTTY)
* [69] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. GLAM: Efficient Scaling of Language Models with Mixture-of-Experts. In _International Conference on Machine Learning_. PMLR, Baltimore, Maryland, 5547-5569.
* [70] Lance Eliot. 2021. Generative Pre-Trained Transformers (GPT-3) Certain To AI In The Law. [https://doi.org/10.2139/ssrn.3974887](https://doi.org/10.2139/ssrn.3974887)
* 라인 468. [https://github.com/facebookresearch/fairseq/blob/main/fairseq/optim/fp16_optimizer.py](https://github.com/facebookresearch/fairseq/blob/main/fairseq/optim/fp16_optimizer.py) 2023-12-13
* [72] FairScale authors. 2021. FairScale: A general purpose modular PyTorch library for high performance and large scale training. [https://github.com/facebookresearch/fairscale](https://github.com/facebookresearch/fairscale).

* Fang et al. (2023) J. Fang et al. 2023. Parallel Training of Pre-Trained Models via Chunk-Based Dynamic Memory Management. _ IEEE Transactions on Parallel and Distributed Systems_34, 1(2023), 304-315.
* Fang 등(2022) Jirui Fang, Geng Zhang, Jiatong Han, Shenggui Li, Zhengda Bian, Yongbin Li, Jin Liu, Yang You. 2022. 대용량 추천 시스템 임베딩을 위한 주파수 인식 소프트웨어 캐시. arXiv:2208.05321 [cs.IR]
* Fedus 등(2022) William Fedus, Barret Zoph, and Noam Shazeer. 2022. 스위치 트랜스포머: 간단하고 효율적인 희소성을 갖는 조 단위의 파라미터 모델들로 스케일링. _ The Journal of Machine Learning Research_23, 1(2022), 5232-5270.
* Feng 등(2023) Siyuan Feng, Bohan Hou, Hongyi Jin, Wuwei Lin, Junru Shao, Ruihang Lai, Zihao Ye, Lianmin Zheng, Cody Hao Yu, Yong Yu, and Tianqi Chen. 2023. TensorIR: 자동 텐서화 프로그램 최적화를 위한 추상화. _Proceedings of 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2_(Vancouver, BC, Canada) _(ASPLOS 2023)_. Association for Computing Machinery, New York, NY, USA, 804-817. [https://doi.org/10.1145/3575693.3576933](https://doi.org/10.1145/3575693.3576933)
* Frantar and Alistarh (2022) Elias Frantar and Dan Alistarh. 202. 최적 뇌 압축: 정확한 훈련 후 양자화 및 프루닝을 위한 프레임워크. _신경 정보 처리 시스템의 발전_ 에서, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyungghyun Cho (Eds.). New Orleans, Louisiana. [https://openreview.net/forum?id=ksVGCOIOEba. (https://openreview.net/forum?id=ksVGCOIOEba)
* Frantar and Alistarh (2023) Elias Frantar and Dan Alistarh. 2023. SparseGPT: 대용량 언어 모델들은 원샷으로 정확하게 프루닝될 수 있다. _ ArXiv_ abs/2301.00774 (2023).
* Frantar 등(2023) Elias Frantar, Saleh Ashkoos, Torsten Hoefler, and Dan Alistarh. 2023. OPTQ: Generative Pre-trained Transformer에 대한 정확한 양자화. _The Eleventh International Conference on Learning Representations_. [https://openreview.net/forum?id=ctcbBPnfwos] (https://openreview.net/forum?id=ctcbBPnfwos)
*Y. Fu et al.(2023) Daniel Y. 푸, 트리다오, 칼레드 K 사브 아민 토마스, 아트리 루드라 크리스토퍼 레 2023. Hungry Hungry Hippos: State Space Model을 이용한 언어 모델링 방향. arXiv:2212.14052[cs.LG]
* Fu et al.(2023) Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. 2023. Lookahead Decoding을 이용한 LLM Inference의 Sequential Dependency Breaking. [https://lmsys.org/blog/2023-11-21-lookahead-decoding/] (https://lmsys.org/blog/2023-11-21-lookahead-decoding/)
* Fu et al.(2023) Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023. Smaller Language Models toward Multi-Step Reasoning. _제40회 기계 학습에 관한 국제 회의의 회보(Machine Learning Research의 Proceedings)_ 에서, Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.), Vol. 202. PMLR, Honolulu, Hawaii, 10421-10430. [https://proceedings.mlr.press/v202/fu23d.html](https://proceedings.mlr.press/v202/fu23d.html)
* Gale 등(2023) Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. 2023. MegaBlocks: Mixture-of-Exprues를 사용한 효율적인 희소 트레이닝. _ Proceedings of Machine Learning and Systems_5 (2023).
* Ge 등(2023) Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. 2023. In-context Autoencoder for Context Compression in a Large Language Model. arXiv:2307.06945 [cs.CL]
* Glass 등(2020) Michael Glass, Alfio Gliozzo, Rishav Chakravarti, Anthony Ferritto, Lin Pan, G P Shrivastava Bhargav, Dinesh Garg, and Avi Sil. 2020. 질의응답을 위한 스팬 선택 사전 트레이닝. _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, Dan Jurafsky, Joyce Chai, Natalie Schluter and Joel Tetreault (Eds.). Association for Computational Linguistics, Online, 2773-2782. [https://doi.org/10.18653/v1/2020.acl-main.247](https://doi.org/10.18653/v1/2020.acl-main.247)
* Gong 등(2019) Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. 2019. Efficient Training of BERT by Progressively Stacking. _Proceedings of 36th International Conference on Machine Learning (Proceedings of Machine Learning Research)_, Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.), Vol. 97. PMLR, Long Beach, California, 2337-2346. [https://proceedings.mlr.press/v97/gong19a.html](https://proceedings.mlr.press/v97/gong19a.html)
* Gou 등(2021) Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. 2021. Knowledge Distillation: A survey. _ International Journal of Computer Vision_ 129 (2021), 1789-1819.
* Griffith 등(2021) Shane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea L Thomaz. 2021. Policy Shaping: Human Feedback with Reinforcement Learning 통합. _Advances in Neural Information Processing Systems_, C.J. Burges, L. 보투모 웰링, Z Ghahramani, and K.Q. Weinberger (Eds.), Vol. 26. Curran Associates, Inc. [https://proceedings.neurips.cc/paper%5Ffiles/paper/2013/file/e034fbb66aacc1d48f45ddfb08da98-Paper.pdf] (https://proceedings.neurips.cc/paper%5Ffiles/paper/2013/file/e034fbb66aacc1d48f45ddfb08da98-Paper.pdf)
* Gu and Dao (2023) Albert Gu and Tri Dao. 2023. Mamba: Selective State Spaces를 갖는 Linear-Time Sequence Modeling. arXiv:2312.00752 [cs.LG]
* Gu et al.(2022) Albert Gu, Karan Goel, and Christopher Re. 2022. 구조화된 상태 공간을 갖는 긴 시퀀스를 효율적으로 모델링. _International Conference on Learning Representations_. [https://openreview.net/forum?id=uYLFo2v1rAC] (https://openreview.net/forum?id=uYLFo2v1rAC)
* Gu et al.(2021) Xiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, and Jiawei Han. 2021. On the Transformer Growth for Progressive BERT Training. _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, Online, 5174-5180. [https://doi.org/10.18653/v1/2021.naacl-main.406](https://doi.org/10.18653/v1/2021.naacl-main.406)
* Gu et al.(2023) Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Large Language Model의 Knowledge Distillation. arXiv:2306.08543 [cs.CL]* [93] Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. 2022. PPT: Few-shot Learning을 위한 사전 트레이닝 프롬프트 튜닝. arXiv:2109.04332 [cs.CL]
* [94] Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan Yang, Yunxin Liu, Minyi Guo, and Yuhao Zhu. 2023. OilvE: Accelerating Large Language Models via Hardware-Friendly Outlier-Victim Pair Quantization. In _Proceedings of the 50th Annual International Symposium on Computer Architecture_ (Orlando, FL, USA) _(ISCA '23)_. Association for Computing Machinery, New York, NY, USA, Article 3, 15 pages. [https://doi.org/10.1145/3579371.3589038](https://doi.org/10.1145/3579371.3589038)
* [95] Ankit Gupta, Albert Gu, and Jonathan Berant. 2022. Diagonal State Spaces are as Effective as Structured State Spaces. _Advances in Neural Information Processing Systems_ 35 (2022), 22982-22994.
* [96] Ahan Gupta, Yueming Yuan, Yanqi Zhou, and Charith Mendis. 2023. FLuRKA: Fast fused Low-Rank & Kernel Attention. arXiv:2306.15799 [cs.LG]
* [97] Tae Jun Ham, Sung Jun Jung, Seonghak Kim, Young H. Oh, Yeonhong Park, Yoonho Song, Jung-Hun Park, Sanghee Lee, Kyoung Park, Jae W. Lee, and Deog-Kyoon Jeong. 2020. A\({}^{3}\): Accelerating Attention Mechanisms in Neural Networks with Approximation. In _2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)_. 328-341. [https://doi.org/10.1109/HPCA47549.2020.00035](https://doi.org/10.1109/HPCA47549.2020.00035)
* [98] Tae Jun Ham, Yejin Lee, Seong Hoon Seo, Soosung Kim, Hyunji Choi, Sung Jun Jung, and Jae W. Lee. 2021. ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks. In _2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)_. 692-705. [https://doi.org/10.1109/ISCA52012.2021.00060](https://doi.org/10.1109/ISCA52012.2021.00060)
* [99] Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David P. Woodruff, and Amir Zandieh. 2023. HyperAttention: Long-context Attention in Near-Linear Time. arXiv:2310.05869 [cs.LG]
* [100] Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, and Jie Tang. 2021. FastMoE: A Fast Mixture-of-Expert Training System. arXiv:2103.13262 [cs.LG]
* [101] Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng Shi, and Qin Li. 2022. FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models. In _Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming_. 120-134.
* [102] Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. 2023. A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics. _arXiv preprint arXiv:2310.05694_ (2023).
* [103] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. In _2015 IEEE International Conference on Computer Vision (ICCV)_. 1026-1034. [https://doi.org/10.1109/ICCV.2015.123](https://doi.org/10.1109/ICCV.2015.123)
* [104] Namsyu Ho, Laura Schmid, and Se-Young Yun. 2023. Large Language Models Are Reasoning Teachers. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 14852-14882. [https://doi.org/10.18653/v1/2023.acl-long.830](https://doi.org/10.18653/v1/2023.acl-long.830)
* [105] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Siffe. 2022. An Empirical Analysis of Compute-Optimal Large Language Model Training. In _Advances in Neural Information Processing Systems_, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). [https://openreview.net/forum?id=iBBrcRUIOAPR](https://openreview.net/forum?id=iBBrcRUIOAPR)
* [106] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, and Yu Wang. 2023. FlashDecoding++: Faster Large Language Model Inference on GPUs. arXiv:2311.01282 [cs.LG]
* [107] Or Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy. 2022. Instruction Induction: From Few Examples to Natural Language Task Descriptions. arXiv:2205.10782 [cs.CL]
* [108] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-Efficient Transfer Learning for NLP. In _Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research)_, Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.), Vol. 97. PMLR, 2790-2799. [https://proceedings.mlr.press/v97/houlsby19a.html](https://proceedings.mlr.press/v97/houlsby19a.html)
* [109] Sissie Hsiao, Yury Pinsky, and Sundar Pichai. 2023. Bard: Google's Generative Language Model. [https://blog.google/products/search/bard-updates/](https://blog.google/products/search/bard-updates/). Accessed: October 7, 2023.
* [110] Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. In _Findings of the Association for Computational Linguistics: ACL 2023_, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 8003-8017. [https://doi.org/10.18653/v1/2023.findings-acl.507](https://doi.org/10.18653/v1/2023.findings-acl.507)* [111] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRa: Low-Rank Adaptation of Large Language Models. In _International Conference on Learning Representations_. [https://openreview.net/forum?id=mZeVKeeYfYf9](https://openreview.net/forum?id=mZeVKeeYfYf9)
* [112] Shengding Hu, Ning Ding, Weilin Zhao, Xingtai Lv, Zhen Zhang, Zhiyuan Liu, and Maosong Sun. 2023. OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)_, Danushka Bollegala, Ruihong Huang, and Alan Ritter (Eds.). Association for Computational Linguistics, Toronto, Canada, 274-281. [https://doi.org/10.18653/v1/2023.acl-demo.26](https://doi.org/10.18653/v1/2023.acl-demo.26)
* [113] Zhiqiang Hu, Lei Wang, Tihuan Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Lee. 2023. LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 5254-5276. [https://aclanthology.org/2023.emnlp-main.319](https://aclanthology.org/2023.emnlp-main.319)
* [114] Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. 2023. LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition. arXiv:2307.13269 [cs.CL]
* [115] Xiao Shi Huang, Felipe Perez, Jimmy Ba, and Maksims Volkovs. 2020. Improving Transformer Optimization Through Better Initialization. In _Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research)_, Hal Daume III and Aarti Singh (Eds.), Vol. 119. PMLR, 4475-4483. [https://proceedings.mlr.press/v119/huang20f.html](https://proceedings.mlr.press/v119/huang20f.html)
* [116] Yukun Huang, Tanda Chen, Zhou Yu, and Kathleen McKeown. 2022. In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models. arXiv:2212.10670 [cs.CL]
* [117] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Iiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. 2019. GPipe: Efficient Training of Giant Neural Networks Using Pipeline Parallelism. In _Proceedings of the 33rd International Conference on Neural Information Processing Systems_. Curran Associates Inc., Red Hook, NY, USA, Article 10, 10 pages.
* [118] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. 2022. Block-recurrent transformers. _Advances in Neural Information Processing Systems_ 35 (2022), 33248-33261.
* [119] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, et al. 2023. Tutel: Adaptive mixture-of-experts at scale. _Proceedings of Machine Learning and Systems_ 5 (2023).
* [120] Regis Perrard Ilyas Moutawakil. 2023. LLM-Perl Leaderboard. [https://huggingface.co/spaces/optimum/llm-perl-leaderboard](https://huggingface.co/spaces/optimum/llm-perl-leaderboard).
* [121] Maor Ivgi, Uri Shaham, and Jonathan Berant. 2023. Efficient long-text understanding with short-text models. _Transactions of the Association for Computational Linguistics_ 11 (2023), 284-299.
* [122] Hamish Ivison, Noah A. Smith, Hannaneh Hajishirzi, and Pradeep Dasigi. 2023. Data-Efficient Finetuning Using Cross-Task Nearest Neighbors. In _Findings of the Association for Computational Linguistics: ACL 2023_, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 9036-9061. [https://doi.org/10.18653/v1/2023.findings-acl.576](https://doi.org/10.18653/v1/2023.findings-acl.576)
* [123] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lilio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Tween Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. 2023. Mistral 7B. arXiv:2301.06825 [cs.CL]
* [124] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. LongLLMIn-gua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression. arXiv:2310.06839 [cs.CL]
* [125] Yuxin Jiang, Chunkit Chan, Mingyang Chen, and Wei Wang. 2023. Lion: Adversarial Distillation of Proprietary Large Language Models. arXiv:2305.12870 [cs.CL]
* [126] Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. 2023. S\({}^{3}\): Increasing GPU Utilization during Generative Inference for Higher Throughput. arXiv:2306.06000 [cs.AR]
* [127] Hoyoun Jung and Kyung-Joong Kim. 2023. Discrete Prompt Compression with Reinforcement Learning. arXiv:2308.08758 [cs.CL]
* [128] Jean Kadodur, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. Challenges and Applications of Large Language Models. arXiv:2307.10169 [cs.CL]
* [129] Dhiraja Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, Iiyan Yang, Jongsoo Park, Alexander Heinecke, Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy, Bharat Kaul, and Pradeep Dubey. 2019. A Study of BFLOAT16 for Deep Learning Training. arXiv:1905.12322 [cs.LG]
* [130] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. 2021. Compacter: Efficient Low-Rank Hypercomplex Adapter Layers. In _Advances in Neural Information Processing Systems_, M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran Associates, Inc., New Orleans, Louisiana, 1022-1035.

[https://proceedings.neurips.cc/paper%5Ffiles/paper/2021/file/081be9dff07Fbc808f935906ef70c0-Paper.pdf](https://proceedings.neurips.cc/paper%5Ffiles/paper/2021/file/081be9dff07Fbc808f935906ef70c0-Paper.pdf)
* Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. 2020. Transformers is RNNs: Fast Autoregressive Transformers with Linear Attention. _Proceedings of 37th International Conference on Machine Learning (Proceedings of Machine Learning Research)_, Hal Daume III and Aarti Singh (Eds.), Vol. 119. PMLR, 5156-5165. [https://proceedings.mlr.press/v119/katharopoulos20a.html](https://proceedings.mlr.press/v119/katharopoulos20a.html)
* Keles et al.(2022) Feyza Duman Keles, Pruthuvi Maheskaya Wijewardena, and Chinmay Hegde. 2022. 자기 주의의 계산 복잡성에 대해. arXiv:2209.04881 [cs.LG]
* Kim 등(2023) 김정훈, 이정현, 김성동, 박준석, 유강민, 세정권, 이동수. 2023. 하위 4비트 정수 양자화를 통한 압축된 대용량 언어 모델의 메모리-효율적인 미세 조정. arXiv:2305.14152[cs.LG]
* Kim 등(2023) 김민수, 이시화, 이장환, 홍숙진, 장두성, 원용성, 최정욱. 2023. Ternary Weight Generative Language Model을 위한 Token-Scaled Logit Distillation. arXiv:2308.06744 [cs.CL]
* Kim 등(2023) Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W. 마호니, 아미르 고르라미, 커트 커처 2023. Big Little Decoder를 이용한 추측 디코딩. arXiv:230.07863 [cs.CL]
* Kim 등(2023) Young Jin Kim, Rawn Henry, Raffy Fahim, and Hany Hassan Awadalla. 2023. FineQuant: LLMs에 대한 Fine-Grained Weight-Only Quantization을 이용한 잠금 해제 효율. arXiv:2308.09723 [cs.LG]
* Kingma and Ba (2017) Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method for Stochastic Optimization. arXiv:1412.6980 [cs.LG]
* Kitaev 등(2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The Efficient Transformer. arXiv:2001.04451 [cs.LG]
* Kojima 등(2023) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. 대용량 언어 모델은 Zero-Shot Reasons이다. arXiv:2205.11916 [cs.CL]
* Korthikanti 등(2023) Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. 2023. 대형 변압기 모델에서의 활성화 재계산 감소. _ Machine Learning and Systems_ 5의 진행률.
* Kumar (2017) Siddharth Krishna Kumar. 2017. On weight initialization in deep neural networks. arXiv:1704.08863 [cs.LG]
* Kwon 등(2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. PagedAttention을 갖는 대용량 언어 모델 서빙을 위한 효율적인 메모리 관리. _Proceedings of the 29th Symposium on Operating Systems Principles_(Koblenz, Germany) _(SOSP '23)_. Association for Computing Machinery, New York, NY, USA, 611-626. [https://doi.org/10.1145/3600006.3613165](https://doi.org/10.1145/3600006.3613165)
* Lee 등(2023) Changun Lee, Jungyu Jin, Taeus Kim, Hyungjun Kim, and Eunhyeok Park. 2023. OWQ: Lessons from activation outliers for weight quantization in large language models. arXiv:2306.02272 [cs.CL]
* Lee et al. (2019) Juhho Lee, Yoonho Lee, Jung택 Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. 2019. Set Transformer: Attention-based Permutation-Invariant Neural Networks를 위한 프레임워크. _제36회 Machine Learning 국제회의의 Proceedings of Machine Learning (Proceedings of Machine Learning Research)_ 에서, Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.), Vol. 97. PMLR, 3744-3753. [https://proceedings.mlr.press/v97/lee19d.html](https://proceedings.mlr.press/v97/lee19d.html)
* Lepikhin 등(2021) Dmitry Lepikhin, Hyouk중 Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2021. GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding. _International Conference on Learning Representations_. [https://openreview.net/forum?id=qrwe7XHTmYb] (https://openreview.net/forum?id=qrwe7XHTmYb)
* Lester 등(2021) Brian Lester, Rami Al-Rfou, Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. _Proceedings of 2021 Conference on Empirical Methods in Natural Language Processing_, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 3045-3059. [https://doi.org/10.18653/v1/2021.emmlp-main.243](https://doi.org/10.18653/v1/2021.emmlp-main.243)
* Leviathan 등(2022) Yaniv Leviathan, Matan Kalman, and Y. 마티아스 2022. Fast Inference from Transformers via Speculative Decoding. _Machine Learning에 대 한 국제 회의_ 에서입니다.
* Lewis 등(2021) Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. 2021. 기본 계층: 크고 희박한 모델의 훈련 간소화. _Machine Learning에 대 한 국제 회의_ 에서입니다. PMLR, 6265-6274.
* Li 등(2021) Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, and Yuxiong He. 2021. 1비트 LAMB: LAMB의 수렴 속도를 갖는 통신 효율 대형 배치 트레이닝. In _HiPC 2022_ arXiv:2104.06069
* Li 등(2023) Liunian Harold Li, Jackk Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. 2023. Symbolic Chain-of-Thought Distillation: Small Models can can "Think" step-by-Step. _ ArXiv_abs/2306.14050 (2023).
* LI 등(2022) SHJANG LI, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jingu Qian, Baolin Peng, Yi Mao, Wenhu Chen, and Xifeng Yan. 2022. 큰 언어 모델로부터의 설명들이 작은 추론들을 더 낫게 한다. _ ArXiv_ abs/2210.06726 (2022).
* Li 등(2023) Shenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen Huang, Yuliang Liu, Boxiang Wang, and Yang You. 2023. 거대-AI: 대규모 병렬 학습을 위한 통합 딥러닝 시스템. _Proceedings of the 52nd International Conference on Parallel Processing_ (<conf-loc>, <city-Salt Lake City</city>, <state-UT</state>, <country>USA</country>, </conf-loc->)(_ICPP '23_). Association for Computing Machinery, New York, NY, USA, 766-775. [https://doi.org/10.1145/3605573.3605613](https://doi.org/10.1145/3605573.3605613)
* Li 등(2021) S. 이봉수 Li, Y. 너 2021. Sequence Parallelism: Long Sequence Training from System Perspective. _ arXiv_ (2021).
* Li 등(2023) Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sangghai, Yiming Yang, Sanjiv Kumar, and Srinadh Rhojanapalli. 2023. 상대 위치를 위한 기능적 보간은 긴 컨텍스트 트랜스포머를 향상시킨다. _ CoRR_ abs/2310.04418 (2023).
* Li 등(2020) Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala. 2020. PyTorch Distributed: Accelerating Data Parallel Training에 대한 경험_ CoRR_ abs/2006.15704 (2020).
* Li 등(2023) Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023. Unified Demonstration Retriever for In-Context Learning. _전산언어학회 제61차 연차총회 회보(제1권: 장문)_에 기재되어 있다. Association for Computational Linguistics, Toronto, Canada, 4644-4668. [https://doi.org/10.18653/v1/2023.acl-long.256](https://doi.org/10.18653/v1/2023.acl-long.256)
* Li 및 Qiu (2023) Xiaonan Li 및 Xipeng Qiu. 2023. In-context 학습을 위한 지원 예들을 찾는 것. _ arXiv preprint arXiv:2302.13539_ (2023).
* Li 등(2023) Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, et al. 2023. FLM-101B: An Open LLM and How to Train It with $100 K Budget. _ arXiv preprint arXiv:2309.03852_ (2023).
* Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. _ 제59회 전산언어학회 연차총회 및 제11회 자연어처리 국제공동회의(권 1: 장문)_ abs/2101.00190(2021)의 회보.
* Li 등(2023) Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, and Tuo Zhao. 2023. LofIQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. _ arXiv preprint arXiv:2310.08659_ (2023).
* Li 등(2023) Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, and Tuo Zhao. 2023. LoSparse: Low-Rank와 Sparse Approximation에 기반한 대용량 언어 모델의 구조화 압축. _International Conference on Machine Learning_. [https://api.semanticscholar.org/CorpusID:259203385] (https://api.semanticscholar.org/CorpusID:259203385)
* Li 등(2023) Zhuohan Li, Liannin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. AlpaServe: Model Parallelism with Statistical Multiplexing for Deep Learning Serving. _Proceedings of the 17th USENIX Symposium on Operating Systems Design and Implementation(OSDI)_에서.
* Liang 등(2023) Chen Liang, Simiao Zuo, Qingru Zhang, Pengcheng He, Weizhu Chen, and Tuo Zhao. 2023. 더 적다: 언어 모델 압축을 위한 태스크 인식 계층별 증류. _Machine Learning에 대 한 국제 회의_ 에서입니다. PMLR, 20852-20867.
* Lin 등(2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. _ arXiv preprint arXiv:2306.00978_ (2023).
* Liu 등(2023) Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. 2023. Sophia: 언어 모델 사전 훈련을 위한 스케일러블 확률적 이차 최적화기. _ arXiv preprint arXiv:2305.14342_ (2023).
* Liu 등(2022) Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022. Few-Shot Parameter-Efficient Fine-Tuning이 In-Context Learning보다 더 좋고 싸다. _ ArXiv_ abs/2205.05638 (2022).
* Liu 등(2022) Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022. Few-Shot Parameter-Efficient Fine-Tuning은 In-Context Learning보다 더 좋고 싸다. arXiv:2205.05638 [cs.LG]
* Liu 등(2023) Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, and Bohan Zhuang. 2023. QLLM: 대용량 언어 모델에 대한 정확하고 효율적인 저-비트폭 양자화_ arXiv preprint arXiv:2310.08041_ (2023).
* Liu 등(2022) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What Makes Good In-Context 예제 for GPT-37. In _Proceedings of Deep Learning Inside Out(DeeLO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures_. Association for Computational Linguistics, Dublin, Ireland and Online, 100-114. [https://doi.org/10.18653/v1/2022.delelo-1.10](https://doi.org/10.18653/v1/2022.delelo-1.10)
* Liu 등(2023) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ 컴퓨팅. Surveys_55, 9(2023), 1-35.
* Liu 등(2021) Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021. P-튜닝 v2: 프롬프트 튜닝은 스케일 및 태스크에 걸쳐 일반적으로 미세 튜닝과 비교할 수 있다. _ ArXiv_ abs/2110.07602 (2021).
* Liu et al.(2022) Xiaoxuan Liu, Linamin Zheng, Dequan Wang, Yukuo Cen, Weize Chen, Xu Han, Jianfei Chen, Zhiyuan Liu, Jie Tang, Joey Gonzalez, et al. 2022. GACT: Activation compressed training for generic network architecture. _Machine Learning에 대 한 국제 회의_ 에서입니다. PMLR, 14139-14152.

* [173] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT Understands, Too. _ArXiv_ abs/2103.10385 (2021).
* [174] Y. Liu, S. Li, J. Fang, Y. Shao, B. Yao, and Y. You. 2023. Colossal-Auto: Unified Automation of Parallelization and Activation Checkpoint for Large-scale Models. _arXiv_ (2023).
* [175] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. 2023. Scissorbands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time. _arXiv preprint arXiv:2305.17118_ (2023).
* [176] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. 2023. LLM-QAT: Data-Free Quantization Aware Training for Large Language Models. _ArXiv_ abs/2305.17888 (2023).
* [177] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. 2023. Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time. In _International Conference on Machine Learning_. PMLR, 22137-22176.
* [178] Ilya Loshchilov and Frank Hutter. 2017. Decoupled Weight Decay Regularization. _arXiv preprint arXiv:1711.05101_ (2017).
* [179] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. Association for Computational Linguistics, Dublin, Ireland, 8086-8098. [https://doi.org/10.18653/v1/2022.acl-long.556](https://doi.org/10.18653/v1/2022.acl-long.556)
* [180] Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, and Yuxiong He. 2022. Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam. (2022). arXiv:arXiv:2202.06009
* [181] Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasquart, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, and Vincent Y Zhao. 2023. Dr. ICL: Demonstration-Retrieved In-context Learning. _arXiv preprint arXiv:2305.14128_ (2023).
* [182] Kai Lv, Yuqing Yang, Tengciao Liu, Qi jie Gao, Qipeng Guo, and Xipeng Qiu. 2023. Full Parameter Fine-tuning for Large Language Models with Limited Resources. _ArXiv_ abs/2306.09782 (2023).
* [183] Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. LLM-Pruner: On the Structural Pruning of Large Language Models. _arXiv preprint arXiv:2305.11627_ (2023).
* [184] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi Chen, and Sanjeev Arora. 2023. Fine-Tuning Language Models with Just Forward Passes. _arXiv preprint arXiv:2305.17333_ (2023).
* [185] Pedro Henrique Martins, Zita Marinho, and Andre Martins. 2022. \(\infty\)-former: Infinite Memory Transformer. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, Smaranda Muresa, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 5468-5485. [https://doi.org/10.18653/v1/2022.acl-long.375](https://doi.org/10.18653/v1/2022.acl-long.375)
* [186] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. 2022. Long range language modeling via gated state spaces. _arXiv preprint arXiv:2206.13947_ (2022).
* [187] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. 2023. SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. _ArXiv_ abs/2305.09781 (2023).
* [188] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Olekskii Kuchaiev, Ganesh Venkatesh, et al. 2017. Mixed precision training. _arXiv preprint arXiv:1710.03740_ (2017).
* [189] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2021. MetaCL: Learning to Learn In Context. _ArXiv_ abs/2110.15943 (2021).
* [190] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the Role of Demonstrations: What makes In-context Learning Work?. In _EMNLP_.
* [191] Amirkeivan Mohtashami and Martin Jaggi. 2023. Landmark Attention: Random-Access Infinite Context Length for Transformers. _arXiv preprint arXiv:2305.16300_ (2023).
* [192] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan, et al. 2018. Ray: A distributed framework for emerging _AI_ applications. In _13th USENIX symposium on operating systems design and implementation (OSDI 18)_. 561-577.
* [193] MoosaitL. 2023. Composer. [https://github.com/mosical/composer](https://github.com/mosical/composer). GitHub repository.
* [194] MosaicML. 2023. LLM Foundry. [https://github.com/mosacional/llm-foundry](https://github.com/mosacional/llm-foundry). GitHub repository.
* [195] Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens. _arXiv preprint arXiv:2304.08467_ (2023).
* [196] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger, Phillip B. Gibbons, and Matei Zaharia. 2019. PipeDream: Generalized Pipeline Parallelism for DNN Training. In _Proceedings of the 27th ACM Symposium on Operating Systems Principles_ (Huntsville, Ontario, Canada) (_SOSP '19)_. Association for Computing Machinery, New York, NY, USA, 1-15. [https://doi.org/10.1145/3341301.3359646](https://doi.org/10.1145/3341301.3359646)
* Narayanan 등(2021) Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. 2021. Megatron-LM을 이용한 GPU 클러스터에서의 효율적인 대규모 언어 모델 트레이닝. _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_ (St. Louis, Missouri) _(SC'21)_. Association for Computing Machinery, New York, NY, USA, Article 58, 15 pages. [https://doi.org/10.1145/3458817.3476209] (https://doi.org/10.1145/3458817.3476209)
* Nijkamp 등(2022) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Liffu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. 코드겐: 멀티턴 프로그램 합성을 갖는 코드를 위한 개방형 대형 언어 모델. _ arXiv preprint arXiv:2203.13474_ (2022).
* NVIDIA(2023) NVIDIA. 2023. FasterTransformer: High Performance Transformer Kernels. [https://github.com/NVIDIA/FasterTransformer] (https://github.com/NVIDIA/FasterTransformer). GitHub 리포지토리입니다.
* OpenAI(2023) OpenAI. 2023. GPT-4 Technical Report. _ ArXiv_ abs/2303.08774 (2023).
* OpenAI(2023) OpenAI. 2023. GPT Base Model. [https://platform.openai.com/docs/models/gpt-base] (https://platform.openai.com/docs/models/gpt-base). 2023-12-13
* Padmanabhan 등(2023) Shankar Padmanabhan, Yasumasa Onoe, Michael JQ Zhang, Greg Durrett, and Eunsol Choi. 2023. 증류를 통해 지식 업데이트를 LMs에 전파. _ arXiv preprint arXiv:2306.09306_ (2023).
* Pagliardini 등(2023) Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and Francois Fleuret. 2023. 희소 플래시 어텐션을 통한 대규모 시퀀스에 대한 더 빠른 인과적 어텐션. _ ArXiv_ abs/2306.01160 (2023).
*Pan 등(2023) Yu Pan, Ye Yuan, Yichun Yin, Zenglin Xu, Lifeng Shang, Xin Jiang, and Qun Liu. 2023. Pretrained Models by multi-linear Operators for Efficient Training. _ CoRR_ abs/2310.10699 (2023).
*Pan 등(2021) Zizheng Pan, Peng Chen, Haoyu He, Jing Liu, Jianfei Cai, and Bohan Zhuang. 2021. Mesa: A Memory-saving Training Framework for Transformers. _ arXiv preprint arXiv:2111.11124_ (2021).
* Peng et al.(2023) Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. 2023. RWCV: Reinventing RNNs for Transformer Era. _ arXiv preprint arXiv:2305.13048_ (2023).
* Peng 등(2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction Tuning with GPT-4. _ArXiv_ abs/2304.03277 (2023).
* Peng 등(2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarm: 대용량 언어 모델들의 효율적인 컨텍스트 윈도우 확장. _ arXiv preprint arXiv:2309.00071_ (2023).
* Peng 등(2021) Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. 2021. Random feature attention. _ arXiv preprint arXiv:2103.02143_ (2021).
*Pham 등(2023) Aaron Pham, Chaoyu Yang, Sean Sheng, Shenyang Zhao, Sauyon Lee, Bo Jiang, Fog Dong, Xipeng Guan, and Frost Ming. 2023. _OpenLLM: Operating LLMs in production_. [https://github.com/bentoml/OpenLLM] (https://github.com/bentoml/OpenLLM)
* Pilault 등(2023) Jonathan Pilault, Mahan Fathi, Orhan Firat, Christopher Pal, Pierre-Luc Bacon, and Ross Goroshin. 2023. Block-State Transformers. 신경 정보 처리 시스템에 관한 37차 회의 New Orleans, Louisiana. [https://openreview.net/forum?id=XRTxIBs2eu] (https://openreview.net/forum?id=XRTxIBs2eu)
* Poli 등(2023) Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. 2023. Hyena hierarchy: 더 큰 convolutional language model을 향합니다. _ arXiv preprint arXiv:2302.10866_ (2023).
* Ponti 등(2023) Edoardo Maria Ponti, Alessandro Sordoni, Yoshua Bengio, and Siva Reddy. 2023. Combining Parameter-efficient Module for Task-level Generalization. _전산 언어학 협회의 유럽 챕터 제17차 회의의 진행문_ 에서. 687-702.
* Pope 등(2023) Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2023. 효율적인 Scaling Transformer Inference. _ Proceedings of Machine Learning and Systems_5 (2023).
* Press et al.(2022) Ofir Press, Noah Smith, and Mike Lewis. 2022. 열차 쇼트, 테스트 롱: 선형 바이어스를 갖는 주의력은 입력 길이 외삽을 가능하게 한다. _Learning Representations에 대한 국제 회의_. [https://openreview.net/forum?id=R8sQPpGCv0] (https://openreview.net/forum?id=R8sQPpGCv0)
* Press 등(2023) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2023. 언어 모델에서의 구성 갭의 측정 및 좁힘. arXiv:2210.03350 [cs.CL]
* Ray_의 LLMs. [https://github.com/ray-project/ray-llm] (https://github.com/ray-project/ray-llm) GitHub 리포지토리입니다.
* Qin 등(2023) Chengwei Qin, Aston Zhang, Anirudh Dagar, and Wenming Ye. 2023. In-Context Learning with Iterative Demonstration Selection. _ arXiv preprint arXiv:2310.09881_ (2023).
* Qin 등(2023) Guangui Qin, Corby Rosset, Ethan C. Chau, Nikhil Rao, and Benjamin Van Durme. 2023. Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models. [https://api.semanticscholar.org/CorpusID:263620438] (https://api.semanticscholar.org/CorpusID:263620438)
* Qin et al. (2021) Yujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han, Zhengyan Zhang, Yusheng Su, Zhiyuan Liu, Peng Li, Maosong Sun, et al. 2021. Knowledge inheritance for pre-trained language models. _ arXiv preprint arXiv:2105.13880_ (2021).

* [221] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. 2019. Blockwise self-attention for long document understanding. _arXiv preprint arXiv:1911.02972_ (2019).
* [222] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. [https://api.semanticscholar.org/CorpusID:160025533](https://api.semanticscholar.org/CorpusID:160025533)
* [223] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, et al. 2021. Scaling Language Models: Methods, Analysis & Insights from Training Gopher. _ArXiv_ abs/2112.11446 (2021). [https://api.semanticscholar.org/CorpusID:245353475](https://api.semanticscholar.org/CorpusID:245353475)
* [224] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_ 21, 1 (2020), 5485-5551.
* [225] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minja Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. 2022. DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale. In _International Conference on Machine Learning_.
* [226] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. ZeRO: Memory Optimizations toward Training Trillion Parameter Models. In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_ (Atlanta, Georgia) _(SC '20)_. IEEE Press, Article 20, 16 pages.
* [227] Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. 2021. ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_ (St. Louis, Missouri) _(SC '21)_. Association for Computing Machinery, New York, NY, USA, Article 59, 14 pages. [https://doi.org/10.1145/3458817.3476205](https://doi.org/10.1145/3458817.3476205)
* [228] Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. 2021. ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. In _SC 2021_. arXiv:arXiv:2104.07857
* [229] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '20, Tutorial)_.
* [230] Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. Parallel context windows for large language models. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. 6383-6402.
* [231] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. ZeRO-Offload: Democratizing Billion-Scale Model Training. In _USENIX ATC 2021_. arXiv:arXiv:2101.06840
* [232] Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, and ChengXiang Zhai. 2023. Sparse Modular Activation for Efficient Sequence Modeling. In _Thirty-seventh Conference on Neural Information Processing Systems_. [https://openreview.net/forum?id=TfbZx6I14i](https://openreview.net/forum?id=TfbZx6I14i)
* [233] Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda Zhang, A. V. Podolskiy, Grigory Arshinov, A. Bout, Irina Piontkovskaya, Jiansheng Wei, Xin Jiang, Teng Su, Qun Liu, and Jun Yao. 2023. PanGu-\(\Sigma\): Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing. _ArXiv_ abs/2303.10845 (2023). [https://api.semanticscholar.org/CorpusID:257666647](https://api.semanticscholar.org/CorpusID:257666647)
* [234] Adithya Renduchintala, Tugrul Konuk, and Oleksii Kuchaiev. 2023. Tied-Lora: Enhancing parameter efficiency of LoRa with weight tying.
* [235] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021. Efficient content-based sparse attention with routing transformers. _Transactions of the Association for Computational Linguistics_ 9 (2021), 53-68.
* [236] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning To Retrieve Prompts for In-Context Learning. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, Seattle, United States, 2655-2671. [https://doi.org/10.18653/v1/2022.naacl-main.191](https://doi.org/10.18653/v1/2022.naacl-main.191)
* [237] Lucia Santamaria and Amittai Axelrod. 2019. Data selection with cluster-based language difference models and cynical selection. _arXiv preprint arXiv:1904.04900_ (2019).
* [238] Andrea Santilli, Silvio Severino, Emilian Postolache, Valenttino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodola. 2023. Accelerating Transformer Inference for Translation via Parallel Decoding. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 12336-12355. [https://doi.org/10.18653/v1/2023.acl-long.689](https://doi.org/10.18653/v1/2023.acl-long.689)
* [239] Teven Le Scao, Angela Fan, Christopher Akiki, Elizabeth-Jane Pavlick, Suzana Ili'c, Daniel Hesslow, Roman Castagn'e, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. 2022. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. _ArXiv_ abs/2211.05100 (2022). [https://api.semanticscholar.org/CorpusID:253420279](https://api.semanticscholar.org/CorpusID:253420279)* Schoch et al. (2023) Stephanie Schoch, Ritwick Mishra, and Yangfeng Ji. 2023. Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values. _arXiv preprint arXiv:2306.10165_ (2023).
* Shallue et al. (2018) Christopher J. Shallue, Jaehoon Lee, Joseph M. 안토그니, 자샤 나랭 솔-딕스타인, 로이 프로스티그, 조지 E 달 2018. Data Parallelism이 Neural Network Training에 미치는 영향 측정. _ ArXiv_ abs/1811.03600 (2018). [https://api.semanticscholar.org/CorpusID:53214190] (https://api.semanticscholar.org/CorpusID:53214190)
* Shao 등(2023) Hang Shao, Bei Liu, and Yanmin Qian. 2023. One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. [https://api.semanticscholar.org/CorpusID:264146174] (https://api.semanticscholar.org/CorpusID:264146174)
* Shao 등(2022) Junru Shao, Xiyou Zhou, Siyuan Feng, Bohan Hou, Ruihang Lai, Hongyi Jin, Wuwei Lin, Masahiro Masuda, Cody Hao Yu, and Tianqi Chen. 2022. 확률 프로그램을 이용한 텐서 프로그램 최적화. S. 고예조 Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 35783-35796. [https://proceedings.neurips.cc/paper%5Ffiles/paper/2022/file/e894eafae43e68btc8dfac742bcbf3-Paper-Conference.pdf](https://proceedings.neurips.cc/paper%5Ffiles/paper/2022/file/e894eafae43e68btc8dfac742bcbf3-Paper-Conference.pdf)
* Shaw et al. (2018) Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. _ arXiv preprint arXiv:1803.02155_ (2018).
* Shazeer(2019) Noam Shazeer. 2019. Fast transformer decoding: One write-head만 있으면 됩니다. _ arXiv preprint arXiv:1911.02150_ (2019).
* Shen 등(2023) Sheng Shen, Le Hou, Yan-Quan Zhou, Nan Du, S. Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, and Denny Zhou. 2023. 혼합-전문가들은 명령어 튜닝을 만난다: 대형 언어 모델들에 대한 승리 조합.
* Shen 등(2022) Sheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and Iz Beltagy. 2022. 변압기 언어 모델에 대한 단계적 트레이닝. _Machine Learning에 대 한 국제 회의_ 에서입니다. PMLR, 19893-19908.
* Sheng 등(2023) Ying Sheng, Liamin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. 푸, 지창시, 베이디 첸, 클락 W. 배럿, 조셉 곤잘레스, 퍼시 량, 크리스토퍼 레, 이오안 크리스티안 스토이카, 그리고 세 장. 2023. High-throughput Generative Inference of Large Language Models with a Single GPU. _Machine Learning에 대 한 국제 회의_ 에서입니다.
* Shin 등(2020) Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: 자동으로 생성된 프롬프트를 사용하여 언어 모델에서 지식을 이끌어냅니다. _ arXiv preprint arXiv:2010.15980_ (2020).
* Shoeybi 등(2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: 모델 병렬 처리를 사용 하 여 수십억 매개 변수 언어 모델을 훈련 합니다. _ arXiv preprint arXiv:1909.08053_ (2019).
* Shridhar 등(2022) Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. 2022. 추론 능력을 더 작은 언어 모델로 증류한다. _계산 언어 학회 연례 회의_ 에서입니다.
* Simoulin 등(2023) Antoine Simoulin, Namyong Park, Xiaoyi Liu, and Grey Yang. 2023. Memory-Efficient Selective Fine-Tuning. _Workshop on Efficient Systems for Foundation Models@ICML2023_에서.
* Smith 등(2022) Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Anand Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. 2022. DeepSpeed and dagatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. _ ArXiv_ abs/2201.11990(2022).
* Soltan et al.(2022) Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, et al. 2022. Alexatm 20b: Few-shot learning using a large scale multilingual seq2seq model. _ arXiv preprint arXiv:2208.01448_ (2022).
* Spall(1992) James C. Spall. 1992. Multivariate stochastic approximation using simultaneous perturbation gradient approximation. _ IEEE Trans. Automat. Control_ 37 (1992), 332-341).
* Spector and Re(2023) Benjamin Spector and Chris Re. 2023. Accelerating llm inference with staged speculation decoding. _ arXiv preprint arXiv:2308.04623_ (2023).
* Su et al.(2022) Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. 2022. 선택적 어노테이션은 언어 모델을 적은 샷 학습자로 만든다. arXiv:2209.01975 [cs.CL]
* Su 등(2022) Hui Su, Xiao Zhu, Houjin Yu, Xiaoyu Shen, Yuwen Chen, Zilin Zhu, Yang Yu, and Jie Zhou. 2022. WeLM: A Well-Read Pre-trained Language Model for Chinese. _ arXiv preprint arXiv:2209.10372_ (2022).
* Su et al.(2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu. 2021. Roformer: 회전 위치 매립을 갖는 향상된 변압기. _ arXiv preprint arXiv:2104.09864_ (2021).
* Sun et al.(2023) Mingjie Sun, Zhu Liu, Anna Bair, and J. Zico Kolter. 2023. Simple and Effective Pruning Approach for Large Language Models. _ ArXiv_ abs/2306.11695 (2023).

* [261] Tianxiang Sun, Zhengfu He, Qinen Zhu, Xipeng Qiu, and Xuanjing Huang. 2022. Multitask Pre-training of Modular Prompt for Chinese Few-Shot Learning. In _Annual Meeting of the Association for Computational Linguistics_.
* [262] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. [n.d.]. Retentive Network: A Successor to Transformer for Large Language Models (2023). _arXiv preprint ArXiv:2307.08621_ ([n. d.]).
* [263] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022. A length-extrapolatable transformer. _arXiv preprint arXiv:2212.10554_ (2022).
* [264] Richard S. Sutton, David A. McAllester, Satinder Singh, and Y. Mansour. 1999. Policy Gradient Methods for Reinforcement Learning with Function Approximation. In _NIPS_.
* [265] Weng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Xingjian Zhang, Yuxiao Dong, Jiahua Liu, Maodi Hu, and Jie Tang. 2022. Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated Neural Text Retrievers. _CoRR_ abs/2207.07087 (2022). [https://doi.org/10.48550/arXiv:2207.07087](https://doi.org/10.48550/arXiv:2207.07087) arXiv:2207.07087
* [266] Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, and Yuxiong He. 2021. 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. In _ICML 2021_. arXiv:arXiv:2102.02888
* [267] Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, and Ngai Wong. 2022. Compression of generative pre-trained language models via quantization. _arXiv preprint arXiv:2203.10705_ (2022).
* [268] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020. Sparse sinkhorn attention. In _International Conference on Machine Learning_. PMLR, 9438-9447.
* [269] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022. Efficient Transformers: A Survey. _ACM Comput. Surv._ 55, G. Article 109 (dec 2022), 28 pages. [https://doi.org/10.1145/3530811](https://doi.org/10.1145/3530811)
* [270] Gemini Team Google. 2023. Gemini: A Family of Highly Capable Multimodal Models. [https://storage.googleapis.com/deepmind-media/gemini_1_report.pdf](https://storage.googleapis.com/deepmind-media/gemini_1_report.pdf).
* [271] MLC team. 2023. _MLC-LIM_. [https://github.com/mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm)
* [272] The MosaicML NLP Team. 2023. Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs. [https://www.mosaicml.com/blog/mpt-7b](https://www.mosaicml.com/blog/mpt-7b). Accessed: 2023-12-13.
* [273] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulhreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lambda: Language models for dialog applications. _arXiv preprint arXiv:2201.08239_ (2022).
* [274] Inar Timiryasov and Jean-Loup Tastet. 2023. Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty. _arXiv preprint arXiv:2308.02019_ (2023).
* [275] Denis Timonin, Bo Yang Hsueh, and Vinh Nguyen. 2022. Accelerated Inference for Large Transformer Models using Nvidia Triton Inference Server. _NVIDIA blog_ (2022).
* [276] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. _ArXiv_ abs/2302.13971 (2023). [https://api.semanticscholar.org/CorpusID:257219404](https://api.semanticscholar.org/CorpusID:257219404)
* [277] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, et al. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. _ArXiv_ abs/2307.09288 (2023). [https://api.semanticscholar.org/CorpusID:259950998](https://api.semanticscholar.org/CorpusID:259950998)
* [278] Szymon Tworkworkk, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr MiO's. 2023. Focused transformer: Contrastive training for context scaling. _arXiv preprint arXiv:2307.03170_ (2023).
* [279] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. 2022. DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation. _ArXiv_ abs/2210.07558 (2022).
* [280] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In _NIPS_. [https://api.semanticscholar.org/CorpusID:13756489](https://api.semanticscholar.org/CorpusID:13756489)
* [281] Apoorv Vyas, Angelos Katharopoulos, and Francois Fleuret. 2020. Fast transformers with clustered attention. _Advances in Neural Information Processing Systems_ 33 (2020), 21665-21674.
* [282] Zhongwei Wan, Che Liu, Mi Zhang, Jie Fu, Benyou Wang, Sibo Cheng, Lei Ma, Cesar Quilodran-Casas, and Rossella Arcucci. 2023. Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias. _arXiv preprint arXiv:2305.19894_ (2023).
* [283] Zhongwei Wan, Yichun Yin, Wei Zhang, Jiaxin Shi, Lifeng Shang, Guangyong Chen, Xin Jiang, and Qun Liu. 2022. G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 6585-6597. [https://doi.org/10.18653/v1/2022.emmlp-main.441](https://doi.org/10.18653/v1/2022.emmlp-main.441)* Wang et al. [2021] Boxiang Wang, Qifan Xu, Zhengda Bian, and Yang You. 2021. 2.5-dimensional distributed model training. _arXiv e-prints_ (2021), arXiv-2105.
* Wang et al. [2022] B. Wang, Q. Xu, Z. Bian, and Y. You. 2022. Tesseract: Parallelize the Tensor Parallelism Efficiently. In _Proceedings of the 51th International Conference on Parallel Processing_.
* Wang et al. [2022] Guoxin Wang, Yijuan Lu, Lei Cui, Tengchao Lv, Dinei Florencio, and Cha Zhang. 2022. A Simple yet Effective Learnable Positional Encoding Method for Improving Document Transformer Model. In _Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022_. 453-463.
* Wang et al. [2023] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. 2023. BitNet: Scaling 1-bit Transformers for Large Language Models. [https://api.semanticscholar.org/CorpusID:264172438](https://api.semanticscholar.org/CorpusID:264172438)
* Wang et al. [2022] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. 2022. Deepnet: Scaling transformers to 1,000 layers. _arXiv preprint arXiv:2203.00555_ (2022).
* Wang et al. [2023] Liang Wang, Nan Yang, and Furu Wei. 2023. Learning to Retrieve In-Context Examples for Large Language Models. arXiv:2307.07164 [cs.CL]
* Wang et al. [2022] Ningning Wang, Guobing Gan, Peng Zhang, Shuai Zhang, Junqiu Wei, Qun Liu, and Xin Jiang. 2022. Clusterformer: Neural clustering attention for efficient and effective transformer. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. 2390-2402.
* Wang et al. [2023] Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. 2023. Learning to grow pretrained models for efficient transformer training. _arXiv preprint arXiv:2303.00980_ (2023).
* Wang et al. [2023] Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023. SCOTT: Self-Consistent Chain-of-Thought Distillation. In _Annual Meeting of the Association for Computational Linguistics_.
* Wang et al. [2020] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_ (2020).
* Wang et al. [2023] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. 2023. Augmenting Language Models with Long-Term Memory. _arXiv preprint arXiv:2306.07174_ (2023).
* Wang et al. [2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. 2022. Self-Consistency Improves Chain of Thought Reasoning in Language Models. _ArXiv_ abs/2203.11171 (2022).
* Wang et al. [2023] Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. 2023. Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning. In _Thirty-seventh Conference on Neural Information Processing Systems_.
* Wang et al. [2023] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Language Models with Self-Generated Instructions. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 13484-13508. [https://doi.org/10.18653/v1/2023.acl-long.754](https://doi.org/10.18653/v1/2023.acl-long.754)
* Wang et al. [2022] Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, and Jianfeng Gao. 2022. AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning. _ArXiv_ abs/2210.17451 (2022).
* Wang et al. [2023] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyang Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023. Aligning large language models with human: A survey. _arXiv preprint arXiv:2307.12966_ (2023).
* Wang et al. [2023] Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Schmidt Feris, Huan Sun, and Yoon Kim. 2023. Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning. _ArXiv_ abs/2303.02861 (2023).
* Wei et al. [2022] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent Abilities of Large Language Models. _Trans. Mach. Learn. Res._ 2022 (2022).
* Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_ 35, 24824-24837.
* Wei et al. [2023] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. 2023. Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. _ArXiv_ abs/2304.09145 (2023).
* Winata et al. [2020] Genta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, and Pascale Fung. 2020. Lightweight and efficient end-to-end speech recognition using low-rank transformer. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE, 6144-6148.
* Wu et al. [2020] Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, and Zhou Yu. 2020. Memformer: A memory-augmented transformer for sequence modeling. _arXiv preprint arXiv:2010.06891_ (2020).

* [306] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. 2023. Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. (2023). arXiv:arXiv:2301.12017
* [307] Xiaoxia Wu, Zhewei Yao, and Yuxiong He. 2023. ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats. (2023). arXiv:arXiv:2307.09782
* [308] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. 2022. Memorizing transformers. _arXiv preprint arXiv:2203.08913_ (2022).
* [309] Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. 2023. Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering. arXiv:2212.10375 [cs.CL]
* [310] M. Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2023. Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning.
* [311] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In _International Conference on Machine Learning_. PMLR, 38087-38099.
* [312] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient Streaming Language Models with Attention Sinks. _arXiv preprint arXiv:2309.17453_ (2023).
* [313] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. 2023. Data selection for language models via importance resampling. _arXiv preprint arXiv:2302.03169_ (2023).
* [314] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. 2021. Nystromformer: A nystrom-based algorithm for approximating self-attention. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 35. 14138-14148.
* [315] Mingxue Xu, Yao Lei Xu, and Danilo P. Mandic. 2023. TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition. _ArXiv_ abs/2307.00526 (2023).
* [316] Peng Xu, Wei Ping, Xianchao Wu, Lawrence C. McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhutrina, Mohammad Shoeybi, and Bryan Catanzaro. 2023. Retrieval meets Long Context Large Language Models. [https://api.semanticscholar.org/CorpusID:263620134](https://api.semanticscholar.org/CorpusID:263620134)
* [317] Qifan Xu and Yang You. 2023. An Efficient 2D Method for Training Super-Large Deep Learning Models. In _2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)_. 222-232. [https://doi.org/10.1109/IPDPS54959.2023.00031](https://doi.org/10.1109/IPDPS54959.2023.00031)
* [318] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen, Xiaopeng Zhang, and Qi Tian. 2023. QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models. _arXiv preprint arXiv:2309.14177_ (2023).
* [319] Cheng Yang, Shengnan Wang, Chao Yang, Yuechuan Li, Ru He, and Jingqiao Zhang. 2020. Progressively stacking 2.0: A Multi-stage Layerwise Training Method for BERT Training Speedup. _arXiv preprint arXiv:2011.13635_ (2020).
* [320] Chengru Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. 2023. Large Language Models as Optimizers. arXiv:2309.03409 [cs.LG]
* [321] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. _ArXiv_ abs/2304.13712 (2023).
* [322] Keming Yang, Zichen Liu, and Philip Cheng. 2021. _MOSEC: Model Serving made Efficient in the Cloud_. [https://github.com/moseorg/mosee](https://github.com/moseorg/mosee)
* [323] Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023. Inference with Reference: Lossless Acceleration of Large Language Models. _ArXiv_ abs/2304.04487 (2023). [https://api.semanticscholar.org/CorpusID:258048436](https://api.semanticscholar.org/CorpusID:258048436)
* [324] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. arXiv:2305.10601 [cs.CL]
* [325] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs.CL]
* [326] Xingcheng Yao, Yanan Zheng, Xiaocong Yang, and Zhilin Yang. 2022. Nlp from scratch without large-scale pretraining: A simple and efficient framework. In _International Conference on Machine Learning_. PMLR, 25438-25451.
* [327] Yiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang. 2023. 2x Faster Language Model Pre-training via Masked Structural Growth. _arXiv preprint arXiv:2305.02869_ (2023).
* [328] Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, and Yuxiong He. 2023. DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. (2023). arXiv:arXiv:2308.01320
* [329] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. 2022. ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. In _NeurIPS 2022_. arXiv:arXiv:2206.01861* [330] Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. 2023. ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation.
* [331] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei Xu. 2023. EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models. _arXiv preprint arXiv:2308.14352_ (2023).
* [332] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. 2022. Orca: A distributed serving system for Transformer-Based generative models. In _16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)_. 521-538.
* [333] Lili Yu, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. 2023. Megabyte: Predicting million-byte sequences with multiscale transformers. _arXiv preprint arXiv:2305.07185_ (2023).
* [334] Zhihang Yuan, Lin Niu, Jia-Wen Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Jiangxiang Wu, and Bingzhe Wu. 2023. RPTQ: Reorder-based Post-training Quantization for Large Language Models. _ArXiv_ abs/2304.01089 (2023).
* [335] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. _Advances in neural information processing systems_ 33 (2020), 17283-17297.
* [336] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, P. Zhang, Yuxiao Dong, and Jie Tang. 2022. GLM-130B: An Open Bilingual Pre-trained Model. _ArXiv_ abs/2210.02414 (2022). [https://api.semanticscholar.org/CorpusID-252715691](https://api.semanticscholar.org/CorpusID-252715691)
* [337] Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, et al. 2021. Pangu-\(\alpha\): Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation. _arXiv preprint arXiv:2104.12369_ (2021).
* [338] Mingshu Zhai, Jiao He, Zixuan Ma, Zan Zong, Runqing Zhang, and Jidong Zhai. 2023. SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization. In _2023 USENIX Annual Technical Conference (USENIX ATC 23)_. 961-975.
* [339] Chen Zhang, Dawei Song, Zheyu Ye, and Yan Gao. 2023. Towards the Law of Capacity Gap in Distilling Language Models. _arXiv preprint arXiv:2311.07052_ (2023).
* [340] Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. 2019. Fixup initialization: Residual learning without normalization. _arXiv preprint arXiv:1901.09321_ (2019).
* [341] Hang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021. Poolingformer: Long document modeling with pooling attention. In _International Conference on Machine Learning_. PMLR, 12437-12446.
* [342] Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo Li. 2023. LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning. _ArXiv_ abs/2308.03303 (2023).
* [343] Mingyang Zhang, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang, et al. 2023. Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning. _arXiv preprint arXiv:2305.18403_ (2023).
* [344] Qingru Zhang, Minshuo Chen, Alexander W. Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. 2023. Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning. _ArXiv_ abs/2303.10512 (2023).
* [345] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Jiao Qiao. 2023. LLMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. _ArXiv_ abs/2303.16199 (2023).
* [346] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. OPT: Open Pre-trained Transformer Language Models. _arXiv preprint arXiv:2205.01068_ (2022).
* [347] Tianyi Zhang, Mina Lee, Lisa Li, Ende Shen, and Tatsunori B Hashimoto. 2022. TempLM: Distilling Language Models into Template-Based Generators. _arXiv preprint arXiv:2205.11055_ (2022).
* [348] Yue Zhang, Hongliang Fei, Dingcheng Li, and Ping Li. 2022. Promptgen: Automatically generate prompts using generative models. In _Findings of the Association for Computational Linguistics: NAACL 2022_. 30-37.
* [349] Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. Active Example Selection for In-Context Learning. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 9134-9148. [https://aclanthology.org/2022.emlp-main.622](https://aclanthology.org/2022.emlp-main.622)
* [350] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, et al. 2023. H\({}_{2}\)O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models. _arXiv preprint arXiv:2306.14048_ (2023).
* [351] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023. Automatic Chain of Thought Prompting in Large Language Models. In _The Eleventh International Conference on Learning Representations_. [https://openreview.net/forum?id=SNt6GFjUHkr](https://openreview.net/forum?id=SNt6GFjUHkr)* [352] Jiawei Zhao, Florian Schafer, and Anima Anandkumar. 2021. Zero Initialization: Initializing Neural Networks with only Zeros and Ones. _arXiv preprint arXiv:2110.12661_ (2021).
* [353] Weilin Zhao, Yuxiang Huang, Xu Han, Zhiyuan Liu, Zhengyan Zhang, and Maosong Sun. 2023. CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models. _ArXiv_ abs/2307.07705 (2023).
* [354] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Z. Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong Wen. 2023. A Survey of Large Language Models. _ArXiv_ abs/2303.18223 (2023).
* [355] Yanli Zhao, Andrew Gu, Rohan Varma, Liangchen Luo, Chien chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, and Shen Li. 2023. PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel. _Proc. VLDB Endow._ 16 (2023), 3848-3860. [https://api.semanticscholar.org/CorpusID-258297871](https://api.semanticscholar.org/CorpusID-258297871)
* [356] Linamin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P. Xing, Joseph E. Gonzalez, and Ion Stoica. 2022. Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning. In _Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI)_.
* [357] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al. 2023. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. _arXiv preprint arXiv:2303.17568_ (2023).
* [358] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: Less Is More for Alignment. arXiv:2305.11206 [cs.CL]
* [359] Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2023. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. arXiv:2205.10625 [cs.AI]
* [360] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, et al. 2022. Mixture-of-experts with expert choice routing. _Advances in Neural Information Processing Systems_ 35 (2022), 7103-7114.
* [361] Yongchao Zhou, Andre I Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2023. Large Language Models and Area Human-Level Prompt Engineers. In _The Eleventh International Conference on Learning Representations_. [https://openreview.net/forum?id=92gyk82DE-](https://openreview.net/forum?id=92gyk82DE-)
* [362] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2023. Pose: Efficient context window extension of llms via positional skip-wise training. _arXiv preprint arXiv:2309.10400_ (2023).
* [363] Bohan Zhuang, Jing Liu, Zizheng Pan, Haoyu He, Yuetian Weng, and Chunhua Shen. 2023. A survey on efficient training of transformers. _arXiv preprint arXiv:2302.01107_ (2023).
* [364] Yonghao Zhuang, Hexu Zhao, Lianmin Zheng, Zhuohan Li, Eric P. Xing, Qirong Ho, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. 2023. On Optimizing the Communication of Model Parallelism. In _Proceedings of Machine Learning and Systems (MLSys)_.
* [365] Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, and Jianfeng Gao. 2021. Taming Sparsely Activated Transformer with Stochastic Experts. _ArXiv_ abs/2110.04260 (2021).
